Implementation smell,Namespace,Class,File,Method,Description
Long Method,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The method has 900 lines of code.
Long Method,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,ChangeTrainingStrategy,The method has 117 lines of code.
Long Method,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The method has 857 lines of code.
Long Method,CNNWB.Model,CNNDataSet,C:\repos\supby_cnnwb\CNNWB.CNN\CNNDataSet.Designer.cs,InitClass,The method has 101 lines of code.
Complex Method,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,Cyclomatic complexity of the method is 356
Complex Method,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,ChangeTrainingStrategy,Cyclomatic complexity of the method is 45
Complex Method,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,CalculateSoftMax,Cyclomatic complexity of the method is 8
Complex Method,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,NeuralNetwork,Cyclomatic complexity of the method is 12
Complex Method,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,SaveDefinition,Cyclomatic complexity of the method is 18
Complex Method,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,SaveWeights,Cyclomatic complexity of the method is 12
Complex Method,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,LoadWeights,Cyclomatic complexity of the method is 8
Complex Method,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,LoadWeightsXmlBin,Cyclomatic complexity of the method is 8
Complex Method,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,AddGlobalTrainingRate,Cyclomatic complexity of the method is 8
Complex Method,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,Cyclomatic complexity of the method is 73
Complex Method,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,SetTrainingInputSample,Cyclomatic complexity of the method is 8
Complex Method,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TestingTask,Cyclomatic complexity of the method is 17
Complex Method,CNNWB.Model,CNNDataSet,C:\repos\supby_cnnwb\CNNWB.CNN\CNNDataSet.Designer.cs,CNNDataSet,Cyclomatic complexity of the method is 15
Complex Method,CNNWB.Model,CNNDataSet,C:\repos\supby_cnnwb\CNNWB.CNN\CNNDataSet.Designer.cs,ReadXmlSerializable,Cyclomatic complexity of the method is 14
Complex Method,CNNWB.Model,CNNDataSet,C:\repos\supby_cnnwb\CNNWB.CNN\CNNDataSet.Designer.cs,InitVars,Cyclomatic complexity of the method is 25
Long Parameter List,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The method has 5 parameters.
Long Parameter List,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The method has 6 parameters.
Long Parameter List,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The method has 17 parameters.
Long Parameter List,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The method has 16 parameters.
Long Parameter List,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The method has 10 parameters.
Long Parameter List,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The method has 8 parameters.
Long Parameter List,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The method has 44 parameters.
Long Parameter List,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,ChangeTrainingStrategy,The method has 17 parameters.
Long Parameter List,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,NeuralNetwork,The method has 12 parameters.
Long Parameter List,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,AddLayer,The method has 12 parameters.
Long Parameter List,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,AddLayer,The method has 12 parameters.
Long Parameter List,CNNWB.CNN,TestingParameters,C:\repos\supby_cnnwb\CNNWB.CNN\TestingParameters.cs,TestingParameters,The method has 8 parameters.
Long Parameter List,CNNWB.CNN,TrainingRate,C:\repos\supby_cnnwb\CNNWB.CNN\TrainingRate.cs,TrainingRate,The method has 10 parameters.
Long Parameter List,CNNWB.CNN,TrainingRate,C:\repos\supby_cnnwb\CNNWB.CNN\TrainingRate.cs,TrainingRate,The method has 17 parameters.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The length of the statement  "					Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue; " is 137.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,CalculateSoftMaxRBF,The length of the statement  "			dSum += MathUtil.Pow2 (PreviousLayer.Neurons [connection.ToNeuronIndex].Output - Weights [connection.ToWeightIndex].Value); " is 123.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,CalculateCCFDropOut,The length of the statement  "		Parallel.For (0' NeuronCount' Network.ParallelOption' i => NeuronActive [i] = Network.RandomGenerator.NextPercentage () < DropOutPercentage ? 1 : 0); " is 149.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,CalculateStochasticPooling,The length of the statement  "	if ((Network.OperationState == NetworkStates.Training) || (Network.OperationState == NetworkStates.CalculatingHessian)) { " is 121.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,CalculateStochasticPooling,The length of the statement  "					prob += PreviousLayer.Neurons [connection.ToNeuronIndex].Output * (PreviousLayer.Neurons [connection.ToNeuronIndex].Output / sum); " is 130.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,CalculateLocalResponseNormalization,The length of the statement  "		Neurons [i].Output = PreviousLayer.Neurons [i].Output / Math.Pow (((MathUtil.Scale * sum / Connections [i].Length) + 1D)' MathUtil.Pow); " is 136.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,CalculateLocalResponseNormalizationCM,The length of the statement  "		Neurons [i].Output = PreviousLayer.Neurons [i].Output / Math.Pow (((MathUtil.Scale * sum / Connections [i].Length) + 1D)' MathUtil.Pow); " is 136.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,CalculateLocalContrastNormalization,The length of the statement  "		Neurons [i].Output = PreviousLayer.Neurons [i].Output / Math.Pow (((MathUtil.Scale * sum / Connections [i].Length) + 1D)' MathUtil.Pow); " is 136.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,CalculateRBF,The length of the statement  "			dSum += MathUtil.Pow2 (PreviousLayer.Neurons [connection.ToNeuronIndex].Output - Weights [connection.ToWeightIndex].Value); " is 123.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,UpdateWeightsSGDLMParallel,The length of the statement  "	Parallel.For (0' WeightCount' Network.ParallelOption' i => Weights [i].Value -= (Network.TrainingRate.Rate / (Weights [i].DiagonalHessian + Network.dMicron)) * Weights [i].D1Err); " is 179.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,UpdateWeightsMiniBatchSGDLMSerial,The length of the statement  "		Weights [i].Value -= (Network.TrainingRate.Rate / (Weights [i].DiagonalHessian + Network.dMicron)) * (Weights [i].D1Err / batchSize); " is 133.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,UpdateWeightsMiniBatchSGDLMParallel,The length of the statement  "	Parallel.For (0' WeightCount' Network.ParallelOption' i => Weights [i].Value -= ((Network.TrainingRate.Rate / (Weights [i].DiagonalHessian + Network.dMicron)) * (Weights [i].D1Err / batchSize))); " is 195.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,UpdateWeighsSGDSerial,The length of the statement  "		Weights [i].Value -= (Network.TrainingRate.Rate * Weights [i].D1Err) - (Network.TrainingRate.Rate * Network.TrainingRate.WeightDecayFactor * Weights [i].Value); " is 160.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,UpdateWeightsSGDParallel,The length of the statement  "	Parallel.For (0' WeightCount' Network.ParallelOption' i => Weights [i].Value -= (Network.TrainingRate.Rate * Weights [i].D1Err) - (Network.TrainingRate.Rate * Network.TrainingRate.WeightDecayFactor * Weights [i].Value)); " is 220.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,UpdateWeighsSGDMSerial,The length of the statement  "		Weights [i].Value -= (Network.TrainingRate.Momentum * Weights [i].Value) - (Network.TrainingRate.Rate * Weights [i].D1Err); " is 123.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,UpdateWeightsSGDMParallel,The length of the statement  "	Parallel.For (0' WeightCount' Network.ParallelOption' i => Weights [i].Value -= (Network.TrainingRate.Momentum * Weights [i].Value) - (Network.TrainingRate.Rate * Weights [i].D1Err)); " is 183.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,UpdateWeightsMiniBatchSGDSerial,The length of the statement  "		Weights [i].Value -= (Network.TrainingRate.Rate * (Weights [i].D1Err / batchSize)) - (Network.TrainingRate.Rate * Network.TrainingRate.WeightDecayFactor * Weights [i].Value); " is 174.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,UpdateWeightsMiniBatchSGDParallel,The length of the statement  "	Parallel.For (0' WeightCount' Network.ParallelOption' i => Weights [i].Value -= (Network.TrainingRate.Rate * (Weights [i].D1Err / batchSize)) - (Network.TrainingRate.Rate * Network.TrainingRate.WeightDecayFactor * Weights [i].Value)); " is 234.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,UpdateWeightsMiniBatchSGDMSerial,The length of the statement  "		Weights [i].Value -= (Network.TrainingRate.Momentum * Weights [i].Value) - (Network.TrainingRate.Rate * (Weights [i].D1Err / batchSize)); " is 137.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,UpdateWeightsMiniBatchSGDMParallel,The length of the statement  "	Parallel.For (0' WeightCount' Network.ParallelOption' i => Weights [i].Value -= (Network.TrainingRate.Momentum * Weights [i].Value) - (Network.TrainingRate.Rate * (Weights [i].D1Err / batchSize))); " is 197.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateAveragePoolingSerial,The length of the statement  "				Weights [connection.ToWeightIndex].D1Err += neuronD1ErrY * PreviousLayer.Neurons [connection.ToNeuronIndex].Output * sf; " is 120.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateAveragePoolingParallel,The length of the statement  "				Weights [connection.ToWeightIndex].D1Err += neuronD1ErrY * PreviousLayer.Neurons [connection.ToNeuronIndex].Output * sf; " is 120.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateMaxPoolingWeightlessSerial,The length of the statement  "		PreviousLayer.Neurons [NeuronActive [i]].D1ErrX += DerivativeActivationFunction (Neurons [i].Output) * Neurons [i].D1ErrX; " is 122.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateMaxPoolingWeightlessParallel,The length of the statement  "		PreviousLayer.Neurons [NeuronActive [i]].D1ErrX += DerivativeActivationFunction (Neurons [i].Output) * Neurons [i].D1ErrX; " is 122.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateStochasticPoolingSerial,The length of the statement  "		PreviousLayer.Neurons [NeuronActive [i]].D1ErrX += DerivativeActivationFunction (Neurons [i].Output) * Neurons [i].D1ErrX; " is 122.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateStochasticPoolingParallel,The length of the statement  "		PreviousLayer.Neurons [NeuronActive [i]].D1ErrX += DerivativeActivationFunction (Neurons [i].Output) * Neurons [i].D1ErrX; " is 122.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateLocalResponseNormalizationSerial,The length of the statement  "		PreviousLayer.Neurons [i].D1ErrX += (1D / (double)Math.Pow (((MathUtil.Scale * sum / Connections [i].Length) + 1D)' MathUtil.Pow)) * Neurons [i].D1ErrX; " is 152.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateLocalResponseNormalizationParallel,The length of the statement  "		PreviousLayer.Neurons [i].D1ErrX += (1D / (double)Math.Pow (((MathUtil.Scale * sum / Connections [i].Length) + 1D)' MathUtil.Pow)) * Neurons [i].D1ErrX; " is 152.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagationLocalResponseNormalizationCMSerial,The length of the statement  "		PreviousLayer.Neurons [i].D1ErrX += (1D / (double)Math.Pow (((MathUtil.Scale * sum / Connections [i].Length) + 1D)' MathUtil.Pow)) * Neurons [i].D1ErrX; " is 152.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagationLocalResponseNormalizationCMParallel,The length of the statement  "		PreviousLayer.Neurons [i].D1ErrX += (1D / (double)Math.Pow (((MathUtil.Scale * sum / Connections [i].Length) + 1D)' MathUtil.Pow)) * Neurons [i].D1ErrX; " is 152.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateLocalContrastNormalizationSerial,The length of the statement  "		PreviousLayer.Neurons [i].D1ErrX += (1D / (double)Math.Pow (((MathUtil.Scale * sum / Connections [i].Length) + 1D)' MathUtil.Pow)) * Neurons [i].D1ErrX; " is 152.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateLocalContrastNormalizationParallel,The length of the statement  "		PreviousLayer.Neurons [i].D1ErrX += (1D / (double)Math.Pow (((MathUtil.Scale * sum / Connections [i].Length) + 1D)' MathUtil.Pow)) * Neurons [i].D1ErrX; " is 152.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateRBF,The length of the statement  "					Weights [connection.ToWeightIndex].D1Err += neuronD1ErrY * (PreviousLayer.Neurons [connection.ToNeuronIndex].Output - Weights [connection.ToWeightIndex].Value); " is 160.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateRBF,The length of the statement  "					PreviousLayer.Neurons [connection.ToNeuronIndex].D1ErrX += neuronD1ErrY * (PreviousLayer.Neurons [connection.ToNeuronIndex].Output - Weights [connection.ToWeightIndex].Value); " is 175.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateRBF,The length of the statement  "					Weights [connection.ToWeightIndex].D1Err += neuronD1ErrY * (PreviousLayer.Neurons [connection.ToNeuronIndex].Output - Weights [connection.ToWeightIndex].Value); " is 160.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateRBF,The length of the statement  "					PreviousLayer.Neurons [connection.ToNeuronIndex].D1ErrX += neuronD1ErrY * (PreviousLayer.Neurons [connection.ToNeuronIndex].Output - Weights [connection.ToWeightIndex].Value); " is 175.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateSecondDerivativesCCFParallel,The length of the statement  "				weightsD2Err [connection.ToWeightIndex] += neuronsD2ErrY [i] * MathUtil.Pow2 (PreviousLayer.Neurons [connection.ToNeuronIndex].Output); " is 135.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateSecondDerivativesCCFParallel,The length of the statement  "				prevLayerNeuronsD2ErrX [connection.ToNeuronIndex] += neuronsD2ErrY [i] * MathUtil.Pow2 (Weights [connection.ToWeightIndex].Value); " is 130.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateSecondDerivativesCCFSerial,The length of the statement  "				weightsD2Err [connection.ToWeightIndex] += neuronsD2ErrY [i] * MathUtil.Pow2 (PreviousLayer.Neurons [connection.ToNeuronIndex].Output); " is 135.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateSecondDerivativesCCFSerial,The length of the statement  "				prevLayerNeuronsD2ErrX [connection.ToNeuronIndex] += neuronsD2ErrY [i] * MathUtil.Pow2 (Weights [connection.ToWeightIndex].Value); " is 130.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateSecondDerivativesCCFSoftMaxParallel,The length of the statement  "				weightsD2Err [connection.ToWeightIndex] += neuronsD2ErrY [i] * MathUtil.Pow2 (PreviousLayer.Neurons [connection.ToNeuronIndex].Output); " is 135.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateSecondDerivativesCCFSoftMaxParallel,The length of the statement  "				prevLayerNeuronsD2ErrX [connection.ToNeuronIndex] += neuronsD2ErrY [i] * MathUtil.Pow2 (Weights [connection.ToWeightIndex].Value); " is 130.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateSecondDerivativesCCFSoftMaxSerial,The length of the statement  "				weightsD2Err [connection.ToWeightIndex] += neuronsD2ErrY [i] * MathUtil.Pow2 (PreviousLayer.Neurons [connection.ToNeuronIndex].Output); " is 135.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateSecondDerivativesCCFSoftMaxSerial,The length of the statement  "				prevLayerNeuronsD2ErrX [connection.ToNeuronIndex] += neuronsD2ErrY [i] * MathUtil.Pow2 (Weights [connection.ToWeightIndex].Value); " is 130.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateSecondDerivativesAveragePooling,The length of the statement  "				weightsD2Err [connection.ToWeightIndex] += neuronsD2ErrY [i] * MathUtil.Pow2 (PreviousLayer.Neurons [connection.ToNeuronIndex].Output * sf); " is 140.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateSecondDerivativesAveragePooling,The length of the statement  "				prevLayerNeuronsD2ErrX [connection.ToNeuronIndex] += neuronsD2ErrY [i] * MathUtil.Pow2 (Weights [connection.ToWeightIndex].Value); " is 130.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateSecondDerivativesMaxPooling,The length of the statement  "				prevLayerNeuronsD2ErrX [connection.ToNeuronIndex] += neuronsD2ErrY [i] * MathUtil.Pow2 (Weights [connection.ToWeightIndex].Value); " is 130.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateSecondDerivativesMaxPoolingWeightless,The length of the statement  "		prevLayerNeuronsD2ErrX [NeuronActive [i]] += MathUtil.Pow2 (DerivativeActivationFunction (Neurons [i].Output)) * neuronsD2ErrX [i]; " is 131.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateSecondDerivativesStochasticPooling,The length of the statement  "		prevLayerNeuronsD2ErrX [NeuronActive [i]] += MathUtil.Pow2 (DerivativeActivationFunction (Neurons [i].Output)) * neuronsD2ErrX [i]; " is 131.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateSecondDerivativeLocalResponseNormalization,The length of the statement  "		prevLayerNeuronsD2ErrX [i] = MathUtil.Pow2 (1D / (double)Math.Pow (((MathUtil.Scale * sum / Connections [i].Length) + 1D)' MathUtil.Pow)) * neuronsD2ErrX [i]; " is 158.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateSecondDerivativeLocalResponseNormalizationCM,The length of the statement  "		prevLayerNeuronsD2ErrX [i] = MathUtil.Pow2 (1D / (double)Math.Pow (((MathUtil.Scale * sum / Connections [i].Length) + 1D)' MathUtil.Pow)) * neuronsD2ErrX [i]; " is 158.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateSecondDerivativeLocalContrastNormalization,The length of the statement  "		prevLayerNeuronsD2ErrX [i] = MathUtil.Pow2 (1D / (double)Math.Pow (((MathUtil.Scale * sum / Connections [i].Length) + 1D)' MathUtil.Pow)) * neuronsD2ErrX [i]; " is 158.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateSecondDerivativesRBF,The length of the statement  "					weightsD2Err [connection.ToWeightIndex] += neuronsD2ErrY [i] * MathUtil.Pow2 (PreviousLayer.Neurons [connection.ToNeuronIndex].Output - Weights [connection.ToWeightIndex].Value); " is 178.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateSecondDerivativesRBF,The length of the statement  "					prevLayerNeuronsD2ErrX [connection.ToNeuronIndex] += neuronsD2ErrY [i] * MathUtil.Pow2 (PreviousLayer.Neurons [connection.ToNeuronIndex].Output - Weights [connection.ToWeightIndex].Value); " is 188.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateSecondDerivativesRBF,The length of the statement  "					weightsD2Err [connection.ToWeightIndex] += neuronsD2ErrY [i] * MathUtil.Pow2 (PreviousLayer.Neurons [connection.ToNeuronIndex].Output - Weights [connection.ToWeightIndex].Value); " is 178.
Long Statement,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BackpropagateSecondDerivativesRBF,The length of the statement  "					prevLayerNeuronsD2ErrX [connection.ToNeuronIndex] += neuronsD2ErrY [i] * MathUtil.Pow2 (PreviousLayer.Neurons [connection.ToNeuronIndex].Output - Weights [connection.ToWeightIndex].Value); " is 188.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,AddLayer,The length of the statement  "	Layer newLayer = new Layer (this' layerType' activationFunction' mapCount' mapWidth' mapHeight' receptiveFieldWidth' receptiveFieldHeight' strideX' strideY' padX' padY' mappings); " is 179.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,AddLayer,The length of the statement  "	Layer newLayer = new Layer (this' layerType' activationFunction' mapCount' mapWidth' mapHeight' receptiveFieldWidth' receptiveFieldHeight' strideX' strideY' padX' padY' true' dropOutPercentage); " is 194.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,SaveDefinition,The length of the statement  "			CNNDataSet.NeuralNetworksRow networkRow = ds.NeuralNetworks.AddNeuralNetworksRow ((byte)NetworkIndex' Name' Description' ClassCount' dataProviderSetRow' trainingStrategyRow' dMicron' TrainToValue' lossFunctionRow' DropOutUsed' SubstractMean' HessianSamples' Min' Max); " is 268.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,SaveDefinition,The length of the statement  "				ds.Layers.AddLayersRow (networkRow' (byte)layer.LayerIndex' layerTypeRow' activationTypeRow' layer.NeuronCount' layer.UseMapInfo' layer.MapCount' layer.MapWidth' layer.MapHeight' layer.ReceptiveFieldWidth' layer.ReceptiveFieldHeight' layer.StrideX' layer.StrideY' layer.PadX' layer.PadY' layer.IsFullyMapped' layer.LockedWeights' layer.UseDropOut' layer.DropOutPercentage); " is 373.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,SaveFullyZipped,The length of the statement  "		archive.CreateEntryFromFile (tempPath + @"\" + definitionFileName' definitionFileName + "-gz"' CompressionLevel.Optimal); " is 121.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,LoadDefinition,The length of the statement  "			network = new NeuralNetwork (dataProvider' networkRow.Name' networkRow.ClassCount' networkRow.TrainToValue' (LossFunctions)networkRow.LossFunction' (DataProviderSets)networkRow.DataProviderSet' (TrainingStrategy)networkRow.TrainingStrategy' networkRow.DMicron' networkRow.HessianSamples' networkRow.SubstractMean); " is 314.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,LoadDefinition,The length of the statement  "				layer = new Layer (network' layerRow.LayerIndex' l' f' layerRow.NeuronCount' layerRow.UseMapInfo' layerRow.MapCount' layerRow.MapWidth' layerRow.MapHeight' layerRow.IsFullyMapped' layerRow.ReceptiveFieldWidth' layerRow.ReceptiveFieldHeight' layerRow.StrideX' layerRow.StrideY' layerRow.PadX' layerRow.PadY' previousLayer' mappings' layerRow.LockedWeights' layerRow.UseDropOut' layerRow.DropOutPercentage); " is 405.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,LoadWeightsXmlBin,The length of the statement  "	System.Runtime.Serialization.Formatters.Binary.BinaryFormatter fmt = new System.Runtime.Serialization.Formatters.Binary.BinaryFormatter (); " is 139.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,LoadWeightsXmlBin,The length of the statement  "		if (Description.Contains (networkRow.Description.Trim ()))// check if we're dealing with an identical nertwork definition as the current one " is 140.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,LoadFullyZipped,The length of the statement  "		if ((archive.Entries [0].Name.Contains (".definition-xml-gz")) && (archive.Entries [1].Name.Contains (".weights-bin-gz"))) { " is 124.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,AddGlobalTrainingRate,The length of the statement  "			TrainingRates.Add (new TrainingRate (newRating' rate.DecayAfterEpochs' rate.MinimumRate' rate.WeightDecayFactor' rate.Momentum' rate.BatchSize' rate.InitialAvgLoss' 1' 1' rate.WeightSaveTreshold' rate.Distorted' rate.DistortionPercentage' rate.SeverityFactor' rate.MaxScaling' rate.MaxRotation' rate.ElasticSigma' rate.ElasticScaling)); " is 336.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,AddGlobalTrainingRate,The length of the statement  "			TrainingRates.Add (new TrainingRate (newRating' rate.Epochs - (totIteration * rate.DecayAfterEpochs)' rate.MinimumRate' rate.WeightDecayFactor' rate.Momentum' rate.BatchSize' rate.InitialAvgLoss' 1' 1' rate.WeightSaveTreshold' rate.Distorted' rate.DistortionPercentage' rate.SeverityFactor' rate.MaxScaling' rate.MaxRotation' rate.ElasticSigma' rate.ElasticScaling)); " is 367.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,GetTaskDuration,The length of the statement  "			timeStringBuilder.AppendFormat ("{0:D} day {1:D2}:{2:D2}:{3:D2}"' TaskDuration.Elapsed.Days' TaskDuration.Elapsed.Hours' TaskDuration.Elapsed.Minutes' TaskDuration.Elapsed.Seconds); " is 181.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,GetTaskDuration,The length of the statement  "			timeStringBuilder.AppendFormat ("{0:D} days {1:D2}:{2:D2}:{3:D2}"' TaskDuration.Elapsed.Days' TaskDuration.Elapsed.Hours' TaskDuration.Elapsed.Minutes' TaskDuration.Elapsed.Seconds); " is 182.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,GetTaskDuration,The length of the statement  "		timeStringBuilder.AppendFormat ("{0:D2}:{1:D2}:{2:D2}"' TaskDuration.Elapsed.Hours' TaskDuration.Elapsed.Minutes' TaskDuration.Elapsed.Seconds); " is 144.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "	bool calculatePseudoHessian = (TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardt) || (TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardt) || (TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA); " is 309.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "	int bestScore = (int)(((double)DataProvider.TestingSamplesCount / 100D) * (100D - TrainingRates [0].WeightSaveTreshold)); " is 121.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "					double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider); " is 124.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "						Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min; " is 155.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "								double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling).SubstractMean (DataProvider); " is 285.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "								CurrentSample = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling); " is 256.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "								double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider); " is 124.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "									Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min; " is 155.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "							double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider); " is 124.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "								Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min; " is 155.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "								double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling).SubstractMean (DataProvider); " is 285.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "								CurrentSample = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling); " is 256.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "								double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider); " is 124.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "									Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min; " is 155.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "							double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider); " is 124.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "								Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min; " is 155.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "									double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling).SubstractMean (DataProvider); " is 298.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "									CurrentSample = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling); " is 269.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "									double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].SubstractMean (DataProvider); " is 137.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "										Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Image [i] / 255D) * Spread) + Min; " is 168.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "						for (SampleIndex = DataProvider.TrainingSamplesCount - finalBatch; SampleIndex < DataProvider.TrainingSamplesCount; SampleIndex++) { " is 132.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "									double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling).SubstractMean (DataProvider); " is 285.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "									CurrentSample = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling); " is 256.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "									double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider); " is 124.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "										Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min; " is 155.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "								double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].SubstractMean (DataProvider); " is 137.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "									Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Image [i] / 255D) * Spread) + Min; " is 168.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "						for (SampleIndex = DataProvider.TrainingSamplesCount - finalBatch; SampleIndex < DataProvider.TrainingSamplesCount; SampleIndex++) { " is 132.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "								double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider); " is 124.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "									Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min; " is 155.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "									double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling).SubstractMean (DataProvider); " is 298.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "									CurrentSample = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling); " is 269.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "									double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].SubstractMean (DataProvider); " is 137.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "										Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Image [i] / 255D) * Spread) + Min; " is 168.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "						for (SampleIndex = DataProvider.TrainingSamplesCount - finalBatch; SampleIndex < DataProvider.TrainingSamplesCount; SampleIndex++) { " is 132.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "									double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling).SubstractMean (DataProvider); " is 285.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "									CurrentSample = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling); " is 256.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "									double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider); " is 124.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "										Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min; " is 155.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "								double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].SubstractMean (DataProvider); " is 137.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "									Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Image [i] / 255D) * Spread) + Min; " is 168.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "						for (SampleIndex = DataProvider.TrainingSamplesCount - finalBatch; SampleIndex < DataProvider.TrainingSamplesCount; SampleIndex++) { " is 132.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "								double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider); " is 124.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "									Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min; " is 155.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The length of the statement  "			string fileName = DataProvider.StorageDirectory + @"\" + Name + " (epoch " + CurrentEpoch.ToString () + " - " + bestScore.ToString () + " errors).weights-bin"; " is 159.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,SetTrainingInputSample,The length of the statement  "			CurrentSample = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [index]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling); " is 250.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TestingTask,The length of the statement  "				CurrentSample = DataProvider.TrainingSamples [SampleIndex].Distorted (DataProvider' TestParameters.SeverityFactor' TestParameters.MaxScaling' TestParameters.MaxRotation' TestParameters.ElasticSigma' TestParameters.ElasticScaling); " is 230.
Long Statement,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TestingTask,The length of the statement  "				CurrentSample = DataProvider.TestingSamples [SampleIndex].Distorted (DataProvider' TestParameters.SeverityFactor' TestParameters.MaxScaling' TestParameters.MaxRotation' TestParameters.ElasticSigma' TestParameters.ElasticScaling); " is 229.
Long Statement,CNNWB.Model,CNNDataSet,C:\repos\supby_cnnwb\CNNWB.CNN\CNNDataSet.Designer.cs,InitClass,The length of the statement  "	this.relationFK_Connections_Neurons = new global::System.Data.DataRelation ("FK_Connections_Neurons"' new global::System.Data.DataColumn[] { " is 140.
Long Statement,CNNWB.Model,CNNDataSet,C:\repos\supby_cnnwb\CNNWB.CNN\CNNDataSet.Designer.cs,InitClass,The length of the statement  "	this.relationFK_Layers_ActivationFunctions = new global::System.Data.DataRelation ("FK_Layers_ActivationFunctions"' new global::System.Data.DataColumn[] { " is 154.
Long Statement,CNNWB.Model,CNNDataSet,C:\repos\supby_cnnwb\CNNWB.CNN\CNNDataSet.Designer.cs,InitClass,The length of the statement  "	this.relationFK_Layers_LayerTypes = new global::System.Data.DataRelation ("FK_Layers_LayerTypes"' new global::System.Data.DataColumn[] { " is 136.
Long Statement,CNNWB.Model,CNNDataSet,C:\repos\supby_cnnwb\CNNWB.CNN\CNNDataSet.Designer.cs,InitClass,The length of the statement  "	this.relationFK_Layers_NeuralNetworks = new global::System.Data.DataRelation ("FK_Layers_NeuralNetworks"' new global::System.Data.DataColumn[] { " is 144.
Long Statement,CNNWB.Model,CNNDataSet,C:\repos\supby_cnnwb\CNNWB.CNN\CNNDataSet.Designer.cs,InitClass,The length of the statement  "	this.relationFK_Mappings_Layers = new global::System.Data.DataRelation ("FK_Mappings_Layers"' new global::System.Data.DataColumn[] { " is 132.
Long Statement,CNNWB.Model,CNNDataSet,C:\repos\supby_cnnwb\CNNWB.CNN\CNNDataSet.Designer.cs,InitClass,The length of the statement  "	this.relationFK_NeuralNetworks_DataProviderSets = new global::System.Data.DataRelation ("FK_NeuralNetworks_DataProviderSets"' new global::System.Data.DataColumn[] { " is 164.
Long Statement,CNNWB.Model,CNNDataSet,C:\repos\supby_cnnwb\CNNWB.CNN\CNNDataSet.Designer.cs,InitClass,The length of the statement  "	this.relationFK_NeuralNetworks_LossFunctions = new global::System.Data.DataRelation ("FK_NeuralNetworks_LossFunctions"' new global::System.Data.DataColumn[] { " is 158.
Long Statement,CNNWB.Model,CNNDataSet,C:\repos\supby_cnnwb\CNNWB.CNN\CNNDataSet.Designer.cs,InitClass,The length of the statement  "	this.relationFK_NeuralNetworks_TrainingStrategies = new global::System.Data.DataRelation ("FK_NeuralNetworks_TrainingStrategies"' new global::System.Data.DataColumn[] { " is 168.
Long Statement,CNNWB.Model,CNNDataSet,C:\repos\supby_cnnwb\CNNWB.CNN\CNNDataSet.Designer.cs,InitClass,The length of the statement  "	this.relationFK_Neurons_Layers = new global::System.Data.DataRelation ("FK_Neurons_Layers"' new global::System.Data.DataColumn[] { " is 130.
Long Statement,CNNWB.Model,CNNDataSet,C:\repos\supby_cnnwb\CNNWB.CNN\CNNDataSet.Designer.cs,InitClass,The length of the statement  "	this.relationFK_Weights_Layers = new global::System.Data.DataRelation ("FK_Weights_Layers"' new global::System.Data.DataColumn[] { " is 130.
Long Statement,CNNWB.Model,CNNDataSet,C:\repos\supby_cnnwb\CNNWB.CNN\CNNDataSet.Designer.cs,GetTypedDataSetSchema,The length of the statement  "			for (global::System.Collections.IEnumerator schemas = xs.Schemas (dsSchema.TargetNamespace).GetEnumerator (); schemas.MoveNext ();) { " is 133.
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.Input:  	ActivationFunctionId = ActivationFunctions.None;  	HasWeights = false;  	ActivationFunction = null;  	UseWeightPartitioner = false;  	WeightCount = 0;  	Weights = null;  	CalculateAction = null;  	BackpropagateAction = null;  	BackpropagateSecondDerivativesAction = null;  	break;  case LayerTypes.Local:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = totalMappings * MapSize * (ReceptiveFieldSize + 1);  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	//CalculateLocalConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		// CalculateLocalConnectedDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount)) {  					int iNumWeight = mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					int iNumWeight = mapping * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				int iNumWeight = curMap * (ReceptiveFieldSize + 1) * MapSize;  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						AddBias (ref Connections [position]' iNumWeight++);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.Convolutional:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskWidth) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.ConvolutionalSubsampling:  	// Simard's implementation  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	for (int map = 0; map < PreviousLayer.MapCount; map++)  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	if (!IsFullyMapped)// not fully mapped  	 {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.AvgPooling:  case LayerTypes.MaxPooling:  	HasWeights = true;  	WeightCount = MapCount * 2;  	Weights = new Weight[WeightCount];  	if (LayerType == LayerTypes.AvgPooling) {  		CalculateAction = CalculateAveragePooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePooling;  	}  	else {  		CalculateAction = CalculateMaxPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPooling;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  										else  											outOfBounds = false;  									}  							}  					}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  							}  					}  				}  			});  		}  	}  	break;  case LayerTypes.AvgPoolingWeightless:  case LayerTypes.MaxPoolingWeightless:  case LayerTypes.L2Pooling:  case LayerTypes.StochasticPooling:  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	switch (LayerType) {  	case LayerTypes.AvgPoolingWeightless:  		CalculateAction = CalculateAveragePoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePoolingWeightless;  		break;  	case LayerTypes.MaxPoolingWeightless:  		CalculateAction = CalculateMaxPoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPoolingWeightless;  		break;  	case LayerTypes.L2Pooling:  		CalculateAction = CalculateL2Pooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateL2PoolingParallel;  		else  			BackpropagateAction = BackpropagateL2PoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesL2Pooling;  		break;  	case LayerTypes.StochasticPooling:  		CalculateAction = CalculateStochasticPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateStochasticPoolingParallel;  		else  			BackpropagateAction = BackpropagateStochasticPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesStochasticPooling;  		break;  	}  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  										else  											outOfBounds = false;  									}  							}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  							}  				}  			});  		}  	}  	break;  case LayerTypes.FullyConnected:  	HasWeights = true;  	WeightCount = (PreviousLayer.NeuronCount + 1) * NeuronCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMax;  	else  		CalculateAction = CalculateFullyConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxParallel;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxSerial;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		Parallel.For (0' MapCount' curMap =>  {  			for (int yc = 0; yc < MapHeight; yc++)  				for (int xc = 0; xc < MapWidth; xc++) {  					int position = xc + (yc * MapWidth) + (curMap * MapSize);  					AddBias (ref Connections [position]' iNumWeight++);  					for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  						for (int y = 0; y < PreviousLayer.MapHeight; y++)  							for (int x = 0; x < PreviousLayer.MapWidth; x++)  								AddConnection (ref Connections [position]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  				}  		});  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++) {  			AddBias (ref Connections [y]' iNumWeight++);  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  		}  	}  	break;  case LayerTypes.RBF:  	HasWeights = true;  	WeightCount = PreviousLayer.NeuronCount * NeuronCount;  	// no biasses  	Weights = new Weight[WeightCount];  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMaxRBF;  	else  		CalculateAction = CalculateRBF;  	BackpropagateAction = BackpropagateRBF;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesRBF;  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		for (int n = 0; n < NeuronCount; n++)  			for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  				for (int y = 0; y < PreviousLayer.MapHeight; y++)  					for (int x = 0; x < PreviousLayer.MapWidth; x++)  						AddConnection (ref Connections [n]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++)  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  	}  	break;  case LayerTypes.LocalResponseNormalization:  	// same map  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalResponseNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalResponseNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalResponseNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalResponseNormalizationCM:  	// across maps  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztionCM layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	int size = 9;  	int a' b;  	for (int map = 0; map < MapCount; map++)  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				a = Math.Max (0' map - (size / 2));  				// from map  				b = Math.Min (MapCount' map - (size / 2) + size);  				// to map  				int position = x + (y * MapWidth) + (map * MapSize);  				for (int f = a; f < b; f++)  					AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  			}  	CalculateAction = CalculateLocalResponseNormalizationCM;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMParallel;  	else  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalizationCM;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalContrastNormalization:  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	Gaussian2DKernel = MathUtil.CreateGaussian2DKernel (ReceptiveFieldWidth' ReceptiveFieldHeight);  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalContrastNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalContrastNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalContrastNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalContrastNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.Input:  	ActivationFunctionId = ActivationFunctions.None;  	HasWeights = false;  	ActivationFunction = null;  	UseWeightPartitioner = false;  	WeightCount = 0;  	Weights = null;  	CalculateAction = null;  	BackpropagateAction = null;  	BackpropagateSecondDerivativesAction = null;  	break;  case LayerTypes.Local:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = totalMappings * MapSize * (ReceptiveFieldSize + 1);  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	//CalculateLocalConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		// CalculateLocalConnectedDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount)) {  					int iNumWeight = mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					int iNumWeight = mapping * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				int iNumWeight = curMap * (ReceptiveFieldSize + 1) * MapSize;  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						AddBias (ref Connections [position]' iNumWeight++);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.Convolutional:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskWidth) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.ConvolutionalSubsampling:  	// Simard's implementation  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	for (int map = 0; map < PreviousLayer.MapCount; map++)  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	if (!IsFullyMapped)// not fully mapped  	 {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.AvgPooling:  case LayerTypes.MaxPooling:  	HasWeights = true;  	WeightCount = MapCount * 2;  	Weights = new Weight[WeightCount];  	if (LayerType == LayerTypes.AvgPooling) {  		CalculateAction = CalculateAveragePooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePooling;  	}  	else {  		CalculateAction = CalculateMaxPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPooling;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  										else  											outOfBounds = false;  									}  							}  					}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  							}  					}  				}  			});  		}  	}  	break;  case LayerTypes.AvgPoolingWeightless:  case LayerTypes.MaxPoolingWeightless:  case LayerTypes.L2Pooling:  case LayerTypes.StochasticPooling:  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	switch (LayerType) {  	case LayerTypes.AvgPoolingWeightless:  		CalculateAction = CalculateAveragePoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePoolingWeightless;  		break;  	case LayerTypes.MaxPoolingWeightless:  		CalculateAction = CalculateMaxPoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPoolingWeightless;  		break;  	case LayerTypes.L2Pooling:  		CalculateAction = CalculateL2Pooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateL2PoolingParallel;  		else  			BackpropagateAction = BackpropagateL2PoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesL2Pooling;  		break;  	case LayerTypes.StochasticPooling:  		CalculateAction = CalculateStochasticPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateStochasticPoolingParallel;  		else  			BackpropagateAction = BackpropagateStochasticPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesStochasticPooling;  		break;  	}  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  										else  											outOfBounds = false;  									}  							}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  							}  				}  			});  		}  	}  	break;  case LayerTypes.FullyConnected:  	HasWeights = true;  	WeightCount = (PreviousLayer.NeuronCount + 1) * NeuronCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMax;  	else  		CalculateAction = CalculateFullyConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxParallel;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxSerial;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		Parallel.For (0' MapCount' curMap =>  {  			for (int yc = 0; yc < MapHeight; yc++)  				for (int xc = 0; xc < MapWidth; xc++) {  					int position = xc + (yc * MapWidth) + (curMap * MapSize);  					AddBias (ref Connections [position]' iNumWeight++);  					for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  						for (int y = 0; y < PreviousLayer.MapHeight; y++)  							for (int x = 0; x < PreviousLayer.MapWidth; x++)  								AddConnection (ref Connections [position]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  				}  		});  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++) {  			AddBias (ref Connections [y]' iNumWeight++);  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  		}  	}  	break;  case LayerTypes.RBF:  	HasWeights = true;  	WeightCount = PreviousLayer.NeuronCount * NeuronCount;  	// no biasses  	Weights = new Weight[WeightCount];  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMaxRBF;  	else  		CalculateAction = CalculateRBF;  	BackpropagateAction = BackpropagateRBF;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesRBF;  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		for (int n = 0; n < NeuronCount; n++)  			for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  				for (int y = 0; y < PreviousLayer.MapHeight; y++)  					for (int x = 0; x < PreviousLayer.MapWidth; x++)  						AddConnection (ref Connections [n]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++)  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  	}  	break;  case LayerTypes.LocalResponseNormalization:  	// same map  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalResponseNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalResponseNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalResponseNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalResponseNormalizationCM:  	// across maps  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztionCM layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	int size = 9;  	int a' b;  	for (int map = 0; map < MapCount; map++)  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				a = Math.Max (0' map - (size / 2));  				// from map  				b = Math.Min (MapCount' map - (size / 2) + size);  				// to map  				int position = x + (y * MapWidth) + (map * MapSize);  				for (int f = a; f < b; f++)  					AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  			}  	CalculateAction = CalculateLocalResponseNormalizationCM;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMParallel;  	else  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalizationCM;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalContrastNormalization:  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	Gaussian2DKernel = MathUtil.CreateGaussian2DKernel (ReceptiveFieldWidth' ReceptiveFieldHeight);  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalContrastNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalContrastNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalContrastNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalContrastNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.Input:  	ActivationFunctionId = ActivationFunctions.None;  	HasWeights = false;  	ActivationFunction = null;  	UseWeightPartitioner = false;  	WeightCount = 0;  	Weights = null;  	CalculateAction = null;  	BackpropagateAction = null;  	BackpropagateSecondDerivativesAction = null;  	break;  case LayerTypes.Local:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = totalMappings * MapSize * (ReceptiveFieldSize + 1);  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	//CalculateLocalConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		// CalculateLocalConnectedDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount)) {  					int iNumWeight = mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					int iNumWeight = mapping * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				int iNumWeight = curMap * (ReceptiveFieldSize + 1) * MapSize;  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						AddBias (ref Connections [position]' iNumWeight++);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.Convolutional:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskWidth) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.ConvolutionalSubsampling:  	// Simard's implementation  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	for (int map = 0; map < PreviousLayer.MapCount; map++)  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	if (!IsFullyMapped)// not fully mapped  	 {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.AvgPooling:  case LayerTypes.MaxPooling:  	HasWeights = true;  	WeightCount = MapCount * 2;  	Weights = new Weight[WeightCount];  	if (LayerType == LayerTypes.AvgPooling) {  		CalculateAction = CalculateAveragePooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePooling;  	}  	else {  		CalculateAction = CalculateMaxPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPooling;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  										else  											outOfBounds = false;  									}  							}  					}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  							}  					}  				}  			});  		}  	}  	break;  case LayerTypes.AvgPoolingWeightless:  case LayerTypes.MaxPoolingWeightless:  case LayerTypes.L2Pooling:  case LayerTypes.StochasticPooling:  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	switch (LayerType) {  	case LayerTypes.AvgPoolingWeightless:  		CalculateAction = CalculateAveragePoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePoolingWeightless;  		break;  	case LayerTypes.MaxPoolingWeightless:  		CalculateAction = CalculateMaxPoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPoolingWeightless;  		break;  	case LayerTypes.L2Pooling:  		CalculateAction = CalculateL2Pooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateL2PoolingParallel;  		else  			BackpropagateAction = BackpropagateL2PoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesL2Pooling;  		break;  	case LayerTypes.StochasticPooling:  		CalculateAction = CalculateStochasticPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateStochasticPoolingParallel;  		else  			BackpropagateAction = BackpropagateStochasticPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesStochasticPooling;  		break;  	}  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  										else  											outOfBounds = false;  									}  							}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  							}  				}  			});  		}  	}  	break;  case LayerTypes.FullyConnected:  	HasWeights = true;  	WeightCount = (PreviousLayer.NeuronCount + 1) * NeuronCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMax;  	else  		CalculateAction = CalculateFullyConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxParallel;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxSerial;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		Parallel.For (0' MapCount' curMap =>  {  			for (int yc = 0; yc < MapHeight; yc++)  				for (int xc = 0; xc < MapWidth; xc++) {  					int position = xc + (yc * MapWidth) + (curMap * MapSize);  					AddBias (ref Connections [position]' iNumWeight++);  					for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  						for (int y = 0; y < PreviousLayer.MapHeight; y++)  							for (int x = 0; x < PreviousLayer.MapWidth; x++)  								AddConnection (ref Connections [position]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  				}  		});  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++) {  			AddBias (ref Connections [y]' iNumWeight++);  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  		}  	}  	break;  case LayerTypes.RBF:  	HasWeights = true;  	WeightCount = PreviousLayer.NeuronCount * NeuronCount;  	// no biasses  	Weights = new Weight[WeightCount];  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMaxRBF;  	else  		CalculateAction = CalculateRBF;  	BackpropagateAction = BackpropagateRBF;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesRBF;  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		for (int n = 0; n < NeuronCount; n++)  			for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  				for (int y = 0; y < PreviousLayer.MapHeight; y++)  					for (int x = 0; x < PreviousLayer.MapWidth; x++)  						AddConnection (ref Connections [n]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++)  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  	}  	break;  case LayerTypes.LocalResponseNormalization:  	// same map  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalResponseNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalResponseNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalResponseNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalResponseNormalizationCM:  	// across maps  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztionCM layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	int size = 9;  	int a' b;  	for (int map = 0; map < MapCount; map++)  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				a = Math.Max (0' map - (size / 2));  				// from map  				b = Math.Min (MapCount' map - (size / 2) + size);  				// to map  				int position = x + (y * MapWidth) + (map * MapSize);  				for (int f = a; f < b; f++)  					AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  			}  	CalculateAction = CalculateLocalResponseNormalizationCM;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMParallel;  	else  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalizationCM;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalContrastNormalization:  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	Gaussian2DKernel = MathUtil.CreateGaussian2DKernel (ReceptiveFieldWidth' ReceptiveFieldHeight);  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalContrastNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalContrastNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalContrastNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalContrastNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.Input:  	ActivationFunctionId = ActivationFunctions.None;  	HasWeights = false;  	ActivationFunction = null;  	UseWeightPartitioner = false;  	WeightCount = 0;  	Weights = null;  	CalculateAction = null;  	BackpropagateAction = null;  	BackpropagateSecondDerivativesAction = null;  	break;  case LayerTypes.Local:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = totalMappings * MapSize * (ReceptiveFieldSize + 1);  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	//CalculateLocalConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		// CalculateLocalConnectedDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount)) {  					int iNumWeight = mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					int iNumWeight = mapping * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				int iNumWeight = curMap * (ReceptiveFieldSize + 1) * MapSize;  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						AddBias (ref Connections [position]' iNumWeight++);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.Convolutional:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskWidth) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.ConvolutionalSubsampling:  	// Simard's implementation  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	for (int map = 0; map < PreviousLayer.MapCount; map++)  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	if (!IsFullyMapped)// not fully mapped  	 {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.AvgPooling:  case LayerTypes.MaxPooling:  	HasWeights = true;  	WeightCount = MapCount * 2;  	Weights = new Weight[WeightCount];  	if (LayerType == LayerTypes.AvgPooling) {  		CalculateAction = CalculateAveragePooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePooling;  	}  	else {  		CalculateAction = CalculateMaxPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPooling;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  										else  											outOfBounds = false;  									}  							}  					}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  							}  					}  				}  			});  		}  	}  	break;  case LayerTypes.AvgPoolingWeightless:  case LayerTypes.MaxPoolingWeightless:  case LayerTypes.L2Pooling:  case LayerTypes.StochasticPooling:  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	switch (LayerType) {  	case LayerTypes.AvgPoolingWeightless:  		CalculateAction = CalculateAveragePoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePoolingWeightless;  		break;  	case LayerTypes.MaxPoolingWeightless:  		CalculateAction = CalculateMaxPoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPoolingWeightless;  		break;  	case LayerTypes.L2Pooling:  		CalculateAction = CalculateL2Pooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateL2PoolingParallel;  		else  			BackpropagateAction = BackpropagateL2PoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesL2Pooling;  		break;  	case LayerTypes.StochasticPooling:  		CalculateAction = CalculateStochasticPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateStochasticPoolingParallel;  		else  			BackpropagateAction = BackpropagateStochasticPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesStochasticPooling;  		break;  	}  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  										else  											outOfBounds = false;  									}  							}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  							}  				}  			});  		}  	}  	break;  case LayerTypes.FullyConnected:  	HasWeights = true;  	WeightCount = (PreviousLayer.NeuronCount + 1) * NeuronCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMax;  	else  		CalculateAction = CalculateFullyConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxParallel;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxSerial;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		Parallel.For (0' MapCount' curMap =>  {  			for (int yc = 0; yc < MapHeight; yc++)  				for (int xc = 0; xc < MapWidth; xc++) {  					int position = xc + (yc * MapWidth) + (curMap * MapSize);  					AddBias (ref Connections [position]' iNumWeight++);  					for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  						for (int y = 0; y < PreviousLayer.MapHeight; y++)  							for (int x = 0; x < PreviousLayer.MapWidth; x++)  								AddConnection (ref Connections [position]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  				}  		});  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++) {  			AddBias (ref Connections [y]' iNumWeight++);  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  		}  	}  	break;  case LayerTypes.RBF:  	HasWeights = true;  	WeightCount = PreviousLayer.NeuronCount * NeuronCount;  	// no biasses  	Weights = new Weight[WeightCount];  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMaxRBF;  	else  		CalculateAction = CalculateRBF;  	BackpropagateAction = BackpropagateRBF;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesRBF;  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		for (int n = 0; n < NeuronCount; n++)  			for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  				for (int y = 0; y < PreviousLayer.MapHeight; y++)  					for (int x = 0; x < PreviousLayer.MapWidth; x++)  						AddConnection (ref Connections [n]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++)  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  	}  	break;  case LayerTypes.LocalResponseNormalization:  	// same map  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalResponseNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalResponseNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalResponseNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalResponseNormalizationCM:  	// across maps  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztionCM layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	int size = 9;  	int a' b;  	for (int map = 0; map < MapCount; map++)  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				a = Math.Max (0' map - (size / 2));  				// from map  				b = Math.Min (MapCount' map - (size / 2) + size);  				// to map  				int position = x + (y * MapWidth) + (map * MapSize);  				for (int f = a; f < b; f++)  					AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  			}  	CalculateAction = CalculateLocalResponseNormalizationCM;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMParallel;  	else  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalizationCM;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalContrastNormalization:  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	Gaussian2DKernel = MathUtil.CreateGaussian2DKernel (ReceptiveFieldWidth' ReceptiveFieldHeight);  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalContrastNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalContrastNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalContrastNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalContrastNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.Input:  	ActivationFunctionId = ActivationFunctions.None;  	HasWeights = false;  	ActivationFunction = null;  	UseWeightPartitioner = false;  	WeightCount = 0;  	Weights = null;  	CalculateAction = null;  	BackpropagateAction = null;  	BackpropagateSecondDerivativesAction = null;  	break;  case LayerTypes.Local:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = totalMappings * MapSize * (ReceptiveFieldSize + 1);  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	//CalculateLocalConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		// CalculateLocalConnectedDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount)) {  					int iNumWeight = mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					int iNumWeight = mapping * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				int iNumWeight = curMap * (ReceptiveFieldSize + 1) * MapSize;  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						AddBias (ref Connections [position]' iNumWeight++);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.Convolutional:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskWidth) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.ConvolutionalSubsampling:  	// Simard's implementation  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	for (int map = 0; map < PreviousLayer.MapCount; map++)  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	if (!IsFullyMapped)// not fully mapped  	 {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.AvgPooling:  case LayerTypes.MaxPooling:  	HasWeights = true;  	WeightCount = MapCount * 2;  	Weights = new Weight[WeightCount];  	if (LayerType == LayerTypes.AvgPooling) {  		CalculateAction = CalculateAveragePooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePooling;  	}  	else {  		CalculateAction = CalculateMaxPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPooling;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  										else  											outOfBounds = false;  									}  							}  					}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  							}  					}  				}  			});  		}  	}  	break;  case LayerTypes.AvgPoolingWeightless:  case LayerTypes.MaxPoolingWeightless:  case LayerTypes.L2Pooling:  case LayerTypes.StochasticPooling:  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	switch (LayerType) {  	case LayerTypes.AvgPoolingWeightless:  		CalculateAction = CalculateAveragePoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePoolingWeightless;  		break;  	case LayerTypes.MaxPoolingWeightless:  		CalculateAction = CalculateMaxPoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPoolingWeightless;  		break;  	case LayerTypes.L2Pooling:  		CalculateAction = CalculateL2Pooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateL2PoolingParallel;  		else  			BackpropagateAction = BackpropagateL2PoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesL2Pooling;  		break;  	case LayerTypes.StochasticPooling:  		CalculateAction = CalculateStochasticPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateStochasticPoolingParallel;  		else  			BackpropagateAction = BackpropagateStochasticPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesStochasticPooling;  		break;  	}  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  										else  											outOfBounds = false;  									}  							}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  							}  				}  			});  		}  	}  	break;  case LayerTypes.FullyConnected:  	HasWeights = true;  	WeightCount = (PreviousLayer.NeuronCount + 1) * NeuronCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMax;  	else  		CalculateAction = CalculateFullyConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxParallel;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxSerial;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		Parallel.For (0' MapCount' curMap =>  {  			for (int yc = 0; yc < MapHeight; yc++)  				for (int xc = 0; xc < MapWidth; xc++) {  					int position = xc + (yc * MapWidth) + (curMap * MapSize);  					AddBias (ref Connections [position]' iNumWeight++);  					for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  						for (int y = 0; y < PreviousLayer.MapHeight; y++)  							for (int x = 0; x < PreviousLayer.MapWidth; x++)  								AddConnection (ref Connections [position]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  				}  		});  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++) {  			AddBias (ref Connections [y]' iNumWeight++);  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  		}  	}  	break;  case LayerTypes.RBF:  	HasWeights = true;  	WeightCount = PreviousLayer.NeuronCount * NeuronCount;  	// no biasses  	Weights = new Weight[WeightCount];  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMaxRBF;  	else  		CalculateAction = CalculateRBF;  	BackpropagateAction = BackpropagateRBF;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesRBF;  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		for (int n = 0; n < NeuronCount; n++)  			for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  				for (int y = 0; y < PreviousLayer.MapHeight; y++)  					for (int x = 0; x < PreviousLayer.MapWidth; x++)  						AddConnection (ref Connections [n]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++)  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  	}  	break;  case LayerTypes.LocalResponseNormalization:  	// same map  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalResponseNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalResponseNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalResponseNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalResponseNormalizationCM:  	// across maps  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztionCM layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	int size = 9;  	int a' b;  	for (int map = 0; map < MapCount; map++)  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				a = Math.Max (0' map - (size / 2));  				// from map  				b = Math.Min (MapCount' map - (size / 2) + size);  				// to map  				int position = x + (y * MapWidth) + (map * MapSize);  				for (int f = a; f < b; f++)  					AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  			}  	CalculateAction = CalculateLocalResponseNormalizationCM;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMParallel;  	else  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalizationCM;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalContrastNormalization:  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	Gaussian2DKernel = MathUtil.CreateGaussian2DKernel (ReceptiveFieldWidth' ReceptiveFieldHeight);  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalContrastNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalContrastNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalContrastNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalContrastNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.Input:  	ActivationFunctionId = ActivationFunctions.None;  	HasWeights = false;  	ActivationFunction = null;  	UseWeightPartitioner = false;  	WeightCount = 0;  	Weights = null;  	CalculateAction = null;  	BackpropagateAction = null;  	BackpropagateSecondDerivativesAction = null;  	break;  case LayerTypes.Local:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = totalMappings * MapSize * (ReceptiveFieldSize + 1);  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	//CalculateLocalConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		// CalculateLocalConnectedDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount)) {  					int iNumWeight = mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					int iNumWeight = mapping * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				int iNumWeight = curMap * (ReceptiveFieldSize + 1) * MapSize;  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						AddBias (ref Connections [position]' iNumWeight++);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.Convolutional:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskWidth) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.ConvolutionalSubsampling:  	// Simard's implementation  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	for (int map = 0; map < PreviousLayer.MapCount; map++)  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	if (!IsFullyMapped)// not fully mapped  	 {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.AvgPooling:  case LayerTypes.MaxPooling:  	HasWeights = true;  	WeightCount = MapCount * 2;  	Weights = new Weight[WeightCount];  	if (LayerType == LayerTypes.AvgPooling) {  		CalculateAction = CalculateAveragePooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePooling;  	}  	else {  		CalculateAction = CalculateMaxPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPooling;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  										else  											outOfBounds = false;  									}  							}  					}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  							}  					}  				}  			});  		}  	}  	break;  case LayerTypes.AvgPoolingWeightless:  case LayerTypes.MaxPoolingWeightless:  case LayerTypes.L2Pooling:  case LayerTypes.StochasticPooling:  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	switch (LayerType) {  	case LayerTypes.AvgPoolingWeightless:  		CalculateAction = CalculateAveragePoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePoolingWeightless;  		break;  	case LayerTypes.MaxPoolingWeightless:  		CalculateAction = CalculateMaxPoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPoolingWeightless;  		break;  	case LayerTypes.L2Pooling:  		CalculateAction = CalculateL2Pooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateL2PoolingParallel;  		else  			BackpropagateAction = BackpropagateL2PoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesL2Pooling;  		break;  	case LayerTypes.StochasticPooling:  		CalculateAction = CalculateStochasticPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateStochasticPoolingParallel;  		else  			BackpropagateAction = BackpropagateStochasticPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesStochasticPooling;  		break;  	}  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  										else  											outOfBounds = false;  									}  							}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  							}  				}  			});  		}  	}  	break;  case LayerTypes.FullyConnected:  	HasWeights = true;  	WeightCount = (PreviousLayer.NeuronCount + 1) * NeuronCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMax;  	else  		CalculateAction = CalculateFullyConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxParallel;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxSerial;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		Parallel.For (0' MapCount' curMap =>  {  			for (int yc = 0; yc < MapHeight; yc++)  				for (int xc = 0; xc < MapWidth; xc++) {  					int position = xc + (yc * MapWidth) + (curMap * MapSize);  					AddBias (ref Connections [position]' iNumWeight++);  					for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  						for (int y = 0; y < PreviousLayer.MapHeight; y++)  							for (int x = 0; x < PreviousLayer.MapWidth; x++)  								AddConnection (ref Connections [position]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  				}  		});  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++) {  			AddBias (ref Connections [y]' iNumWeight++);  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  		}  	}  	break;  case LayerTypes.RBF:  	HasWeights = true;  	WeightCount = PreviousLayer.NeuronCount * NeuronCount;  	// no biasses  	Weights = new Weight[WeightCount];  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMaxRBF;  	else  		CalculateAction = CalculateRBF;  	BackpropagateAction = BackpropagateRBF;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesRBF;  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		for (int n = 0; n < NeuronCount; n++)  			for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  				for (int y = 0; y < PreviousLayer.MapHeight; y++)  					for (int x = 0; x < PreviousLayer.MapWidth; x++)  						AddConnection (ref Connections [n]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++)  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  	}  	break;  case LayerTypes.LocalResponseNormalization:  	// same map  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalResponseNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalResponseNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalResponseNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalResponseNormalizationCM:  	// across maps  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztionCM layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	int size = 9;  	int a' b;  	for (int map = 0; map < MapCount; map++)  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				a = Math.Max (0' map - (size / 2));  				// from map  				b = Math.Min (MapCount' map - (size / 2) + size);  				// to map  				int position = x + (y * MapWidth) + (map * MapSize);  				for (int f = a; f < b; f++)  					AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  			}  	CalculateAction = CalculateLocalResponseNormalizationCM;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMParallel;  	else  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalizationCM;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalContrastNormalization:  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	Gaussian2DKernel = MathUtil.CreateGaussian2DKernel (ReceptiveFieldWidth' ReceptiveFieldHeight);  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalContrastNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalContrastNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalContrastNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalContrastNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.Input:  	ActivationFunctionId = ActivationFunctions.None;  	HasWeights = false;  	ActivationFunction = null;  	UseWeightPartitioner = false;  	WeightCount = 0;  	Weights = null;  	CalculateAction = null;  	BackpropagateAction = null;  	BackpropagateSecondDerivativesAction = null;  	break;  case LayerTypes.Local:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = totalMappings * MapSize * (ReceptiveFieldSize + 1);  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	//CalculateLocalConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		// CalculateLocalConnectedDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount)) {  					int iNumWeight = mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					int iNumWeight = mapping * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				int iNumWeight = curMap * (ReceptiveFieldSize + 1) * MapSize;  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						AddBias (ref Connections [position]' iNumWeight++);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.Convolutional:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskWidth) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.ConvolutionalSubsampling:  	// Simard's implementation  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	for (int map = 0; map < PreviousLayer.MapCount; map++)  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	if (!IsFullyMapped)// not fully mapped  	 {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.AvgPooling:  case LayerTypes.MaxPooling:  	HasWeights = true;  	WeightCount = MapCount * 2;  	Weights = new Weight[WeightCount];  	if (LayerType == LayerTypes.AvgPooling) {  		CalculateAction = CalculateAveragePooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePooling;  	}  	else {  		CalculateAction = CalculateMaxPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPooling;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  										else  											outOfBounds = false;  									}  							}  					}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  							}  					}  				}  			});  		}  	}  	break;  case LayerTypes.AvgPoolingWeightless:  case LayerTypes.MaxPoolingWeightless:  case LayerTypes.L2Pooling:  case LayerTypes.StochasticPooling:  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	switch (LayerType) {  	case LayerTypes.AvgPoolingWeightless:  		CalculateAction = CalculateAveragePoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePoolingWeightless;  		break;  	case LayerTypes.MaxPoolingWeightless:  		CalculateAction = CalculateMaxPoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPoolingWeightless;  		break;  	case LayerTypes.L2Pooling:  		CalculateAction = CalculateL2Pooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateL2PoolingParallel;  		else  			BackpropagateAction = BackpropagateL2PoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesL2Pooling;  		break;  	case LayerTypes.StochasticPooling:  		CalculateAction = CalculateStochasticPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateStochasticPoolingParallel;  		else  			BackpropagateAction = BackpropagateStochasticPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesStochasticPooling;  		break;  	}  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  										else  											outOfBounds = false;  									}  							}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  							}  				}  			});  		}  	}  	break;  case LayerTypes.FullyConnected:  	HasWeights = true;  	WeightCount = (PreviousLayer.NeuronCount + 1) * NeuronCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMax;  	else  		CalculateAction = CalculateFullyConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxParallel;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxSerial;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		Parallel.For (0' MapCount' curMap =>  {  			for (int yc = 0; yc < MapHeight; yc++)  				for (int xc = 0; xc < MapWidth; xc++) {  					int position = xc + (yc * MapWidth) + (curMap * MapSize);  					AddBias (ref Connections [position]' iNumWeight++);  					for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  						for (int y = 0; y < PreviousLayer.MapHeight; y++)  							for (int x = 0; x < PreviousLayer.MapWidth; x++)  								AddConnection (ref Connections [position]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  				}  		});  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++) {  			AddBias (ref Connections [y]' iNumWeight++);  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  		}  	}  	break;  case LayerTypes.RBF:  	HasWeights = true;  	WeightCount = PreviousLayer.NeuronCount * NeuronCount;  	// no biasses  	Weights = new Weight[WeightCount];  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMaxRBF;  	else  		CalculateAction = CalculateRBF;  	BackpropagateAction = BackpropagateRBF;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesRBF;  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		for (int n = 0; n < NeuronCount; n++)  			for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  				for (int y = 0; y < PreviousLayer.MapHeight; y++)  					for (int x = 0; x < PreviousLayer.MapWidth; x++)  						AddConnection (ref Connections [n]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++)  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  	}  	break;  case LayerTypes.LocalResponseNormalization:  	// same map  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalResponseNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalResponseNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalResponseNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalResponseNormalizationCM:  	// across maps  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztionCM layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	int size = 9;  	int a' b;  	for (int map = 0; map < MapCount; map++)  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				a = Math.Max (0' map - (size / 2));  				// from map  				b = Math.Min (MapCount' map - (size / 2) + size);  				// to map  				int position = x + (y * MapWidth) + (map * MapSize);  				for (int f = a; f < b; f++)  					AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  			}  	CalculateAction = CalculateLocalResponseNormalizationCM;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMParallel;  	else  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalizationCM;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalContrastNormalization:  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	Gaussian2DKernel = MathUtil.CreateGaussian2DKernel (ReceptiveFieldWidth' ReceptiveFieldHeight);  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalContrastNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalContrastNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalContrastNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalContrastNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.Input:  	ActivationFunctionId = ActivationFunctions.None;  	HasWeights = false;  	ActivationFunction = null;  	UseWeightPartitioner = false;  	WeightCount = 0;  	Weights = null;  	CalculateAction = null;  	BackpropagateAction = null;  	BackpropagateSecondDerivativesAction = null;  	break;  case LayerTypes.Local:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = totalMappings * MapSize * (ReceptiveFieldSize + 1);  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	//CalculateLocalConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		// CalculateLocalConnectedDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount)) {  					int iNumWeight = mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					int iNumWeight = mapping * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				int iNumWeight = curMap * (ReceptiveFieldSize + 1) * MapSize;  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						AddBias (ref Connections [position]' iNumWeight++);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.Convolutional:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskWidth) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.ConvolutionalSubsampling:  	// Simard's implementation  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	for (int map = 0; map < PreviousLayer.MapCount; map++)  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	if (!IsFullyMapped)// not fully mapped  	 {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.AvgPooling:  case LayerTypes.MaxPooling:  	HasWeights = true;  	WeightCount = MapCount * 2;  	Weights = new Weight[WeightCount];  	if (LayerType == LayerTypes.AvgPooling) {  		CalculateAction = CalculateAveragePooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePooling;  	}  	else {  		CalculateAction = CalculateMaxPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPooling;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  										else  											outOfBounds = false;  									}  							}  					}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  							}  					}  				}  			});  		}  	}  	break;  case LayerTypes.AvgPoolingWeightless:  case LayerTypes.MaxPoolingWeightless:  case LayerTypes.L2Pooling:  case LayerTypes.StochasticPooling:  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	switch (LayerType) {  	case LayerTypes.AvgPoolingWeightless:  		CalculateAction = CalculateAveragePoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePoolingWeightless;  		break;  	case LayerTypes.MaxPoolingWeightless:  		CalculateAction = CalculateMaxPoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPoolingWeightless;  		break;  	case LayerTypes.L2Pooling:  		CalculateAction = CalculateL2Pooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateL2PoolingParallel;  		else  			BackpropagateAction = BackpropagateL2PoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesL2Pooling;  		break;  	case LayerTypes.StochasticPooling:  		CalculateAction = CalculateStochasticPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateStochasticPoolingParallel;  		else  			BackpropagateAction = BackpropagateStochasticPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesStochasticPooling;  		break;  	}  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  										else  											outOfBounds = false;  									}  							}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  							}  				}  			});  		}  	}  	break;  case LayerTypes.FullyConnected:  	HasWeights = true;  	WeightCount = (PreviousLayer.NeuronCount + 1) * NeuronCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMax;  	else  		CalculateAction = CalculateFullyConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxParallel;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxSerial;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		Parallel.For (0' MapCount' curMap =>  {  			for (int yc = 0; yc < MapHeight; yc++)  				for (int xc = 0; xc < MapWidth; xc++) {  					int position = xc + (yc * MapWidth) + (curMap * MapSize);  					AddBias (ref Connections [position]' iNumWeight++);  					for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  						for (int y = 0; y < PreviousLayer.MapHeight; y++)  							for (int x = 0; x < PreviousLayer.MapWidth; x++)  								AddConnection (ref Connections [position]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  				}  		});  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++) {  			AddBias (ref Connections [y]' iNumWeight++);  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  		}  	}  	break;  case LayerTypes.RBF:  	HasWeights = true;  	WeightCount = PreviousLayer.NeuronCount * NeuronCount;  	// no biasses  	Weights = new Weight[WeightCount];  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMaxRBF;  	else  		CalculateAction = CalculateRBF;  	BackpropagateAction = BackpropagateRBF;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesRBF;  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		for (int n = 0; n < NeuronCount; n++)  			for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  				for (int y = 0; y < PreviousLayer.MapHeight; y++)  					for (int x = 0; x < PreviousLayer.MapWidth; x++)  						AddConnection (ref Connections [n]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++)  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  	}  	break;  case LayerTypes.LocalResponseNormalization:  	// same map  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalResponseNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalResponseNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalResponseNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalResponseNormalizationCM:  	// across maps  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztionCM layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	int size = 9;  	int a' b;  	for (int map = 0; map < MapCount; map++)  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				a = Math.Max (0' map - (size / 2));  				// from map  				b = Math.Min (MapCount' map - (size / 2) + size);  				// to map  				int position = x + (y * MapWidth) + (map * MapSize);  				for (int f = a; f < b; f++)  					AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  			}  	CalculateAction = CalculateLocalResponseNormalizationCM;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMParallel;  	else  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalizationCM;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalContrastNormalization:  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	Gaussian2DKernel = MathUtil.CreateGaussian2DKernel (ReceptiveFieldWidth' ReceptiveFieldHeight);  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalContrastNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalContrastNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalContrastNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalContrastNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.Input:  	ActivationFunctionId = ActivationFunctions.None;  	HasWeights = false;  	ActivationFunction = null;  	UseWeightPartitioner = false;  	WeightCount = 0;  	Weights = null;  	CalculateAction = null;  	BackpropagateAction = null;  	BackpropagateSecondDerivativesAction = null;  	break;  case LayerTypes.Local:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = totalMappings * MapSize * (ReceptiveFieldSize + 1);  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	//CalculateLocalConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		// CalculateLocalConnectedDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount)) {  					int iNumWeight = mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					int iNumWeight = mapping * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				int iNumWeight = curMap * (ReceptiveFieldSize + 1) * MapSize;  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						AddBias (ref Connections [position]' iNumWeight++);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.Convolutional:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskWidth) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.ConvolutionalSubsampling:  	// Simard's implementation  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	for (int map = 0; map < PreviousLayer.MapCount; map++)  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	if (!IsFullyMapped)// not fully mapped  	 {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.AvgPooling:  case LayerTypes.MaxPooling:  	HasWeights = true;  	WeightCount = MapCount * 2;  	Weights = new Weight[WeightCount];  	if (LayerType == LayerTypes.AvgPooling) {  		CalculateAction = CalculateAveragePooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePooling;  	}  	else {  		CalculateAction = CalculateMaxPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPooling;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  										else  											outOfBounds = false;  									}  							}  					}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  							}  					}  				}  			});  		}  	}  	break;  case LayerTypes.AvgPoolingWeightless:  case LayerTypes.MaxPoolingWeightless:  case LayerTypes.L2Pooling:  case LayerTypes.StochasticPooling:  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	switch (LayerType) {  	case LayerTypes.AvgPoolingWeightless:  		CalculateAction = CalculateAveragePoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePoolingWeightless;  		break;  	case LayerTypes.MaxPoolingWeightless:  		CalculateAction = CalculateMaxPoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPoolingWeightless;  		break;  	case LayerTypes.L2Pooling:  		CalculateAction = CalculateL2Pooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateL2PoolingParallel;  		else  			BackpropagateAction = BackpropagateL2PoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesL2Pooling;  		break;  	case LayerTypes.StochasticPooling:  		CalculateAction = CalculateStochasticPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateStochasticPoolingParallel;  		else  			BackpropagateAction = BackpropagateStochasticPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesStochasticPooling;  		break;  	}  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  										else  											outOfBounds = false;  									}  							}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  							}  				}  			});  		}  	}  	break;  case LayerTypes.FullyConnected:  	HasWeights = true;  	WeightCount = (PreviousLayer.NeuronCount + 1) * NeuronCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMax;  	else  		CalculateAction = CalculateFullyConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxParallel;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxSerial;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		Parallel.For (0' MapCount' curMap =>  {  			for (int yc = 0; yc < MapHeight; yc++)  				for (int xc = 0; xc < MapWidth; xc++) {  					int position = xc + (yc * MapWidth) + (curMap * MapSize);  					AddBias (ref Connections [position]' iNumWeight++);  					for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  						for (int y = 0; y < PreviousLayer.MapHeight; y++)  							for (int x = 0; x < PreviousLayer.MapWidth; x++)  								AddConnection (ref Connections [position]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  				}  		});  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++) {  			AddBias (ref Connections [y]' iNumWeight++);  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  		}  	}  	break;  case LayerTypes.RBF:  	HasWeights = true;  	WeightCount = PreviousLayer.NeuronCount * NeuronCount;  	// no biasses  	Weights = new Weight[WeightCount];  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMaxRBF;  	else  		CalculateAction = CalculateRBF;  	BackpropagateAction = BackpropagateRBF;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesRBF;  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		for (int n = 0; n < NeuronCount; n++)  			for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  				for (int y = 0; y < PreviousLayer.MapHeight; y++)  					for (int x = 0; x < PreviousLayer.MapWidth; x++)  						AddConnection (ref Connections [n]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++)  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  	}  	break;  case LayerTypes.LocalResponseNormalization:  	// same map  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalResponseNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalResponseNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalResponseNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalResponseNormalizationCM:  	// across maps  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztionCM layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	int size = 9;  	int a' b;  	for (int map = 0; map < MapCount; map++)  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				a = Math.Max (0' map - (size / 2));  				// from map  				b = Math.Min (MapCount' map - (size / 2) + size);  				// to map  				int position = x + (y * MapWidth) + (map * MapSize);  				for (int f = a; f < b; f++)  					AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  			}  	CalculateAction = CalculateLocalResponseNormalizationCM;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMParallel;  	else  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalizationCM;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalContrastNormalization:  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	Gaussian2DKernel = MathUtil.CreateGaussian2DKernel (ReceptiveFieldWidth' ReceptiveFieldHeight);  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalContrastNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalContrastNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalContrastNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalContrastNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.Input:  	ActivationFunctionId = ActivationFunctions.None;  	HasWeights = false;  	ActivationFunction = null;  	UseWeightPartitioner = false;  	WeightCount = 0;  	Weights = null;  	CalculateAction = null;  	BackpropagateAction = null;  	BackpropagateSecondDerivativesAction = null;  	break;  case LayerTypes.Local:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = totalMappings * MapSize * (ReceptiveFieldSize + 1);  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	//CalculateLocalConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		// CalculateLocalConnectedDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount)) {  					int iNumWeight = mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					int iNumWeight = mapping * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				int iNumWeight = curMap * (ReceptiveFieldSize + 1) * MapSize;  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						AddBias (ref Connections [position]' iNumWeight++);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.Convolutional:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskWidth) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.ConvolutionalSubsampling:  	// Simard's implementation  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	for (int map = 0; map < PreviousLayer.MapCount; map++)  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	if (!IsFullyMapped)// not fully mapped  	 {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.AvgPooling:  case LayerTypes.MaxPooling:  	HasWeights = true;  	WeightCount = MapCount * 2;  	Weights = new Weight[WeightCount];  	if (LayerType == LayerTypes.AvgPooling) {  		CalculateAction = CalculateAveragePooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePooling;  	}  	else {  		CalculateAction = CalculateMaxPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPooling;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  										else  											outOfBounds = false;  									}  							}  					}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  							}  					}  				}  			});  		}  	}  	break;  case LayerTypes.AvgPoolingWeightless:  case LayerTypes.MaxPoolingWeightless:  case LayerTypes.L2Pooling:  case LayerTypes.StochasticPooling:  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	switch (LayerType) {  	case LayerTypes.AvgPoolingWeightless:  		CalculateAction = CalculateAveragePoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePoolingWeightless;  		break;  	case LayerTypes.MaxPoolingWeightless:  		CalculateAction = CalculateMaxPoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPoolingWeightless;  		break;  	case LayerTypes.L2Pooling:  		CalculateAction = CalculateL2Pooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateL2PoolingParallel;  		else  			BackpropagateAction = BackpropagateL2PoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesL2Pooling;  		break;  	case LayerTypes.StochasticPooling:  		CalculateAction = CalculateStochasticPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateStochasticPoolingParallel;  		else  			BackpropagateAction = BackpropagateStochasticPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesStochasticPooling;  		break;  	}  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  										else  											outOfBounds = false;  									}  							}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  							}  				}  			});  		}  	}  	break;  case LayerTypes.FullyConnected:  	HasWeights = true;  	WeightCount = (PreviousLayer.NeuronCount + 1) * NeuronCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMax;  	else  		CalculateAction = CalculateFullyConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxParallel;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxSerial;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		Parallel.For (0' MapCount' curMap =>  {  			for (int yc = 0; yc < MapHeight; yc++)  				for (int xc = 0; xc < MapWidth; xc++) {  					int position = xc + (yc * MapWidth) + (curMap * MapSize);  					AddBias (ref Connections [position]' iNumWeight++);  					for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  						for (int y = 0; y < PreviousLayer.MapHeight; y++)  							for (int x = 0; x < PreviousLayer.MapWidth; x++)  								AddConnection (ref Connections [position]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  				}  		});  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++) {  			AddBias (ref Connections [y]' iNumWeight++);  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  		}  	}  	break;  case LayerTypes.RBF:  	HasWeights = true;  	WeightCount = PreviousLayer.NeuronCount * NeuronCount;  	// no biasses  	Weights = new Weight[WeightCount];  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMaxRBF;  	else  		CalculateAction = CalculateRBF;  	BackpropagateAction = BackpropagateRBF;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesRBF;  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		for (int n = 0; n < NeuronCount; n++)  			for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  				for (int y = 0; y < PreviousLayer.MapHeight; y++)  					for (int x = 0; x < PreviousLayer.MapWidth; x++)  						AddConnection (ref Connections [n]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++)  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  	}  	break;  case LayerTypes.LocalResponseNormalization:  	// same map  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalResponseNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalResponseNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalResponseNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalResponseNormalizationCM:  	// across maps  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztionCM layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	int size = 9;  	int a' b;  	for (int map = 0; map < MapCount; map++)  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				a = Math.Max (0' map - (size / 2));  				// from map  				b = Math.Min (MapCount' map - (size / 2) + size);  				// to map  				int position = x + (y * MapWidth) + (map * MapSize);  				for (int f = a; f < b; f++)  					AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  			}  	CalculateAction = CalculateLocalResponseNormalizationCM;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMParallel;  	else  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalizationCM;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalContrastNormalization:  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	Gaussian2DKernel = MathUtil.CreateGaussian2DKernel (ReceptiveFieldWidth' ReceptiveFieldHeight);  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalContrastNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalContrastNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalContrastNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalContrastNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.Input:  	ActivationFunctionId = ActivationFunctions.None;  	HasWeights = false;  	ActivationFunction = null;  	UseWeightPartitioner = false;  	WeightCount = 0;  	Weights = null;  	CalculateAction = null;  	BackpropagateAction = null;  	BackpropagateSecondDerivativesAction = null;  	break;  case LayerTypes.Local:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = totalMappings * MapSize * (ReceptiveFieldSize + 1);  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	//CalculateLocalConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		// CalculateLocalConnectedDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount)) {  					int iNumWeight = mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					int iNumWeight = mapping * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				int iNumWeight = curMap * (ReceptiveFieldSize + 1) * MapSize;  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						AddBias (ref Connections [position]' iNumWeight++);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.Convolutional:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskWidth) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.ConvolutionalSubsampling:  	// Simard's implementation  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	for (int map = 0; map < PreviousLayer.MapCount; map++)  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	if (!IsFullyMapped)// not fully mapped  	 {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.AvgPooling:  case LayerTypes.MaxPooling:  	HasWeights = true;  	WeightCount = MapCount * 2;  	Weights = new Weight[WeightCount];  	if (LayerType == LayerTypes.AvgPooling) {  		CalculateAction = CalculateAveragePooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePooling;  	}  	else {  		CalculateAction = CalculateMaxPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPooling;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  										else  											outOfBounds = false;  									}  							}  					}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  							}  					}  				}  			});  		}  	}  	break;  case LayerTypes.AvgPoolingWeightless:  case LayerTypes.MaxPoolingWeightless:  case LayerTypes.L2Pooling:  case LayerTypes.StochasticPooling:  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	switch (LayerType) {  	case LayerTypes.AvgPoolingWeightless:  		CalculateAction = CalculateAveragePoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePoolingWeightless;  		break;  	case LayerTypes.MaxPoolingWeightless:  		CalculateAction = CalculateMaxPoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPoolingWeightless;  		break;  	case LayerTypes.L2Pooling:  		CalculateAction = CalculateL2Pooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateL2PoolingParallel;  		else  			BackpropagateAction = BackpropagateL2PoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesL2Pooling;  		break;  	case LayerTypes.StochasticPooling:  		CalculateAction = CalculateStochasticPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateStochasticPoolingParallel;  		else  			BackpropagateAction = BackpropagateStochasticPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesStochasticPooling;  		break;  	}  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  										else  											outOfBounds = false;  									}  							}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  							}  				}  			});  		}  	}  	break;  case LayerTypes.FullyConnected:  	HasWeights = true;  	WeightCount = (PreviousLayer.NeuronCount + 1) * NeuronCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMax;  	else  		CalculateAction = CalculateFullyConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxParallel;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxSerial;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		Parallel.For (0' MapCount' curMap =>  {  			for (int yc = 0; yc < MapHeight; yc++)  				for (int xc = 0; xc < MapWidth; xc++) {  					int position = xc + (yc * MapWidth) + (curMap * MapSize);  					AddBias (ref Connections [position]' iNumWeight++);  					for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  						for (int y = 0; y < PreviousLayer.MapHeight; y++)  							for (int x = 0; x < PreviousLayer.MapWidth; x++)  								AddConnection (ref Connections [position]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  				}  		});  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++) {  			AddBias (ref Connections [y]' iNumWeight++);  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  		}  	}  	break;  case LayerTypes.RBF:  	HasWeights = true;  	WeightCount = PreviousLayer.NeuronCount * NeuronCount;  	// no biasses  	Weights = new Weight[WeightCount];  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMaxRBF;  	else  		CalculateAction = CalculateRBF;  	BackpropagateAction = BackpropagateRBF;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesRBF;  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		for (int n = 0; n < NeuronCount; n++)  			for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  				for (int y = 0; y < PreviousLayer.MapHeight; y++)  					for (int x = 0; x < PreviousLayer.MapWidth; x++)  						AddConnection (ref Connections [n]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++)  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  	}  	break;  case LayerTypes.LocalResponseNormalization:  	// same map  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalResponseNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalResponseNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalResponseNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalResponseNormalizationCM:  	// across maps  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztionCM layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	int size = 9;  	int a' b;  	for (int map = 0; map < MapCount; map++)  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				a = Math.Max (0' map - (size / 2));  				// from map  				b = Math.Min (MapCount' map - (size / 2) + size);  				// to map  				int position = x + (y * MapWidth) + (map * MapSize);  				for (int f = a; f < b; f++)  					AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  			}  	CalculateAction = CalculateLocalResponseNormalizationCM;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMParallel;  	else  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalizationCM;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalContrastNormalization:  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	Gaussian2DKernel = MathUtil.CreateGaussian2DKernel (ReceptiveFieldWidth' ReceptiveFieldHeight);  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalContrastNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalContrastNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalContrastNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalContrastNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.Input:  	ActivationFunctionId = ActivationFunctions.None;  	HasWeights = false;  	ActivationFunction = null;  	UseWeightPartitioner = false;  	WeightCount = 0;  	Weights = null;  	CalculateAction = null;  	BackpropagateAction = null;  	BackpropagateSecondDerivativesAction = null;  	break;  case LayerTypes.Local:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = totalMappings * MapSize * (ReceptiveFieldSize + 1);  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	//CalculateLocalConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		// CalculateLocalConnectedDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount)) {  					int iNumWeight = mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					int iNumWeight = mapping * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				int iNumWeight = curMap * (ReceptiveFieldSize + 1) * MapSize;  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						AddBias (ref Connections [position]' iNumWeight++);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.Convolutional:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskWidth) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.ConvolutionalSubsampling:  	// Simard's implementation  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	for (int map = 0; map < PreviousLayer.MapCount; map++)  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	if (!IsFullyMapped)// not fully mapped  	 {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.AvgPooling:  case LayerTypes.MaxPooling:  	HasWeights = true;  	WeightCount = MapCount * 2;  	Weights = new Weight[WeightCount];  	if (LayerType == LayerTypes.AvgPooling) {  		CalculateAction = CalculateAveragePooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePooling;  	}  	else {  		CalculateAction = CalculateMaxPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPooling;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  										else  											outOfBounds = false;  									}  							}  					}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  							}  					}  				}  			});  		}  	}  	break;  case LayerTypes.AvgPoolingWeightless:  case LayerTypes.MaxPoolingWeightless:  case LayerTypes.L2Pooling:  case LayerTypes.StochasticPooling:  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	switch (LayerType) {  	case LayerTypes.AvgPoolingWeightless:  		CalculateAction = CalculateAveragePoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePoolingWeightless;  		break;  	case LayerTypes.MaxPoolingWeightless:  		CalculateAction = CalculateMaxPoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPoolingWeightless;  		break;  	case LayerTypes.L2Pooling:  		CalculateAction = CalculateL2Pooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateL2PoolingParallel;  		else  			BackpropagateAction = BackpropagateL2PoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesL2Pooling;  		break;  	case LayerTypes.StochasticPooling:  		CalculateAction = CalculateStochasticPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateStochasticPoolingParallel;  		else  			BackpropagateAction = BackpropagateStochasticPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesStochasticPooling;  		break;  	}  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  										else  											outOfBounds = false;  									}  							}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  							}  				}  			});  		}  	}  	break;  case LayerTypes.FullyConnected:  	HasWeights = true;  	WeightCount = (PreviousLayer.NeuronCount + 1) * NeuronCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMax;  	else  		CalculateAction = CalculateFullyConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxParallel;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxSerial;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		Parallel.For (0' MapCount' curMap =>  {  			for (int yc = 0; yc < MapHeight; yc++)  				for (int xc = 0; xc < MapWidth; xc++) {  					int position = xc + (yc * MapWidth) + (curMap * MapSize);  					AddBias (ref Connections [position]' iNumWeight++);  					for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  						for (int y = 0; y < PreviousLayer.MapHeight; y++)  							for (int x = 0; x < PreviousLayer.MapWidth; x++)  								AddConnection (ref Connections [position]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  				}  		});  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++) {  			AddBias (ref Connections [y]' iNumWeight++);  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  		}  	}  	break;  case LayerTypes.RBF:  	HasWeights = true;  	WeightCount = PreviousLayer.NeuronCount * NeuronCount;  	// no biasses  	Weights = new Weight[WeightCount];  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMaxRBF;  	else  		CalculateAction = CalculateRBF;  	BackpropagateAction = BackpropagateRBF;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesRBF;  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		for (int n = 0; n < NeuronCount; n++)  			for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  				for (int y = 0; y < PreviousLayer.MapHeight; y++)  					for (int x = 0; x < PreviousLayer.MapWidth; x++)  						AddConnection (ref Connections [n]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++)  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  	}  	break;  case LayerTypes.LocalResponseNormalization:  	// same map  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalResponseNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalResponseNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalResponseNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalResponseNormalizationCM:  	// across maps  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztionCM layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	int size = 9;  	int a' b;  	for (int map = 0; map < MapCount; map++)  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				a = Math.Max (0' map - (size / 2));  				// from map  				b = Math.Min (MapCount' map - (size / 2) + size);  				// to map  				int position = x + (y * MapWidth) + (map * MapSize);  				for (int f = a; f < b; f++)  					AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  			}  	CalculateAction = CalculateLocalResponseNormalizationCM;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMParallel;  	else  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalizationCM;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalContrastNormalization:  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	Gaussian2DKernel = MathUtil.CreateGaussian2DKernel (ReceptiveFieldWidth' ReceptiveFieldHeight);  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalContrastNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalContrastNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalContrastNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalContrastNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.Input:  	ActivationFunctionId = ActivationFunctions.None;  	HasWeights = false;  	ActivationFunction = null;  	UseWeightPartitioner = false;  	WeightCount = 0;  	Weights = null;  	CalculateAction = null;  	BackpropagateAction = null;  	BackpropagateSecondDerivativesAction = null;  	break;  case LayerTypes.Local:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = totalMappings * MapSize * (ReceptiveFieldSize + 1);  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	//CalculateLocalConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		// CalculateLocalConnectedDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount)) {  					int iNumWeight = mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					int iNumWeight = mapping * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				int iNumWeight = curMap * (ReceptiveFieldSize + 1) * MapSize;  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						AddBias (ref Connections [position]' iNumWeight++);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.Convolutional:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskWidth) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.ConvolutionalSubsampling:  	// Simard's implementation  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	for (int map = 0; map < PreviousLayer.MapCount; map++)  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	if (!IsFullyMapped)// not fully mapped  	 {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.AvgPooling:  case LayerTypes.MaxPooling:  	HasWeights = true;  	WeightCount = MapCount * 2;  	Weights = new Weight[WeightCount];  	if (LayerType == LayerTypes.AvgPooling) {  		CalculateAction = CalculateAveragePooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePooling;  	}  	else {  		CalculateAction = CalculateMaxPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPooling;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  										else  											outOfBounds = false;  									}  							}  					}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  							}  					}  				}  			});  		}  	}  	break;  case LayerTypes.AvgPoolingWeightless:  case LayerTypes.MaxPoolingWeightless:  case LayerTypes.L2Pooling:  case LayerTypes.StochasticPooling:  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	switch (LayerType) {  	case LayerTypes.AvgPoolingWeightless:  		CalculateAction = CalculateAveragePoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePoolingWeightless;  		break;  	case LayerTypes.MaxPoolingWeightless:  		CalculateAction = CalculateMaxPoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPoolingWeightless;  		break;  	case LayerTypes.L2Pooling:  		CalculateAction = CalculateL2Pooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateL2PoolingParallel;  		else  			BackpropagateAction = BackpropagateL2PoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesL2Pooling;  		break;  	case LayerTypes.StochasticPooling:  		CalculateAction = CalculateStochasticPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateStochasticPoolingParallel;  		else  			BackpropagateAction = BackpropagateStochasticPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesStochasticPooling;  		break;  	}  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  										else  											outOfBounds = false;  									}  							}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  							}  				}  			});  		}  	}  	break;  case LayerTypes.FullyConnected:  	HasWeights = true;  	WeightCount = (PreviousLayer.NeuronCount + 1) * NeuronCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMax;  	else  		CalculateAction = CalculateFullyConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxParallel;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxSerial;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		Parallel.For (0' MapCount' curMap =>  {  			for (int yc = 0; yc < MapHeight; yc++)  				for (int xc = 0; xc < MapWidth; xc++) {  					int position = xc + (yc * MapWidth) + (curMap * MapSize);  					AddBias (ref Connections [position]' iNumWeight++);  					for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  						for (int y = 0; y < PreviousLayer.MapHeight; y++)  							for (int x = 0; x < PreviousLayer.MapWidth; x++)  								AddConnection (ref Connections [position]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  				}  		});  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++) {  			AddBias (ref Connections [y]' iNumWeight++);  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  		}  	}  	break;  case LayerTypes.RBF:  	HasWeights = true;  	WeightCount = PreviousLayer.NeuronCount * NeuronCount;  	// no biasses  	Weights = new Weight[WeightCount];  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMaxRBF;  	else  		CalculateAction = CalculateRBF;  	BackpropagateAction = BackpropagateRBF;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesRBF;  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		for (int n = 0; n < NeuronCount; n++)  			for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  				for (int y = 0; y < PreviousLayer.MapHeight; y++)  					for (int x = 0; x < PreviousLayer.MapWidth; x++)  						AddConnection (ref Connections [n]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++)  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  	}  	break;  case LayerTypes.LocalResponseNormalization:  	// same map  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalResponseNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalResponseNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalResponseNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalResponseNormalizationCM:  	// across maps  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztionCM layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	int size = 9;  	int a' b;  	for (int map = 0; map < MapCount; map++)  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				a = Math.Max (0' map - (size / 2));  				// from map  				b = Math.Min (MapCount' map - (size / 2) + size);  				// to map  				int position = x + (y * MapWidth) + (map * MapSize);  				for (int f = a; f < b; f++)  					AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  			}  	CalculateAction = CalculateLocalResponseNormalizationCM;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMParallel;  	else  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalizationCM;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalContrastNormalization:  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	Gaussian2DKernel = MathUtil.CreateGaussian2DKernel (ReceptiveFieldWidth' ReceptiveFieldHeight);  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalContrastNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalContrastNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalContrastNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalContrastNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.Input:  	ActivationFunctionId = ActivationFunctions.None;  	HasWeights = false;  	ActivationFunction = null;  	UseWeightPartitioner = false;  	WeightCount = 0;  	Weights = null;  	CalculateAction = null;  	BackpropagateAction = null;  	BackpropagateSecondDerivativesAction = null;  	break;  case LayerTypes.Local:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = totalMappings * MapSize * (ReceptiveFieldSize + 1);  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	//CalculateLocalConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		// CalculateLocalConnectedDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount)) {  					int iNumWeight = mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					int iNumWeight = mapping * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				int iNumWeight = curMap * (ReceptiveFieldSize + 1) * MapSize;  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						AddBias (ref Connections [position]' iNumWeight++);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.Convolutional:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskWidth) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.ConvolutionalSubsampling:  	// Simard's implementation  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	for (int map = 0; map < PreviousLayer.MapCount; map++)  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	if (!IsFullyMapped)// not fully mapped  	 {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.AvgPooling:  case LayerTypes.MaxPooling:  	HasWeights = true;  	WeightCount = MapCount * 2;  	Weights = new Weight[WeightCount];  	if (LayerType == LayerTypes.AvgPooling) {  		CalculateAction = CalculateAveragePooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePooling;  	}  	else {  		CalculateAction = CalculateMaxPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPooling;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  										else  											outOfBounds = false;  									}  							}  					}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  							}  					}  				}  			});  		}  	}  	break;  case LayerTypes.AvgPoolingWeightless:  case LayerTypes.MaxPoolingWeightless:  case LayerTypes.L2Pooling:  case LayerTypes.StochasticPooling:  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	switch (LayerType) {  	case LayerTypes.AvgPoolingWeightless:  		CalculateAction = CalculateAveragePoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePoolingWeightless;  		break;  	case LayerTypes.MaxPoolingWeightless:  		CalculateAction = CalculateMaxPoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPoolingWeightless;  		break;  	case LayerTypes.L2Pooling:  		CalculateAction = CalculateL2Pooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateL2PoolingParallel;  		else  			BackpropagateAction = BackpropagateL2PoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesL2Pooling;  		break;  	case LayerTypes.StochasticPooling:  		CalculateAction = CalculateStochasticPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateStochasticPoolingParallel;  		else  			BackpropagateAction = BackpropagateStochasticPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesStochasticPooling;  		break;  	}  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  										else  											outOfBounds = false;  									}  							}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  							}  				}  			});  		}  	}  	break;  case LayerTypes.FullyConnected:  	HasWeights = true;  	WeightCount = (PreviousLayer.NeuronCount + 1) * NeuronCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMax;  	else  		CalculateAction = CalculateFullyConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxParallel;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxSerial;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		Parallel.For (0' MapCount' curMap =>  {  			for (int yc = 0; yc < MapHeight; yc++)  				for (int xc = 0; xc < MapWidth; xc++) {  					int position = xc + (yc * MapWidth) + (curMap * MapSize);  					AddBias (ref Connections [position]' iNumWeight++);  					for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  						for (int y = 0; y < PreviousLayer.MapHeight; y++)  							for (int x = 0; x < PreviousLayer.MapWidth; x++)  								AddConnection (ref Connections [position]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  				}  		});  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++) {  			AddBias (ref Connections [y]' iNumWeight++);  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  		}  	}  	break;  case LayerTypes.RBF:  	HasWeights = true;  	WeightCount = PreviousLayer.NeuronCount * NeuronCount;  	// no biasses  	Weights = new Weight[WeightCount];  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMaxRBF;  	else  		CalculateAction = CalculateRBF;  	BackpropagateAction = BackpropagateRBF;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesRBF;  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		for (int n = 0; n < NeuronCount; n++)  			for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  				for (int y = 0; y < PreviousLayer.MapHeight; y++)  					for (int x = 0; x < PreviousLayer.MapWidth; x++)  						AddConnection (ref Connections [n]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++)  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  	}  	break;  case LayerTypes.LocalResponseNormalization:  	// same map  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalResponseNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalResponseNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalResponseNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalResponseNormalizationCM:  	// across maps  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztionCM layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	int size = 9;  	int a' b;  	for (int map = 0; map < MapCount; map++)  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				a = Math.Max (0' map - (size / 2));  				// from map  				b = Math.Min (MapCount' map - (size / 2) + size);  				// to map  				int position = x + (y * MapWidth) + (map * MapSize);  				for (int f = a; f < b; f++)  					AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  			}  	CalculateAction = CalculateLocalResponseNormalizationCM;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMParallel;  	else  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalizationCM;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalContrastNormalization:  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	Gaussian2DKernel = MathUtil.CreateGaussian2DKernel (ReceptiveFieldWidth' ReceptiveFieldHeight);  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalContrastNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalContrastNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalContrastNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalContrastNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.Input:  	ActivationFunctionId = ActivationFunctions.None;  	HasWeights = false;  	ActivationFunction = null;  	UseWeightPartitioner = false;  	WeightCount = 0;  	Weights = null;  	CalculateAction = null;  	BackpropagateAction = null;  	BackpropagateSecondDerivativesAction = null;  	break;  case LayerTypes.Local:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = totalMappings * MapSize * (ReceptiveFieldSize + 1);  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	//CalculateLocalConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		// CalculateLocalConnectedDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount)) {  					int iNumWeight = mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					int iNumWeight = mapping * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				int iNumWeight = curMap * (ReceptiveFieldSize + 1) * MapSize;  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						AddBias (ref Connections [position]' iNumWeight++);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.Convolutional:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskWidth) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.ConvolutionalSubsampling:  	// Simard's implementation  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	for (int map = 0; map < PreviousLayer.MapCount; map++)  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	if (!IsFullyMapped)// not fully mapped  	 {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.AvgPooling:  case LayerTypes.MaxPooling:  	HasWeights = true;  	WeightCount = MapCount * 2;  	Weights = new Weight[WeightCount];  	if (LayerType == LayerTypes.AvgPooling) {  		CalculateAction = CalculateAveragePooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePooling;  	}  	else {  		CalculateAction = CalculateMaxPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPooling;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  										else  											outOfBounds = false;  									}  							}  					}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  							}  					}  				}  			});  		}  	}  	break;  case LayerTypes.AvgPoolingWeightless:  case LayerTypes.MaxPoolingWeightless:  case LayerTypes.L2Pooling:  case LayerTypes.StochasticPooling:  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	switch (LayerType) {  	case LayerTypes.AvgPoolingWeightless:  		CalculateAction = CalculateAveragePoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePoolingWeightless;  		break;  	case LayerTypes.MaxPoolingWeightless:  		CalculateAction = CalculateMaxPoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPoolingWeightless;  		break;  	case LayerTypes.L2Pooling:  		CalculateAction = CalculateL2Pooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateL2PoolingParallel;  		else  			BackpropagateAction = BackpropagateL2PoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesL2Pooling;  		break;  	case LayerTypes.StochasticPooling:  		CalculateAction = CalculateStochasticPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateStochasticPoolingParallel;  		else  			BackpropagateAction = BackpropagateStochasticPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesStochasticPooling;  		break;  	}  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  										else  											outOfBounds = false;  									}  							}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  							}  				}  			});  		}  	}  	break;  case LayerTypes.FullyConnected:  	HasWeights = true;  	WeightCount = (PreviousLayer.NeuronCount + 1) * NeuronCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMax;  	else  		CalculateAction = CalculateFullyConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxParallel;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxSerial;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		Parallel.For (0' MapCount' curMap =>  {  			for (int yc = 0; yc < MapHeight; yc++)  				for (int xc = 0; xc < MapWidth; xc++) {  					int position = xc + (yc * MapWidth) + (curMap * MapSize);  					AddBias (ref Connections [position]' iNumWeight++);  					for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  						for (int y = 0; y < PreviousLayer.MapHeight; y++)  							for (int x = 0; x < PreviousLayer.MapWidth; x++)  								AddConnection (ref Connections [position]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  				}  		});  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++) {  			AddBias (ref Connections [y]' iNumWeight++);  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  		}  	}  	break;  case LayerTypes.RBF:  	HasWeights = true;  	WeightCount = PreviousLayer.NeuronCount * NeuronCount;  	// no biasses  	Weights = new Weight[WeightCount];  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMaxRBF;  	else  		CalculateAction = CalculateRBF;  	BackpropagateAction = BackpropagateRBF;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesRBF;  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		for (int n = 0; n < NeuronCount; n++)  			for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  				for (int y = 0; y < PreviousLayer.MapHeight; y++)  					for (int x = 0; x < PreviousLayer.MapWidth; x++)  						AddConnection (ref Connections [n]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++)  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  	}  	break;  case LayerTypes.LocalResponseNormalization:  	// same map  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalResponseNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalResponseNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalResponseNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalResponseNormalizationCM:  	// across maps  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztionCM layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	int size = 9;  	int a' b;  	for (int map = 0; map < MapCount; map++)  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				a = Math.Max (0' map - (size / 2));  				// from map  				b = Math.Min (MapCount' map - (size / 2) + size);  				// to map  				int position = x + (y * MapWidth) + (map * MapSize);  				for (int f = a; f < b; f++)  					AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  			}  	CalculateAction = CalculateLocalResponseNormalizationCM;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMParallel;  	else  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalizationCM;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalContrastNormalization:  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	Gaussian2DKernel = MathUtil.CreateGaussian2DKernel (ReceptiveFieldWidth' ReceptiveFieldHeight);  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalContrastNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalContrastNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalContrastNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalContrastNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.Input:  	ActivationFunctionId = ActivationFunctions.None;  	HasWeights = false;  	ActivationFunction = null;  	UseWeightPartitioner = false;  	WeightCount = 0;  	Weights = null;  	CalculateAction = null;  	BackpropagateAction = null;  	BackpropagateSecondDerivativesAction = null;  	break;  case LayerTypes.Local:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = totalMappings * MapSize * (ReceptiveFieldSize + 1);  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	//CalculateLocalConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		// CalculateLocalConnectedDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount)) {  					int iNumWeight = mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					int iNumWeight = mapping * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				int iNumWeight = curMap * (ReceptiveFieldSize + 1) * MapSize;  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						AddBias (ref Connections [position]' iNumWeight++);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.Convolutional:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskWidth) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.ConvolutionalSubsampling:  	// Simard's implementation  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	for (int map = 0; map < PreviousLayer.MapCount; map++)  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	if (!IsFullyMapped)// not fully mapped  	 {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.AvgPooling:  case LayerTypes.MaxPooling:  	HasWeights = true;  	WeightCount = MapCount * 2;  	Weights = new Weight[WeightCount];  	if (LayerType == LayerTypes.AvgPooling) {  		CalculateAction = CalculateAveragePooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePooling;  	}  	else {  		CalculateAction = CalculateMaxPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPooling;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  										else  											outOfBounds = false;  									}  							}  					}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  							}  					}  				}  			});  		}  	}  	break;  case LayerTypes.AvgPoolingWeightless:  case LayerTypes.MaxPoolingWeightless:  case LayerTypes.L2Pooling:  case LayerTypes.StochasticPooling:  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	switch (LayerType) {  	case LayerTypes.AvgPoolingWeightless:  		CalculateAction = CalculateAveragePoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePoolingWeightless;  		break;  	case LayerTypes.MaxPoolingWeightless:  		CalculateAction = CalculateMaxPoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPoolingWeightless;  		break;  	case LayerTypes.L2Pooling:  		CalculateAction = CalculateL2Pooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateL2PoolingParallel;  		else  			BackpropagateAction = BackpropagateL2PoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesL2Pooling;  		break;  	case LayerTypes.StochasticPooling:  		CalculateAction = CalculateStochasticPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateStochasticPoolingParallel;  		else  			BackpropagateAction = BackpropagateStochasticPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesStochasticPooling;  		break;  	}  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  										else  											outOfBounds = false;  									}  							}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  							}  				}  			});  		}  	}  	break;  case LayerTypes.FullyConnected:  	HasWeights = true;  	WeightCount = (PreviousLayer.NeuronCount + 1) * NeuronCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMax;  	else  		CalculateAction = CalculateFullyConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxParallel;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxSerial;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		Parallel.For (0' MapCount' curMap =>  {  			for (int yc = 0; yc < MapHeight; yc++)  				for (int xc = 0; xc < MapWidth; xc++) {  					int position = xc + (yc * MapWidth) + (curMap * MapSize);  					AddBias (ref Connections [position]' iNumWeight++);  					for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  						for (int y = 0; y < PreviousLayer.MapHeight; y++)  							for (int x = 0; x < PreviousLayer.MapWidth; x++)  								AddConnection (ref Connections [position]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  				}  		});  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++) {  			AddBias (ref Connections [y]' iNumWeight++);  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  		}  	}  	break;  case LayerTypes.RBF:  	HasWeights = true;  	WeightCount = PreviousLayer.NeuronCount * NeuronCount;  	// no biasses  	Weights = new Weight[WeightCount];  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMaxRBF;  	else  		CalculateAction = CalculateRBF;  	BackpropagateAction = BackpropagateRBF;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesRBF;  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		for (int n = 0; n < NeuronCount; n++)  			for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  				for (int y = 0; y < PreviousLayer.MapHeight; y++)  					for (int x = 0; x < PreviousLayer.MapWidth; x++)  						AddConnection (ref Connections [n]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++)  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  	}  	break;  case LayerTypes.LocalResponseNormalization:  	// same map  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalResponseNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalResponseNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalResponseNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalResponseNormalizationCM:  	// across maps  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztionCM layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	int size = 9;  	int a' b;  	for (int map = 0; map < MapCount; map++)  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				a = Math.Max (0' map - (size / 2));  				// from map  				b = Math.Min (MapCount' map - (size / 2) + size);  				// to map  				int position = x + (y * MapWidth) + (map * MapSize);  				for (int f = a; f < b; f++)  					AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  			}  	CalculateAction = CalculateLocalResponseNormalizationCM;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMParallel;  	else  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalizationCM;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalContrastNormalization:  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	Gaussian2DKernel = MathUtil.CreateGaussian2DKernel (ReceptiveFieldWidth' ReceptiveFieldHeight);  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalContrastNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalContrastNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalContrastNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalContrastNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.Input:  	ActivationFunctionId = ActivationFunctions.None;  	HasWeights = false;  	ActivationFunction = null;  	UseWeightPartitioner = false;  	WeightCount = 0;  	Weights = null;  	CalculateAction = null;  	BackpropagateAction = null;  	BackpropagateSecondDerivativesAction = null;  	break;  case LayerTypes.Local:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = totalMappings * MapSize * (ReceptiveFieldSize + 1);  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	//CalculateLocalConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		// CalculateLocalConnectedDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount)) {  					int iNumWeight = mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					int iNumWeight = mapping * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				int iNumWeight = curMap * (ReceptiveFieldSize + 1) * MapSize;  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						AddBias (ref Connections [position]' iNumWeight++);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.Convolutional:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskWidth) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.ConvolutionalSubsampling:  	// Simard's implementation  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	for (int map = 0; map < PreviousLayer.MapCount; map++)  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	if (!IsFullyMapped)// not fully mapped  	 {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.AvgPooling:  case LayerTypes.MaxPooling:  	HasWeights = true;  	WeightCount = MapCount * 2;  	Weights = new Weight[WeightCount];  	if (LayerType == LayerTypes.AvgPooling) {  		CalculateAction = CalculateAveragePooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePooling;  	}  	else {  		CalculateAction = CalculateMaxPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPooling;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  										else  											outOfBounds = false;  									}  							}  					}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  							}  					}  				}  			});  		}  	}  	break;  case LayerTypes.AvgPoolingWeightless:  case LayerTypes.MaxPoolingWeightless:  case LayerTypes.L2Pooling:  case LayerTypes.StochasticPooling:  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	switch (LayerType) {  	case LayerTypes.AvgPoolingWeightless:  		CalculateAction = CalculateAveragePoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePoolingWeightless;  		break;  	case LayerTypes.MaxPoolingWeightless:  		CalculateAction = CalculateMaxPoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPoolingWeightless;  		break;  	case LayerTypes.L2Pooling:  		CalculateAction = CalculateL2Pooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateL2PoolingParallel;  		else  			BackpropagateAction = BackpropagateL2PoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesL2Pooling;  		break;  	case LayerTypes.StochasticPooling:  		CalculateAction = CalculateStochasticPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateStochasticPoolingParallel;  		else  			BackpropagateAction = BackpropagateStochasticPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesStochasticPooling;  		break;  	}  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  										else  											outOfBounds = false;  									}  							}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  							}  				}  			});  		}  	}  	break;  case LayerTypes.FullyConnected:  	HasWeights = true;  	WeightCount = (PreviousLayer.NeuronCount + 1) * NeuronCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMax;  	else  		CalculateAction = CalculateFullyConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxParallel;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxSerial;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		Parallel.For (0' MapCount' curMap =>  {  			for (int yc = 0; yc < MapHeight; yc++)  				for (int xc = 0; xc < MapWidth; xc++) {  					int position = xc + (yc * MapWidth) + (curMap * MapSize);  					AddBias (ref Connections [position]' iNumWeight++);  					for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  						for (int y = 0; y < PreviousLayer.MapHeight; y++)  							for (int x = 0; x < PreviousLayer.MapWidth; x++)  								AddConnection (ref Connections [position]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  				}  		});  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++) {  			AddBias (ref Connections [y]' iNumWeight++);  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  		}  	}  	break;  case LayerTypes.RBF:  	HasWeights = true;  	WeightCount = PreviousLayer.NeuronCount * NeuronCount;  	// no biasses  	Weights = new Weight[WeightCount];  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMaxRBF;  	else  		CalculateAction = CalculateRBF;  	BackpropagateAction = BackpropagateRBF;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesRBF;  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		for (int n = 0; n < NeuronCount; n++)  			for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  				for (int y = 0; y < PreviousLayer.MapHeight; y++)  					for (int x = 0; x < PreviousLayer.MapWidth; x++)  						AddConnection (ref Connections [n]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++)  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  	}  	break;  case LayerTypes.LocalResponseNormalization:  	// same map  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalResponseNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalResponseNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalResponseNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalResponseNormalizationCM:  	// across maps  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztionCM layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	int size = 9;  	int a' b;  	for (int map = 0; map < MapCount; map++)  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				a = Math.Max (0' map - (size / 2));  				// from map  				b = Math.Min (MapCount' map - (size / 2) + size);  				// to map  				int position = x + (y * MapWidth) + (map * MapSize);  				for (int f = a; f < b; f++)  					AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  			}  	CalculateAction = CalculateLocalResponseNormalizationCM;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMParallel;  	else  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalizationCM;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalContrastNormalization:  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	Gaussian2DKernel = MathUtil.CreateGaussian2DKernel (ReceptiveFieldWidth' ReceptiveFieldHeight);  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalContrastNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalContrastNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalContrastNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalContrastNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.Input:  	ActivationFunctionId = ActivationFunctions.None;  	HasWeights = false;  	ActivationFunction = null;  	UseWeightPartitioner = false;  	WeightCount = 0;  	Weights = null;  	CalculateAction = null;  	BackpropagateAction = null;  	BackpropagateSecondDerivativesAction = null;  	break;  case LayerTypes.Local:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = totalMappings * MapSize * (ReceptiveFieldSize + 1);  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	//CalculateLocalConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		// CalculateLocalConnectedDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount)) {  					int iNumWeight = mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					int iNumWeight = mapping * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				int iNumWeight = curMap * (ReceptiveFieldSize + 1) * MapSize;  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						AddBias (ref Connections [position]' iNumWeight++);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.Convolutional:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskWidth) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.ConvolutionalSubsampling:  	// Simard's implementation  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	for (int map = 0; map < PreviousLayer.MapCount; map++)  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	if (!IsFullyMapped)// not fully mapped  	 {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.AvgPooling:  case LayerTypes.MaxPooling:  	HasWeights = true;  	WeightCount = MapCount * 2;  	Weights = new Weight[WeightCount];  	if (LayerType == LayerTypes.AvgPooling) {  		CalculateAction = CalculateAveragePooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePooling;  	}  	else {  		CalculateAction = CalculateMaxPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPooling;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  										else  											outOfBounds = false;  									}  							}  					}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  							}  					}  				}  			});  		}  	}  	break;  case LayerTypes.AvgPoolingWeightless:  case LayerTypes.MaxPoolingWeightless:  case LayerTypes.L2Pooling:  case LayerTypes.StochasticPooling:  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	switch (LayerType) {  	case LayerTypes.AvgPoolingWeightless:  		CalculateAction = CalculateAveragePoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePoolingWeightless;  		break;  	case LayerTypes.MaxPoolingWeightless:  		CalculateAction = CalculateMaxPoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPoolingWeightless;  		break;  	case LayerTypes.L2Pooling:  		CalculateAction = CalculateL2Pooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateL2PoolingParallel;  		else  			BackpropagateAction = BackpropagateL2PoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesL2Pooling;  		break;  	case LayerTypes.StochasticPooling:  		CalculateAction = CalculateStochasticPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateStochasticPoolingParallel;  		else  			BackpropagateAction = BackpropagateStochasticPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesStochasticPooling;  		break;  	}  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  										else  											outOfBounds = false;  									}  							}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  							}  				}  			});  		}  	}  	break;  case LayerTypes.FullyConnected:  	HasWeights = true;  	WeightCount = (PreviousLayer.NeuronCount + 1) * NeuronCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMax;  	else  		CalculateAction = CalculateFullyConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxParallel;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxSerial;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		Parallel.For (0' MapCount' curMap =>  {  			for (int yc = 0; yc < MapHeight; yc++)  				for (int xc = 0; xc < MapWidth; xc++) {  					int position = xc + (yc * MapWidth) + (curMap * MapSize);  					AddBias (ref Connections [position]' iNumWeight++);  					for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  						for (int y = 0; y < PreviousLayer.MapHeight; y++)  							for (int x = 0; x < PreviousLayer.MapWidth; x++)  								AddConnection (ref Connections [position]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  				}  		});  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++) {  			AddBias (ref Connections [y]' iNumWeight++);  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  		}  	}  	break;  case LayerTypes.RBF:  	HasWeights = true;  	WeightCount = PreviousLayer.NeuronCount * NeuronCount;  	// no biasses  	Weights = new Weight[WeightCount];  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMaxRBF;  	else  		CalculateAction = CalculateRBF;  	BackpropagateAction = BackpropagateRBF;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesRBF;  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		for (int n = 0; n < NeuronCount; n++)  			for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  				for (int y = 0; y < PreviousLayer.MapHeight; y++)  					for (int x = 0; x < PreviousLayer.MapWidth; x++)  						AddConnection (ref Connections [n]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++)  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  	}  	break;  case LayerTypes.LocalResponseNormalization:  	// same map  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalResponseNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalResponseNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalResponseNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalResponseNormalizationCM:  	// across maps  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztionCM layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	int size = 9;  	int a' b;  	for (int map = 0; map < MapCount; map++)  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				a = Math.Max (0' map - (size / 2));  				// from map  				b = Math.Min (MapCount' map - (size / 2) + size);  				// to map  				int position = x + (y * MapWidth) + (map * MapSize);  				for (int f = a; f < b; f++)  					AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  			}  	CalculateAction = CalculateLocalResponseNormalizationCM;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMParallel;  	else  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalizationCM;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalContrastNormalization:  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	Gaussian2DKernel = MathUtil.CreateGaussian2DKernel (ReceptiveFieldWidth' ReceptiveFieldHeight);  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalContrastNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalContrastNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalContrastNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalContrastNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: maskWidth = PreviousLayer.MapWidth + (2 * PadX);  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: maskHeight = PreviousLayer.MapHeight + (2 * PadY);  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: maskWidth = PreviousLayer.MapWidth + (2 * PadX);  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: maskHeight = PreviousLayer.MapHeight + (2 * PadY);  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: maskWidth = PreviousLayer.MapWidth + (2 * PadX);  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: maskHeight = PreviousLayer.MapHeight + (2 * PadY);  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: if (!IsFullyMapped)// not fully mapped   {  	int mapping = 0;  	int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  	for (int curMap = 0; curMap < MapCount; curMap++)  		for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  			mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  			if (Mappings.IsMapped (curMap' prevMap' MapCount))  				mapping++;  		}  	Parallel.For (0' MapCount' curMap =>  {  		for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  			int positionPrevMap = prevMap * maskSize;  			if (Mappings.IsMapped (curMap' prevMap' MapCount))  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  		}  	});  }  else// Fully mapped   {  	if (totalMappings > MapCount) {  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				int mapping = prevMap + (curMap * PreviousLayer.MapCount);  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			}  		});  	}  	else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  	 {  		Parallel.For (0' MapCount' curMap =>  {  			for (int y = 0; y < MapHeight; y++)  				for (int x = 0; x < MapWidth; x++) {  					int position = x + (y * MapWidth) + (curMap * MapSize);  					int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  					AddBias (ref Connections [position]' curMap);  					int pIndex;  					for (int row = 0; row < ReceptiveFieldHeight; row++)  						for (int column = 0; column < ReceptiveFieldWidth; column++) {  							pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  							if (maskMatrix [pIndex] != -1)  								AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  						}  				}  		});  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: if (!IsFullyMapped)// not fully mapped   {  	int mapping = 0;  	int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  	for (int curMap = 0; curMap < MapCount; curMap++)  		for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  			mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  			if (Mappings.IsMapped (curMap' prevMap' MapCount))  				mapping++;  		}  	Parallel.For (0' MapCount' curMap =>  {  		for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  			int positionPrevMap = prevMap * maskSize;  			if (Mappings.IsMapped (curMap' prevMap' MapCount))  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  		}  	});  }  else// Fully mapped   {  	if (totalMappings > MapCount) {  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				int mapping = prevMap + (curMap * PreviousLayer.MapCount);  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			}  		});  	}  	else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  	 {  		Parallel.For (0' MapCount' curMap =>  {  			for (int y = 0; y < MapHeight; y++)  				for (int x = 0; x < MapWidth; x++) {  					int position = x + (y * MapWidth) + (curMap * MapSize);  					int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  					AddBias (ref Connections [position]' curMap);  					int pIndex;  					for (int row = 0; row < ReceptiveFieldHeight; row++)  						for (int column = 0; column < ReceptiveFieldWidth; column++) {  							pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  							if (maskMatrix [pIndex] != -1)  								AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  						}  				}  		});  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: if (!IsFullyMapped)// not fully mapped   {  	int mapping = 0;  	int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  	for (int curMap = 0; curMap < MapCount; curMap++)  		for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  			mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  			if (Mappings.IsMapped (curMap' prevMap' MapCount))  				mapping++;  		}  	Parallel.For (0' MapCount' curMap =>  {  		for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  			int positionPrevMap = prevMap * maskSize;  			if (Mappings.IsMapped (curMap' prevMap' MapCount))  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  		}  	});  }  else// Fully mapped   {  	if (totalMappings > MapCount) {  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				int mapping = prevMap + (curMap * PreviousLayer.MapCount);  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			}  		});  	}  	else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  	 {  		Parallel.For (0' MapCount' curMap =>  {  			for (int y = 0; y < MapHeight; y++)  				for (int x = 0; x < MapWidth; x++) {  					int position = x + (y * MapWidth) + (curMap * MapSize);  					int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  					AddBias (ref Connections [position]' curMap);  					int pIndex;  					for (int row = 0; row < ReceptiveFieldHeight; row++)  						for (int column = 0; column < ReceptiveFieldWidth; column++) {  							pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  							if (maskMatrix [pIndex] != -1)  								AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  						}  				}  		});  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: if (!IsFullyMapped)// not fully mapped   {  	int mapping = 0;  	int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  	for (int curMap = 0; curMap < MapCount; curMap++)  		for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  			mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  			if (Mappings.IsMapped (curMap' prevMap' MapCount))  				mapping++;  		}  	Parallel.For (0' MapCount' curMap =>  {  		for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  			int positionPrevMap = prevMap * maskSize;  			if (Mappings.IsMapped (curMap' prevMap' MapCount))  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  		}  	});  }  else// Fully mapped   {  	if (totalMappings > MapCount) {  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				int mapping = prevMap + (curMap * PreviousLayer.MapCount);  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			}  		});  	}  	else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  	 {  		Parallel.For (0' MapCount' curMap =>  {  			for (int y = 0; y < MapHeight; y++)  				for (int x = 0; x < MapWidth; x++) {  					int position = x + (y * MapWidth) + (curMap * MapSize);  					int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  					AddBias (ref Connections [position]' curMap);  					int pIndex;  					for (int row = 0; row < ReceptiveFieldHeight; row++)  						for (int column = 0; column < ReceptiveFieldWidth; column++) {  							pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  							if (maskMatrix [pIndex] != -1)  								AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  						}  				}  		});  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: if (!IsFullyMapped)// not fully mapped   {  	int mapping = 0;  	int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  	for (int curMap = 0; curMap < MapCount; curMap++)  		for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  			mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  			if (Mappings.IsMapped (curMap' prevMap' MapCount))  				mapping++;  		}  	Parallel.For (0' MapCount' curMap =>  {  		for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  			int positionPrevMap = prevMap * maskSize;  			if (Mappings.IsMapped (curMap' prevMap' MapCount))  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  		}  	});  }  else// Fully mapped   {  	if (totalMappings > MapCount) {  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				int mapping = prevMap + (curMap * PreviousLayer.MapCount);  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			}  		});  	}  	else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  	 {  		Parallel.For (0' MapCount' curMap =>  {  			for (int y = 0; y < MapHeight; y++)  				for (int x = 0; x < MapWidth; x++) {  					int position = x + (y * MapWidth) + (curMap * MapSize);  					int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  					AddBias (ref Connections [position]' curMap);  					int pIndex;  					for (int row = 0; row < ReceptiveFieldHeight; row++)  						for (int column = 0; column < ReceptiveFieldWidth; column++) {  							pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  							if (maskMatrix [pIndex] != -1)  								AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  						}  				}  		});  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: if (!IsFullyMapped)// not fully mapped   {  	int mapping = 0;  	int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  	for (int curMap = 0; curMap < MapCount; curMap++)  		for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  			mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  			if (Mappings.IsMapped (curMap' prevMap' MapCount))  				mapping++;  		}  	Parallel.For (0' MapCount' curMap =>  {  		for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  			int positionPrevMap = prevMap * maskSize;  			if (Mappings.IsMapped (curMap' prevMap' MapCount))  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  		}  	});  }  else// Fully mapped   {  	if (totalMappings > MapCount) {  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				int mapping = prevMap + (curMap * PreviousLayer.MapCount);  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			}  		});  	}  	else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  	 {  		Parallel.For (0' MapCount' curMap =>  {  			for (int y = 0; y < MapHeight; y++)  				for (int x = 0; x < MapWidth; x++) {  					int position = x + (y * MapWidth) + (curMap * MapSize);  					int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  					AddBias (ref Connections [position]' curMap);  					int pIndex;  					for (int row = 0; row < ReceptiveFieldHeight; row++)  						for (int column = 0; column < ReceptiveFieldWidth; column++) {  							pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  							if (maskMatrix [pIndex] != -1)  								AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  						}  				}  		});  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: Parallel.For (0' MapCount' curMap =>  {  	for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  		int positionPrevMap = prevMap * maskSize;  		if (Mappings.IsMapped (curMap' prevMap' MapCount))  			for (int y = 0; y < MapHeight; y++)  				for (int x = 0; x < MapWidth; x++) {  					int position = x + (y * MapWidth) + (curMap * MapSize);  					int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  					AddBias (ref Connections [position]' curMap);  					int pIndex;  					for (int row = 0; row < ReceptiveFieldHeight; row++)  						for (int column = 0; column < ReceptiveFieldWidth; column++) {  							pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  							if (maskMatrix [pIndex] != -1)  								AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  						}  				}  	}  });  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: Parallel.For (0' MapCount' curMap =>  {  	for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  		int positionPrevMap = prevMap * maskSize;  		if (Mappings.IsMapped (curMap' prevMap' MapCount))  			for (int y = 0; y < MapHeight; y++)  				for (int x = 0; x < MapWidth; x++) {  					int position = x + (y * MapWidth) + (curMap * MapSize);  					int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  					AddBias (ref Connections [position]' curMap);  					int pIndex;  					for (int row = 0; row < ReceptiveFieldHeight; row++)  						for (int column = 0; column < ReceptiveFieldWidth; column++) {  							pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  							if (maskMatrix [pIndex] != -1)  								AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  						}  				}  	}  });  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  	int positionPrevMap = prevMap * maskSize;  	if (Mappings.IsMapped (curMap' prevMap' MapCount))  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (curMap * MapSize);  				int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  				AddBias (ref Connections [position]' curMap);  				int pIndex;  				for (int row = 0; row < ReceptiveFieldHeight; row++)  					for (int column = 0; column < ReceptiveFieldWidth; column++) {  						pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  						if (maskMatrix [pIndex] != -1)  							AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  					}  			}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  	int positionPrevMap = prevMap * maskSize;  	if (Mappings.IsMapped (curMap' prevMap' MapCount))  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (curMap * MapSize);  				int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  				AddBias (ref Connections [position]' curMap);  				int pIndex;  				for (int row = 0; row < ReceptiveFieldHeight; row++)  					for (int column = 0; column < ReceptiveFieldWidth; column++) {  						pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  						if (maskMatrix [pIndex] != -1)  							AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  					}  			}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: if (Mappings.IsMapped (curMap' prevMap' MapCount))  	for (int y = 0; y < MapHeight; y++)  		for (int x = 0; x < MapWidth; x++) {  			int position = x + (y * MapWidth) + (curMap * MapSize);  			int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  			AddBias (ref Connections [position]' curMap);  			int pIndex;  			for (int row = 0; row < ReceptiveFieldHeight; row++)  				for (int column = 0; column < ReceptiveFieldWidth; column++) {  					pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  					if (maskMatrix [pIndex] != -1)  						AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  				}  		}  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: if (Mappings.IsMapped (curMap' prevMap' MapCount))  	for (int y = 0; y < MapHeight; y++)  		for (int x = 0; x < MapWidth; x++) {  			int position = x + (y * MapWidth) + (curMap * MapSize);  			int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  			AddBias (ref Connections [position]' curMap);  			int pIndex;  			for (int row = 0; row < ReceptiveFieldHeight; row++)  				for (int column = 0; column < ReceptiveFieldWidth; column++) {  					pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  					if (maskMatrix [pIndex] != -1)  						AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  				}  		}  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int y = 0; y < MapHeight; y++)  	for (int x = 0; x < MapWidth; x++) {  		int position = x + (y * MapWidth) + (curMap * MapSize);  		int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  		AddBias (ref Connections [position]' curMap);  		int pIndex;  		for (int row = 0; row < ReceptiveFieldHeight; row++)  			for (int column = 0; column < ReceptiveFieldWidth; column++) {  				pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  				if (maskMatrix [pIndex] != -1)  					AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  			}  	}  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int y = 0; y < MapHeight; y++)  	for (int x = 0; x < MapWidth; x++) {  		int position = x + (y * MapWidth) + (curMap * MapSize);  		int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  		AddBias (ref Connections [position]' curMap);  		int pIndex;  		for (int row = 0; row < ReceptiveFieldHeight; row++)  			for (int column = 0; column < ReceptiveFieldWidth; column++) {  				pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  				if (maskMatrix [pIndex] != -1)  					AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  			}  	}  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int x = 0; x < MapWidth; x++) {  	int position = x + (y * MapWidth) + (curMap * MapSize);  	int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  	AddBias (ref Connections [position]' curMap);  	int pIndex;  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++) {  			pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  			if (maskMatrix [pIndex] != -1)  				AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  		}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int x = 0; x < MapWidth; x++) {  	int position = x + (y * MapWidth) + (curMap * MapSize);  	int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  	AddBias (ref Connections [position]' curMap);  	int pIndex;  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++) {  			pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  			if (maskMatrix [pIndex] != -1)  				AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  		}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int row = 0; row < ReceptiveFieldHeight; row++)  	for (int column = 0; column < ReceptiveFieldWidth; column++) {  		pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  		if (maskMatrix [pIndex] != -1)  			AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  	}  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int row = 0; row < ReceptiveFieldHeight; row++)  	for (int column = 0; column < ReceptiveFieldWidth; column++) {  		pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  		if (maskMatrix [pIndex] != -1)  			AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  	}  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int column = 0; column < ReceptiveFieldWidth; column++) {  	pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  	if (maskMatrix [pIndex] != -1)  		AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int column = 0; column < ReceptiveFieldWidth; column++) {  	pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  	if (maskMatrix [pIndex] != -1)  		AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: if (totalMappings > MapCount) {  	Parallel.For (0' MapCount' curMap =>  {  		for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  			int positionPrevMap = prevMap * maskSize;  			int mapping = prevMap + (curMap * PreviousLayer.MapCount);  			for (int y = 0; y < MapHeight; y++)  				for (int x = 0; x < MapWidth; x++) {  					int position = x + (y * MapWidth) + (curMap * MapSize);  					int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  					AddBias (ref Connections [position]' curMap);  					int pIndex;  					for (int row = 0; row < ReceptiveFieldHeight; row++)  						for (int column = 0; column < ReceptiveFieldWidth; column++) {  							pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  							if (maskMatrix [pIndex] != -1)  								AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  						}  				}  		}  	});  }  else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer   {  	Parallel.For (0' MapCount' curMap =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (curMap * MapSize);  				int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  				AddBias (ref Connections [position]' curMap);  				int pIndex;  				for (int row = 0; row < ReceptiveFieldHeight; row++)  					for (int column = 0; column < ReceptiveFieldWidth; column++) {  						pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  						if (maskMatrix [pIndex] != -1)  							AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  					}  			}  	});  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: if (totalMappings > MapCount) {  	Parallel.For (0' MapCount' curMap =>  {  		for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  			int positionPrevMap = prevMap * maskSize;  			int mapping = prevMap + (curMap * PreviousLayer.MapCount);  			for (int y = 0; y < MapHeight; y++)  				for (int x = 0; x < MapWidth; x++) {  					int position = x + (y * MapWidth) + (curMap * MapSize);  					int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  					AddBias (ref Connections [position]' curMap);  					int pIndex;  					for (int row = 0; row < ReceptiveFieldHeight; row++)  						for (int column = 0; column < ReceptiveFieldWidth; column++) {  							pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  							if (maskMatrix [pIndex] != -1)  								AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  						}  				}  		}  	});  }  else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer   {  	Parallel.For (0' MapCount' curMap =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (curMap * MapSize);  				int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  				AddBias (ref Connections [position]' curMap);  				int pIndex;  				for (int row = 0; row < ReceptiveFieldHeight; row++)  					for (int column = 0; column < ReceptiveFieldWidth; column++) {  						pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  						if (maskMatrix [pIndex] != -1)  							AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  					}  			}  	});  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: if (totalMappings > MapCount) {  	Parallel.For (0' MapCount' curMap =>  {  		for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  			int positionPrevMap = prevMap * maskSize;  			int mapping = prevMap + (curMap * PreviousLayer.MapCount);  			for (int y = 0; y < MapHeight; y++)  				for (int x = 0; x < MapWidth; x++) {  					int position = x + (y * MapWidth) + (curMap * MapSize);  					int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  					AddBias (ref Connections [position]' curMap);  					int pIndex;  					for (int row = 0; row < ReceptiveFieldHeight; row++)  						for (int column = 0; column < ReceptiveFieldWidth; column++) {  							pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  							if (maskMatrix [pIndex] != -1)  								AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  						}  				}  		}  	});  }  else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer   {  	Parallel.For (0' MapCount' curMap =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (curMap * MapSize);  				int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  				AddBias (ref Connections [position]' curMap);  				int pIndex;  				for (int row = 0; row < ReceptiveFieldHeight; row++)  					for (int column = 0; column < ReceptiveFieldWidth; column++) {  						pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  						if (maskMatrix [pIndex] != -1)  							AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  					}  			}  	});  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: if (totalMappings > MapCount) {  	Parallel.For (0' MapCount' curMap =>  {  		for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  			int positionPrevMap = prevMap * maskSize;  			int mapping = prevMap + (curMap * PreviousLayer.MapCount);  			for (int y = 0; y < MapHeight; y++)  				for (int x = 0; x < MapWidth; x++) {  					int position = x + (y * MapWidth) + (curMap * MapSize);  					int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  					AddBias (ref Connections [position]' curMap);  					int pIndex;  					for (int row = 0; row < ReceptiveFieldHeight; row++)  						for (int column = 0; column < ReceptiveFieldWidth; column++) {  							pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  							if (maskMatrix [pIndex] != -1)  								AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  						}  				}  		}  	});  }  else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer   {  	Parallel.For (0' MapCount' curMap =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (curMap * MapSize);  				int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  				AddBias (ref Connections [position]' curMap);  				int pIndex;  				for (int row = 0; row < ReceptiveFieldHeight; row++)  					for (int column = 0; column < ReceptiveFieldWidth; column++) {  						pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  						if (maskMatrix [pIndex] != -1)  							AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  					}  			}  	});  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: Parallel.For (0' MapCount' curMap =>  {  	for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  		int positionPrevMap = prevMap * maskSize;  		int mapping = prevMap + (curMap * PreviousLayer.MapCount);  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (curMap * MapSize);  				int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  				AddBias (ref Connections [position]' curMap);  				int pIndex;  				for (int row = 0; row < ReceptiveFieldHeight; row++)  					for (int column = 0; column < ReceptiveFieldWidth; column++) {  						pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  						if (maskMatrix [pIndex] != -1)  							AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  					}  			}  	}  });  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: Parallel.For (0' MapCount' curMap =>  {  	for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  		int positionPrevMap = prevMap * maskSize;  		int mapping = prevMap + (curMap * PreviousLayer.MapCount);  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (curMap * MapSize);  				int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  				AddBias (ref Connections [position]' curMap);  				int pIndex;  				for (int row = 0; row < ReceptiveFieldHeight; row++)  					for (int column = 0; column < ReceptiveFieldWidth; column++) {  						pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  						if (maskMatrix [pIndex] != -1)  							AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  					}  			}  	}  });  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  	int positionPrevMap = prevMap * maskSize;  	int mapping = prevMap + (curMap * PreviousLayer.MapCount);  	for (int y = 0; y < MapHeight; y++)  		for (int x = 0; x < MapWidth; x++) {  			int position = x + (y * MapWidth) + (curMap * MapSize);  			int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  			AddBias (ref Connections [position]' curMap);  			int pIndex;  			for (int row = 0; row < ReceptiveFieldHeight; row++)  				for (int column = 0; column < ReceptiveFieldWidth; column++) {  					pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  					if (maskMatrix [pIndex] != -1)  						AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  				}  		}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  	int positionPrevMap = prevMap * maskSize;  	int mapping = prevMap + (curMap * PreviousLayer.MapCount);  	for (int y = 0; y < MapHeight; y++)  		for (int x = 0; x < MapWidth; x++) {  			int position = x + (y * MapWidth) + (curMap * MapSize);  			int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  			AddBias (ref Connections [position]' curMap);  			int pIndex;  			for (int row = 0; row < ReceptiveFieldHeight; row++)  				for (int column = 0; column < ReceptiveFieldWidth; column++) {  					pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  					if (maskMatrix [pIndex] != -1)  						AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  				}  		}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int y = 0; y < MapHeight; y++)  	for (int x = 0; x < MapWidth; x++) {  		int position = x + (y * MapWidth) + (curMap * MapSize);  		int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  		AddBias (ref Connections [position]' curMap);  		int pIndex;  		for (int row = 0; row < ReceptiveFieldHeight; row++)  			for (int column = 0; column < ReceptiveFieldWidth; column++) {  				pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  				if (maskMatrix [pIndex] != -1)  					AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  			}  	}  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int y = 0; y < MapHeight; y++)  	for (int x = 0; x < MapWidth; x++) {  		int position = x + (y * MapWidth) + (curMap * MapSize);  		int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  		AddBias (ref Connections [position]' curMap);  		int pIndex;  		for (int row = 0; row < ReceptiveFieldHeight; row++)  			for (int column = 0; column < ReceptiveFieldWidth; column++) {  				pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  				if (maskMatrix [pIndex] != -1)  					AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  			}  	}  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int x = 0; x < MapWidth; x++) {  	int position = x + (y * MapWidth) + (curMap * MapSize);  	int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  	AddBias (ref Connections [position]' curMap);  	int pIndex;  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++) {  			pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  			if (maskMatrix [pIndex] != -1)  				AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  		}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int x = 0; x < MapWidth; x++) {  	int position = x + (y * MapWidth) + (curMap * MapSize);  	int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  	AddBias (ref Connections [position]' curMap);  	int pIndex;  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++) {  			pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  			if (maskMatrix [pIndex] != -1)  				AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  		}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int row = 0; row < ReceptiveFieldHeight; row++)  	for (int column = 0; column < ReceptiveFieldWidth; column++) {  		pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  		if (maskMatrix [pIndex] != -1)  			AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  	}  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int row = 0; row < ReceptiveFieldHeight; row++)  	for (int column = 0; column < ReceptiveFieldWidth; column++) {  		pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  		if (maskMatrix [pIndex] != -1)  			AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  	}  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int column = 0; column < ReceptiveFieldWidth; column++) {  	pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  	if (maskMatrix [pIndex] != -1)  		AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int column = 0; column < ReceptiveFieldWidth; column++) {  	pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  	if (maskMatrix [pIndex] != -1)  		AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: Parallel.For (0' MapCount' curMap =>  {  	for (int y = 0; y < MapHeight; y++)  		for (int x = 0; x < MapWidth; x++) {  			int position = x + (y * MapWidth) + (curMap * MapSize);  			int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  			AddBias (ref Connections [position]' curMap);  			int pIndex;  			for (int row = 0; row < ReceptiveFieldHeight; row++)  				for (int column = 0; column < ReceptiveFieldWidth; column++) {  					pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  					if (maskMatrix [pIndex] != -1)  						AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  				}  		}  });  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: Parallel.For (0' MapCount' curMap =>  {  	for (int y = 0; y < MapHeight; y++)  		for (int x = 0; x < MapWidth; x++) {  			int position = x + (y * MapWidth) + (curMap * MapSize);  			int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  			AddBias (ref Connections [position]' curMap);  			int pIndex;  			for (int row = 0; row < ReceptiveFieldHeight; row++)  				for (int column = 0; column < ReceptiveFieldWidth; column++) {  					pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  					if (maskMatrix [pIndex] != -1)  						AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  				}  		}  });  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int y = 0; y < MapHeight; y++)  	for (int x = 0; x < MapWidth; x++) {  		int position = x + (y * MapWidth) + (curMap * MapSize);  		int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  		AddBias (ref Connections [position]' curMap);  		int pIndex;  		for (int row = 0; row < ReceptiveFieldHeight; row++)  			for (int column = 0; column < ReceptiveFieldWidth; column++) {  				pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  				if (maskMatrix [pIndex] != -1)  					AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  			}  	}  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int y = 0; y < MapHeight; y++)  	for (int x = 0; x < MapWidth; x++) {  		int position = x + (y * MapWidth) + (curMap * MapSize);  		int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  		AddBias (ref Connections [position]' curMap);  		int pIndex;  		for (int row = 0; row < ReceptiveFieldHeight; row++)  			for (int column = 0; column < ReceptiveFieldWidth; column++) {  				pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  				if (maskMatrix [pIndex] != -1)  					AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  			}  	}  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int x = 0; x < MapWidth; x++) {  	int position = x + (y * MapWidth) + (curMap * MapSize);  	int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  	AddBias (ref Connections [position]' curMap);  	int pIndex;  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++) {  			pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  			if (maskMatrix [pIndex] != -1)  				AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  		}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int x = 0; x < MapWidth; x++) {  	int position = x + (y * MapWidth) + (curMap * MapSize);  	int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  	AddBias (ref Connections [position]' curMap);  	int pIndex;  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++) {  			pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  			if (maskMatrix [pIndex] != -1)  				AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  		}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int row = 0; row < ReceptiveFieldHeight; row++)  	for (int column = 0; column < ReceptiveFieldWidth; column++) {  		pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  		if (maskMatrix [pIndex] != -1)  			AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  	}  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int row = 0; row < ReceptiveFieldHeight; row++)  	for (int column = 0; column < ReceptiveFieldWidth; column++) {  		pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  		if (maskMatrix [pIndex] != -1)  			AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  	}  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int column = 0; column < ReceptiveFieldWidth; column++) {  	pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  	if (maskMatrix [pIndex] != -1)  		AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int column = 0; column < ReceptiveFieldWidth; column++) {  	pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  	if (maskMatrix [pIndex] != -1)  		AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: WeightCount = MapCount * 2;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: if (PreviousLayer.MapCount > 1)//fully symmetrical mapped   {  	if (ReceptiveFieldSize != (StrideX * StrideY)) {  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * PreviousLayer.MapSize;  				if (prevMap == curMap) {  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = curMap * 2;  							AddBias (ref Connections [position]' iNumWeight++);  							bool outOfBounds = false;  							for (int row = -rMid; row <= rMid; row++)  								for (int col = -cMid; col <= cMid; col++) {  									if (row + (y * StrideY) < 0)  										outOfBounds = true;  									if (row + (y * StrideY) >= PreviousLayer.MapHeight)  										outOfBounds = true;  									if (col + (x * StrideX) < 0)  										outOfBounds = true;  									if (col + (x * StrideX) >= PreviousLayer.MapWidth)  										outOfBounds = true;  									if (!outOfBounds)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  									else  										outOfBounds = false;  								}  						}  				}  			}  		});  	}  	else {  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  				if (prevMap == curMap) {  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = curMap * 2;  							AddBias (ref Connections [position]' iNumWeight++);  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int col = 0; col < ReceptiveFieldWidth; col++)  									AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  						}  				}  			}  		});  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: if (PreviousLayer.MapCount > 1)//fully symmetrical mapped   {  	if (ReceptiveFieldSize != (StrideX * StrideY)) {  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * PreviousLayer.MapSize;  				if (prevMap == curMap) {  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = curMap * 2;  							AddBias (ref Connections [position]' iNumWeight++);  							bool outOfBounds = false;  							for (int row = -rMid; row <= rMid; row++)  								for (int col = -cMid; col <= cMid; col++) {  									if (row + (y * StrideY) < 0)  										outOfBounds = true;  									if (row + (y * StrideY) >= PreviousLayer.MapHeight)  										outOfBounds = true;  									if (col + (x * StrideX) < 0)  										outOfBounds = true;  									if (col + (x * StrideX) >= PreviousLayer.MapWidth)  										outOfBounds = true;  									if (!outOfBounds)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  									else  										outOfBounds = false;  								}  						}  				}  			}  		});  	}  	else {  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  				if (prevMap == curMap) {  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = curMap * 2;  							AddBias (ref Connections [position]' iNumWeight++);  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int col = 0; col < ReceptiveFieldWidth; col++)  									AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  						}  				}  			}  		});  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: if (ReceptiveFieldSize != (StrideX * StrideY)) {  	Parallel.For (0' MapCount' curMap =>  {  		for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  			int positionPrevMap = prevMap * PreviousLayer.MapSize;  			if (prevMap == curMap) {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = curMap * 2;  						AddBias (ref Connections [position]' iNumWeight++);  						bool outOfBounds = false;  						for (int row = -rMid; row <= rMid; row++)  							for (int col = -cMid; col <= cMid; col++) {  								if (row + (y * StrideY) < 0)  									outOfBounds = true;  								if (row + (y * StrideY) >= PreviousLayer.MapHeight)  									outOfBounds = true;  								if (col + (x * StrideX) < 0)  									outOfBounds = true;  								if (col + (x * StrideX) >= PreviousLayer.MapWidth)  									outOfBounds = true;  								if (!outOfBounds)  									AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  								else  									outOfBounds = false;  							}  					}  			}  		}  	});  }  else {  	Parallel.For (0' MapCount' curMap =>  {  		for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  			int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  			if (prevMap == curMap) {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = curMap * 2;  						AddBias (ref Connections [position]' iNumWeight++);  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int col = 0; col < ReceptiveFieldWidth; col++)  								AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  					}  			}  		}  	});  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: if (ReceptiveFieldSize != (StrideX * StrideY)) {  	Parallel.For (0' MapCount' curMap =>  {  		for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  			int positionPrevMap = prevMap * PreviousLayer.MapSize;  			if (prevMap == curMap) {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = curMap * 2;  						AddBias (ref Connections [position]' iNumWeight++);  						bool outOfBounds = false;  						for (int row = -rMid; row <= rMid; row++)  							for (int col = -cMid; col <= cMid; col++) {  								if (row + (y * StrideY) < 0)  									outOfBounds = true;  								if (row + (y * StrideY) >= PreviousLayer.MapHeight)  									outOfBounds = true;  								if (col + (x * StrideX) < 0)  									outOfBounds = true;  								if (col + (x * StrideX) >= PreviousLayer.MapWidth)  									outOfBounds = true;  								if (!outOfBounds)  									AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  								else  									outOfBounds = false;  							}  					}  			}  		}  	});  }  else {  	Parallel.For (0' MapCount' curMap =>  {  		for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  			int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  			if (prevMap == curMap) {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = curMap * 2;  						AddBias (ref Connections [position]' iNumWeight++);  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int col = 0; col < ReceptiveFieldWidth; col++)  								AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  					}  			}  		}  	});  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: Parallel.For (0' MapCount' curMap =>  {  	for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  		int positionPrevMap = prevMap * PreviousLayer.MapSize;  		if (prevMap == curMap) {  			for (int y = 0; y < MapHeight; y++)  				for (int x = 0; x < MapWidth; x++) {  					int position = x + (y * MapWidth) + (curMap * MapSize);  					int iNumWeight = curMap * 2;  					AddBias (ref Connections [position]' iNumWeight++);  					bool outOfBounds = false;  					for (int row = -rMid; row <= rMid; row++)  						for (int col = -cMid; col <= cMid; col++) {  							if (row + (y * StrideY) < 0)  								outOfBounds = true;  							if (row + (y * StrideY) >= PreviousLayer.MapHeight)  								outOfBounds = true;  							if (col + (x * StrideX) < 0)  								outOfBounds = true;  							if (col + (x * StrideX) >= PreviousLayer.MapWidth)  								outOfBounds = true;  							if (!outOfBounds)  								AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  							else  								outOfBounds = false;  						}  				}  		}  	}  });  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  	int positionPrevMap = prevMap * PreviousLayer.MapSize;  	if (prevMap == curMap) {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (curMap * MapSize);  				int iNumWeight = curMap * 2;  				AddBias (ref Connections [position]' iNumWeight++);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (row + (y * StrideY) < 0)  							outOfBounds = true;  						if (row + (y * StrideY) >= PreviousLayer.MapHeight)  							outOfBounds = true;  						if (col + (x * StrideX) < 0)  							outOfBounds = true;  						if (col + (x * StrideX) >= PreviousLayer.MapWidth)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  						else  							outOfBounds = false;  					}  			}  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: if (prevMap == curMap) {  	for (int y = 0; y < MapHeight; y++)  		for (int x = 0; x < MapWidth; x++) {  			int position = x + (y * MapWidth) + (curMap * MapSize);  			int iNumWeight = curMap * 2;  			AddBias (ref Connections [position]' iNumWeight++);  			bool outOfBounds = false;  			for (int row = -rMid; row <= rMid; row++)  				for (int col = -cMid; col <= cMid; col++) {  					if (row + (y * StrideY) < 0)  						outOfBounds = true;  					if (row + (y * StrideY) >= PreviousLayer.MapHeight)  						outOfBounds = true;  					if (col + (x * StrideX) < 0)  						outOfBounds = true;  					if (col + (x * StrideX) >= PreviousLayer.MapWidth)  						outOfBounds = true;  					if (!outOfBounds)  						AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  					else  						outOfBounds = false;  				}  		}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int y = 0; y < MapHeight; y++)  	for (int x = 0; x < MapWidth; x++) {  		int position = x + (y * MapWidth) + (curMap * MapSize);  		int iNumWeight = curMap * 2;  		AddBias (ref Connections [position]' iNumWeight++);  		bool outOfBounds = false;  		for (int row = -rMid; row <= rMid; row++)  			for (int col = -cMid; col <= cMid; col++) {  				if (row + (y * StrideY) < 0)  					outOfBounds = true;  				if (row + (y * StrideY) >= PreviousLayer.MapHeight)  					outOfBounds = true;  				if (col + (x * StrideX) < 0)  					outOfBounds = true;  				if (col + (x * StrideX) >= PreviousLayer.MapWidth)  					outOfBounds = true;  				if (!outOfBounds)  					AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  				else  					outOfBounds = false;  			}  	}  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int x = 0; x < MapWidth; x++) {  	int position = x + (y * MapWidth) + (curMap * MapSize);  	int iNumWeight = curMap * 2;  	AddBias (ref Connections [position]' iNumWeight++);  	bool outOfBounds = false;  	for (int row = -rMid; row <= rMid; row++)  		for (int col = -cMid; col <= cMid; col++) {  			if (row + (y * StrideY) < 0)  				outOfBounds = true;  			if (row + (y * StrideY) >= PreviousLayer.MapHeight)  				outOfBounds = true;  			if (col + (x * StrideX) < 0)  				outOfBounds = true;  			if (col + (x * StrideX) >= PreviousLayer.MapWidth)  				outOfBounds = true;  			if (!outOfBounds)  				AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  			else  				outOfBounds = false;  		}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: Parallel.For (0' MapCount' curMap =>  {  	for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  		int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  		if (prevMap == curMap) {  			for (int y = 0; y < MapHeight; y++)  				for (int x = 0; x < MapWidth; x++) {  					int position = x + (y * MapWidth) + (curMap * MapSize);  					int iNumWeight = curMap * 2;  					AddBias (ref Connections [position]' iNumWeight++);  					for (int row = 0; row < ReceptiveFieldHeight; row++)  						for (int col = 0; col < ReceptiveFieldWidth; col++)  							AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  				}  		}  	}  });  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  	int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  	if (prevMap == curMap) {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (curMap * MapSize);  				int iNumWeight = curMap * 2;  				AddBias (ref Connections [position]' iNumWeight++);  				for (int row = 0; row < ReceptiveFieldHeight; row++)  					for (int col = 0; col < ReceptiveFieldWidth; col++)  						AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  			}  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: if (prevMap == curMap) {  	for (int y = 0; y < MapHeight; y++)  		for (int x = 0; x < MapWidth; x++) {  			int position = x + (y * MapWidth) + (curMap * MapSize);  			int iNumWeight = curMap * 2;  			AddBias (ref Connections [position]' iNumWeight++);  			for (int row = 0; row < ReceptiveFieldHeight; row++)  				for (int col = 0; col < ReceptiveFieldWidth; col++)  					AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  		}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int y = 0; y < MapHeight; y++)  	for (int x = 0; x < MapWidth; x++) {  		int position = x + (y * MapWidth) + (curMap * MapSize);  		int iNumWeight = curMap * 2;  		AddBias (ref Connections [position]' iNumWeight++);  		for (int row = 0; row < ReceptiveFieldHeight; row++)  			for (int col = 0; col < ReceptiveFieldWidth; col++)  				AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  	}  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int x = 0; x < MapWidth; x++) {  	int position = x + (y * MapWidth) + (curMap * MapSize);  	int iNumWeight = curMap * 2;  	AddBias (ref Connections [position]' iNumWeight++);  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int col = 0; col < ReceptiveFieldWidth; col++)  			AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int map = 0; map < MapCount; map++)  	for (int y = 0; y < MapHeight; y++)  		for (int x = 0; x < MapWidth; x++) {  			a = Math.Max (0' map - (size / 2));  			// from map  			b = Math.Min (MapCount' map - (size / 2) + size);  			// to map  			int position = x + (y * MapWidth) + (map * MapSize);  			for (int f = a; f < b; f++)  				AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  		}  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int map = 0; map < MapCount; map++)  	for (int y = 0; y < MapHeight; y++)  		for (int x = 0; x < MapWidth; x++) {  			a = Math.Max (0' map - (size / 2));  			// from map  			b = Math.Min (MapCount' map - (size / 2) + size);  			// to map  			int position = x + (y * MapWidth) + (map * MapSize);  			for (int f = a; f < b; f++)  				AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  		}  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int y = 0; y < MapHeight; y++)  	for (int x = 0; x < MapWidth; x++) {  		a = Math.Max (0' map - (size / 2));  		// from map  		b = Math.Min (MapCount' map - (size / 2) + size);  		// to map  		int position = x + (y * MapWidth) + (map * MapSize);  		for (int f = a; f < b; f++)  			AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  	}  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int y = 0; y < MapHeight; y++)  	for (int x = 0; x < MapWidth; x++) {  		a = Math.Max (0' map - (size / 2));  		// from map  		b = Math.Min (MapCount' map - (size / 2) + size);  		// to map  		int position = x + (y * MapWidth) + (map * MapSize);  		for (int f = a; f < b; f++)  			AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  	}  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int x = 0; x < MapWidth; x++) {  	a = Math.Max (0' map - (size / 2));  	// from map  	b = Math.Min (MapCount' map - (size / 2) + size);  	// to map  	int position = x + (y * MapWidth) + (map * MapSize);  	for (int f = a; f < b; f++)  		AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: for (int x = 0; x < MapWidth; x++) {  	a = Math.Max (0' map - (size / 2));  	// from map  	b = Math.Min (MapCount' map - (size / 2) + size);  	// to map  	int position = x + (y * MapWidth) + (map * MapSize);  	for (int f = a; f < b; f++)  		AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: a = Math.Max (0' map - (size / 2));  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following statement contains a magic number: b = Math.Min (MapCount' map - (size / 2) + size);  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BReLU6,The following statement contains a magic number: return value < 0D ? 0D : value > 6 ? 6 : value;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,BReLU6,The following statement contains a magic number: return value < 0D ? 0D : value > 6 ? 6 : value;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,DBReLU6,The following statement contains a magic number: return value < 0D ? 0 : value > 6 ? 0 : 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: checked {  	switch (LayerType) {  	case LayerTypes.RBF:  		int index = 0;  		for (int i = 0; i < NeuronCount; i++) {  			byte[] weightImage = new byte[12];  			weightImage = Network.RbfWeights [index++].ToArray ();  			double[] realWeights = new double[7 * 12];  			int row = 0;  			for (int y = 0; y < 12; y++) {  				row = (int)weightImage [y];  				realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  				realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  				realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  				realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  				realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  				realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  				realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  			}  			foreach (Connection connection in Connections [i])  				//84x  				Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  		}  		break;  	default:  		if (Weights != null && WeightCount > 0) {  			switch (ActivationFunctionId) {  			case ActivationFunctions.ReLU:  			case ActivationFunctions.BReLU1:  			case ActivationFunctions.BReLU6:  			case ActivationFunctions.SoftReLU:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  							//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			default:  				{  					Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  						double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  						foreach (Connection connection in Connections [i])  							if (connection.ToNeuronIndex == int.MaxValue)  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  							else  								Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  					});  				}  				break;  			}  		}  		break;  	}  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: switch (LayerType) {  case LayerTypes.RBF:  	int index = 0;  	for (int i = 0; i < NeuronCount; i++) {  		byte[] weightImage = new byte[12];  		weightImage = Network.RbfWeights [index++].ToArray ();  		double[] realWeights = new double[7 * 12];  		int row = 0;  		for (int y = 0; y < 12; y++) {  			row = (int)weightImage [y];  			realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  			realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  			realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  			realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  			realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  			realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  			realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  		}  		foreach (Connection connection in Connections [i])  			//84x  			Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  	}  	break;  default:  	if (Weights != null && WeightCount > 0) {  		switch (ActivationFunctionId) {  		case ActivationFunctions.ReLU:  		case ActivationFunctions.BReLU1:  		case ActivationFunctions.BReLU6:  		case ActivationFunctions.SoftReLU:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev' 0.00001D);  						//(LayerIndex == 1) ? 1.0D : Network.RandomGenerator.NextDouble(stdDev' 0D);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		default:  			{  				Parallel.For (0' NeuronCount' Network.ParallelOption' i =>  {  					double stdDev = 1D / (double)Math.Sqrt (Connections [i].Length);  					foreach (Connection connection in Connections [i])  						if (connection.ToNeuronIndex == int.MaxValue)  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  						else  							Weights [connection.ToWeightIndex].Value = Network.RandomGenerator.NextDouble (stdDev);  				});  			}  			break;  		}  	}  	break;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int i = 0; i < NeuronCount; i++) {  	byte[] weightImage = new byte[12];  	weightImage = Network.RbfWeights [index++].ToArray ();  	double[] realWeights = new double[7 * 12];  	int row = 0;  	for (int y = 0; y < 12; y++) {  		row = (int)weightImage [y];  		realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  		realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  		realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  		realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  		realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  		realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  		realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  	}  	foreach (Connection connection in Connections [i])  		//84x  		Weights [connection.ToWeightIndex].Value = (realWeights [connection.ToNeuronIndex] == 1D) ? Network.TrainToValue : -Network.TrainToValue;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: for (int y = 0; y < 12; y++) {  	row = (int)weightImage [y];  	realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  	realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  	realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  	realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  	realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  	realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  	realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  }  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [0 + (7 * y)] = (((128 & ~row) / 128) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [1 + (7 * y)] = (((64 & ~row) / 64) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [2 + (7 * y)] = (((32 & ~row) / 32) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [3 + (7 * y)] = (((16 & ~row) / 16) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [4 + (7 * y)] = (((8 & ~row) / 8) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [5 + (7 * y)] = (((4 & ~row) / 4) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  
Magic Number,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,InitializeWeights,The following statement contains a magic number: realWeights [6 + (7 * y)] = (((2 & ~row) / 2) * 2) - 1;  
Magic Number,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,CheckStateChange,The following statement contains a magic number: if (CurrentTaskState != TaskState.Running) {  	if (CurrentTaskState == TaskState.Paused) {  		workerTimer.Stop ();  		TaskDuration.Stop ();  		SampleSpeedTimer.Stop ();  		while (CurrentTaskState == TaskState.Paused)  			Thread.Sleep (100);  		TaskDuration.Start ();  		workerTimer.Start ();  		SampleSpeedTimer.Start ();  	}  	else {  		TaskDuration.Stop ();  		SampleSpeedTimer.Stop ();  		return false;  	}  }  
Magic Number,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,CheckStateChange,The following statement contains a magic number: if (CurrentTaskState == TaskState.Paused) {  	workerTimer.Stop ();  	TaskDuration.Stop ();  	SampleSpeedTimer.Stop ();  	while (CurrentTaskState == TaskState.Paused)  		Thread.Sleep (100);  	TaskDuration.Start ();  	workerTimer.Start ();  	SampleSpeedTimer.Start ();  }  else {  	TaskDuration.Stop ();  	SampleSpeedTimer.Stop ();  	return false;  }  
Magic Number,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,CheckStateChange,The following statement contains a magic number: while (CurrentTaskState == TaskState.Paused)  	Thread.Sleep (100);  
Magic Number,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,CheckStateChange,The following statement contains a magic number: Thread.Sleep (100);  
Magic Number,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,StartTraining,The following statement contains a magic number: if (CurrentTaskState == TaskState.Stopped) {  	workerTimer = new System.Timers.Timer (1000);  	workerTimer.Elapsed += new ElapsedEventHandler (WorkerTimerElapsed);  	TaskDuration.Reset ();  	SampleSpeedTimer.Reset ();  	CurrentTaskState = TaskState.Running;  	EpochDuration = TimeSpan.Zero;  	TaskDuration.Start ();  	SampleSpeedTimer.Start ();  	workerTimer.Start ();  	await Task.Factory.StartNew (TrainingTask' cnToken).ContinueWith (t =>  {  		StopTraining ();  	});  }  
Magic Number,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,StartTraining,The following statement contains a magic number: workerTimer = new System.Timers.Timer (1000);  
Magic Number,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,SetTrainingInputSample,The following statement contains a magic number: if (TrainingRate.Distorted) {  	if (RandomGenerator.Next (100) <= TrainingRate.DistortionPercentage)  		CurrentSample = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [index]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling);  	else  		CurrentSample = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [index]];  }  else  	CurrentSample = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [index]];  
Magic Number,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,SetTrainingInputSample,The following statement contains a magic number: if (RandomGenerator.Next (100) <= TrainingRate.DistortionPercentage)  	CurrentSample = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [index]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling);  else  	CurrentSample = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [index]];  
Magic Number,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,StartTesting,The following statement contains a magic number: if (CurrentTaskState == TaskState.Stopped) {  	workerTimer = new System.Timers.Timer (1000);  	workerTimer.Elapsed += new ElapsedEventHandler (WorkerTimerElapsed);  	TaskDuration.Reset ();  	SampleSpeedTimer.Reset ();  	CurrentTaskState = TaskState.Running;  	TaskDuration.Start ();  	SampleSpeedTimer.Start ();  	workerTimer.Start ();  	await Task.Factory.StartNew (TestingTask' cnToken).ContinueWith (t =>  {  		StopTesting ();  	});  }  
Magic Number,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,StartTesting,The following statement contains a magic number: workerTimer = new System.Timers.Timer (1000);  
Duplicate Code,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The method contains a code clone-set at the following line numbers (starting from the method definition): ((86' 112)' (212' 238))
Duplicate Code,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The method contains a code clone-set at the following line numbers (starting from the method definition): ((86' 109)' (338' 361)' (212' 235))
Duplicate Code,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The method contains a code clone-set at the following line numbers (starting from the method definition): ((189' 230)' (315' 356))
Duplicate Code,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The method contains a code clone-set at the following line numbers (starting from the method definition): ((242' 261)' (368' 387))
Duplicate Code,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The method contains a code clone-set at the following line numbers (starting from the method definition): ((263' 284)' (389' 410))
Duplicate Code,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The method contains a code clone-set at the following line numbers (starting from the method definition): ((711' 735)' (786' 810))
Duplicate Code,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The method contains a code clone-set at the following line numbers (starting from the method definition): ((77' 104)' (195' 222))
Duplicate Code,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The method contains a code clone-set at the following line numbers (starting from the method definition): ((79' 125)' (375' 421))
Duplicate Code,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The method contains a code clone-set at the following line numbers (starting from the method definition): ((79' 104)' (602' 627)' (197' 222)' (375' 400))
Duplicate Code,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The method contains a code clone-set at the following line numbers (starting from the method definition): ((197' 236)' (602' 641))
Duplicate Code,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The method contains a code clone-set at the following line numbers (starting from the method definition): ((101' 141)' (152' 192))
Duplicate Code,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The method contains a code clone-set at the following line numbers (starting from the method definition): ((101' 125)' (499' 523)' (152' 176)' (397' 421))
Duplicate Code,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The method contains a code clone-set at the following line numbers (starting from the method definition): ((397' 428)' (499' 530))
Duplicate Code,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The method contains a code clone-set at the following line numbers (starting from the method definition): ((129' 155)' (240' 266))
Duplicate Code,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The method contains a code clone-set at the following line numbers (starting from the method definition): ((144' 170)' (491' 517))
Duplicate Code,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The method contains a code clone-set at the following line numbers (starting from the method definition): ((219' 252)' (263' 296))
Duplicate Code,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The method contains a code clone-set at the following line numbers (starting from the method definition): ((255' 280)' (704' 729))
Duplicate Code,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The method contains a code clone-set at the following line numbers (starting from the method definition): ((306' 336)' (540' 570))
Duplicate Code,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The method contains a code clone-set at the following line numbers (starting from the method definition): ((333' 374)' (449' 490))
Duplicate Code,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The method contains a code clone-set at the following line numbers (starting from the method definition): ((360' 393)' (587' 620))
Duplicate Code,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The method contains a code clone-set at the following line numbers (starting from the method definition): ((424' 452)' (644' 672))
Duplicate Code,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The method contains a code clone-set at the following line numbers (starting from the method definition): ((476' 502)' (689' 715))
Duplicate Code,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The method contains a code clone-set at the following line numbers (starting from the method definition): ((567' 601)' (669' 703))
Duplicate Code,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The method contains a code clone-set at the following line numbers (starting from the method definition): ((624' 655)' (712' 743))
Missing Default,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following switch statement is missing a default case: switch (LayerType) {  case LayerTypes.Input:  	ActivationFunctionId = ActivationFunctions.None;  	HasWeights = false;  	ActivationFunction = null;  	UseWeightPartitioner = false;  	WeightCount = 0;  	Weights = null;  	CalculateAction = null;  	BackpropagateAction = null;  	BackpropagateSecondDerivativesAction = null;  	break;  case LayerTypes.Local:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = totalMappings * MapSize * (ReceptiveFieldSize + 1);  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	//CalculateLocalConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		// CalculateLocalConnectedDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount)) {  					int iNumWeight = mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					int iNumWeight = mapping * (ReceptiveFieldSize + 1) * MapSize;  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							AddBias (ref Connections [position]' iNumWeight++);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				int iNumWeight = curMap * (ReceptiveFieldSize + 1) * MapSize;  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						AddBias (ref Connections [position]' iNumWeight++);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.Convolutional:  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	Parallel.For (0' PreviousLayer.MapCount' map =>  {  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskWidth) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	});  	if (!IsFullyMapped) {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = x + (y * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.ConvolutionalSubsampling:  	// Simard's implementation  	if (IsFullyMapped)  		totalMappings = PreviousLayer.MapCount * MapCount;  	else {  		if (Mappings != null) {  			if (Mappings.Mapping.Count () == PreviousLayer.MapCount * MapCount)  				totalMappings = Mappings.Mapping.Count (p => p == true);  			else  				throw new ArgumentException ("Invalid mappings definition");  		}  		else  			throw new ArgumentException ("Empty mappings definition");  	}  	HasWeights = true;  	WeightCount = (totalMappings * ReceptiveFieldSize) + MapCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	CalculateAction = CalculateCCF;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	ChangeTrainingStrategy ();  	maskWidth = PreviousLayer.MapWidth + (2 * PadX);  	maskHeight = PreviousLayer.MapHeight + (2 * PadY);  	maskSize = maskWidth * maskHeight;  	kernelTemplate = new int[ReceptiveFieldSize];  	for (int row = 0; row < ReceptiveFieldHeight; row++)  		for (int column = 0; column < ReceptiveFieldWidth; column++)  			kernelTemplate [column + (row * ReceptiveFieldWidth)] = column + (row * maskWidth);  	maskMatrix = new int[maskSize * PreviousLayer.MapCount];  	for (int i = 0; i < maskSize * PreviousLayer.MapCount; i++)  		maskMatrix [i] = -1;  	for (int map = 0; map < PreviousLayer.MapCount; map++)  		for (int y = PadY; y < PreviousLayer.MapHeight + PadY; y++)  			for (int x = PadX; x < PreviousLayer.MapWidth + PadX; x++)  				maskMatrix [x + (y * maskHeight) + (map * maskSize)] = (x - PadX) + ((y - PadY) * PreviousLayer.MapWidth) + (map * PreviousLayer.MapSize);  	if (!IsFullyMapped)// not fully mapped  	 {  		int mapping = 0;  		int[] mappingCount = new int[MapCount * PreviousLayer.MapCount];  		for (int curMap = 0; curMap < MapCount; curMap++)  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] = mapping;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					mapping++;  			}  		Parallel.For (0' MapCount' curMap =>  {  			for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  				int positionPrevMap = prevMap * maskSize;  				if (Mappings.IsMapped (curMap' prevMap' MapCount))  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mappingCount [prevMap + (curMap * PreviousLayer.MapCount)] * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  			}  		});  	}  	else// Fully mapped  	 {  		if (totalMappings > MapCount) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * maskSize;  					int mapping = prevMap + (curMap * PreviousLayer.MapCount);  					for (int y = 0; y < MapHeight; y++)  						for (int x = 0; x < MapWidth; x++) {  							int position = x + (y * MapWidth) + (curMap * MapSize);  							int iNumWeight = (mapping * ReceptiveFieldSize) + MapCount;  							AddBias (ref Connections [position]' curMap);  							int pIndex;  							for (int row = 0; row < ReceptiveFieldHeight; row++)  								for (int column = 0; column < ReceptiveFieldWidth; column++) {  									pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)] + positionPrevMap;  									if (maskMatrix [pIndex] != -1)  										AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  								}  						}  				}  			});  		}  		else// PreviousLayer has only one map         // 36*36 phantom input ' padXY=2' filterSize=5' results in 32x32 conv layer  		 {  			Parallel.For (0' MapCount' curMap =>  {  				for (int y = 0; y < MapHeight; y++)  					for (int x = 0; x < MapWidth; x++) {  						int position = x + (y * MapWidth) + (curMap * MapSize);  						int iNumWeight = MapCount + (curMap * ReceptiveFieldSize);  						AddBias (ref Connections [position]' curMap);  						int pIndex;  						for (int row = 0; row < ReceptiveFieldHeight; row++)  							for (int column = 0; column < ReceptiveFieldWidth; column++) {  								pIndex = (x * 2) + (y * 2 * maskWidth) + kernelTemplate [column + (row * ReceptiveFieldWidth)];  								if (maskMatrix [pIndex] != -1)  									AddConnection (ref Connections [position]' maskMatrix [pIndex]' iNumWeight++);  							}  					}  			});  		}  	}  	break;  case LayerTypes.AvgPooling:  case LayerTypes.MaxPooling:  	HasWeights = true;  	WeightCount = MapCount * 2;  	Weights = new Weight[WeightCount];  	if (LayerType == LayerTypes.AvgPooling) {  		CalculateAction = CalculateAveragePooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePooling;  	}  	else {  		CalculateAction = CalculateMaxPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPooling;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  										else  											outOfBounds = false;  									}  							}  					}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap) {  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								int iNumWeight = curMap * 2;  								AddBias (ref Connections [position]' iNumWeight++);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' iNumWeight);  							}  					}  				}  			});  		}  	}  	break;  case LayerTypes.AvgPoolingWeightless:  case LayerTypes.MaxPoolingWeightless:  case LayerTypes.L2Pooling:  case LayerTypes.StochasticPooling:  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	switch (LayerType) {  	case LayerTypes.AvgPoolingWeightless:  		CalculateAction = CalculateAveragePoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateAveragePoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateAveragePoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePoolingWeightless;  		break;  	case LayerTypes.MaxPoolingWeightless:  		CalculateAction = CalculateMaxPoolingWeightless;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateMaxPoolingWeightlessParallel;  		else  			BackpropagateAction = BackpropagateMaxPoolingWeightlessSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPoolingWeightless;  		break;  	case LayerTypes.L2Pooling:  		CalculateAction = CalculateL2Pooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateL2PoolingParallel;  		else  			BackpropagateAction = BackpropagateL2PoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesL2Pooling;  		break;  	case LayerTypes.StochasticPooling:  		CalculateAction = CalculateStochasticPooling;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateStochasticPoolingParallel;  		else  			BackpropagateAction = BackpropagateStochasticPoolingSerial;  		BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesStochasticPooling;  		break;  	}  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	SubsamplingScalingFactor = 1.0D / (StrideX * StrideY);  	if (PreviousLayer.MapCount > 1)//fully symmetrical mapped  	 {  		if (ReceptiveFieldSize != (StrideX * StrideY)) {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapSize;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								bool outOfBounds = false;  								for (int row = -rMid; row <= rMid; row++)  									for (int col = -cMid; col <= cMid; col++) {  										if (row + (y * StrideY) < 0)  											outOfBounds = true;  										if (row + (y * StrideY) >= PreviousLayer.MapHeight)  											outOfBounds = true;  										if (col + (x * StrideX) < 0)  											outOfBounds = true;  										if (col + (x * StrideX) >= PreviousLayer.MapWidth)  											outOfBounds = true;  										if (!outOfBounds)  											AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  										else  											outOfBounds = false;  									}  							}  				}  			});  		}  		else {  			Parallel.For (0' MapCount' curMap =>  {  				for (int prevMap = 0; prevMap < PreviousLayer.MapCount; prevMap++) {  					int positionPrevMap = prevMap * PreviousLayer.MapWidth * PreviousLayer.MapHeight;  					if (prevMap == curMap)  						for (int y = 0; y < MapHeight; y++)  							for (int x = 0; x < MapWidth; x++) {  								int position = x + (y * MapWidth) + (curMap * MapSize);  								for (int row = 0; row < ReceptiveFieldHeight; row++)  									for (int col = 0; col < ReceptiveFieldWidth; col++)  										AddConnection (ref Connections [position]' (col + (x * StrideX)) + ((row + (y * StrideY)) * PreviousLayer.MapWidth) + positionPrevMap' 0);  							}  				}  			});  		}  	}  	break;  case LayerTypes.FullyConnected:  	HasWeights = true;  	WeightCount = (PreviousLayer.NeuronCount + 1) * NeuronCount;  	Weights = new Weight[WeightCount];  	UseWeightPartitioner = WeightCount > network.ParallelTreshold;  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMax;  	else  		CalculateAction = CalculateFullyConnected;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateCCFParallel;  	else  		BackpropagateAction = BackpropagateCCFSerial;  	if (UseDropOut) {  		CalculateAction = CalculateCCFDropOut;  		if (PreviousLayer.UseNeuronPartitioner)  			BackpropagateAction = BackpropagateCCFDropOutParallel;  		else  			BackpropagateAction = BackpropagateCCFDropOutSerial;  	}  	if (UseWeightPartitioner) {  		EraseGradientWeights = EraseGradientsWeightsParallel;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxParallel;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  	}  	else {  		EraseGradientWeights = EraseGradientsWeightsSerial;  		if ((ActivationFunctionId == ActivationFunctions.SoftMax) && ((Network.TrainingStrategy == TrainingStrategy.SGDLevenbergMarquardtModA) || (Network.TrainingStrategy == TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA)))  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxSerial;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		Parallel.For (0' MapCount' curMap =>  {  			for (int yc = 0; yc < MapHeight; yc++)  				for (int xc = 0; xc < MapWidth; xc++) {  					int position = xc + (yc * MapWidth) + (curMap * MapSize);  					AddBias (ref Connections [position]' iNumWeight++);  					for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  						for (int y = 0; y < PreviousLayer.MapHeight; y++)  							for (int x = 0; x < PreviousLayer.MapWidth; x++)  								AddConnection (ref Connections [position]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  				}  		});  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++) {  			AddBias (ref Connections [y]' iNumWeight++);  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  		}  	}  	break;  case LayerTypes.RBF:  	HasWeights = true;  	WeightCount = PreviousLayer.NeuronCount * NeuronCount;  	// no biasses  	Weights = new Weight[WeightCount];  	if (ActivationFunctionId == ActivationFunctions.SoftMax)  		CalculateAction = CalculateSoftMaxRBF;  	else  		CalculateAction = CalculateRBF;  	BackpropagateAction = BackpropagateRBF;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesRBF;  	if (UseWeightPartitioner)  		EraseGradientWeights = EraseGradientsWeightsParallel;  	else  		EraseGradientWeights = EraseGradientsWeightsSerial;  	ChangeTrainingStrategy ();  	if (UseMapInfo) {  		int iNumWeight = 0;  		for (int n = 0; n < NeuronCount; n++)  			for (int prevMaps = 0; prevMaps < PreviousLayer.MapCount; prevMaps++)  				for (int y = 0; y < PreviousLayer.MapHeight; y++)  					for (int x = 0; x < PreviousLayer.MapWidth; x++)  						AddConnection (ref Connections [n]' (x + (y * PreviousLayer.MapWidth) + (prevMaps * PreviousLayer.MapSize))' iNumWeight++);  	}  	else {  		int iNumWeight = 0;  		for (int y = 0; y < NeuronCount; y++)  			for (int x = 0; x < PreviousLayer.NeuronCount; x++)  				AddConnection (ref Connections [y]' x' iNumWeight++);  	}  	break;  case LayerTypes.LocalResponseNormalization:  	// same map  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalResponseNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalResponseNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalResponseNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalResponseNormalizationCM:  	// across maps  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztionCM layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	int size = 9;  	int a' b;  	for (int map = 0; map < MapCount; map++)  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				a = Math.Max (0' map - (size / 2));  				// from map  				b = Math.Min (MapCount' map - (size / 2) + size);  				// to map  				int position = x + (y * MapWidth) + (map * MapSize);  				for (int f = a; f < b; f++)  					AddConnection (ref Connections [position]' x + (y * MapWidth) + (f * MapSize)' -1);  			}  	CalculateAction = CalculateLocalResponseNormalizationCM;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMParallel;  	else  		BackpropagateAction = BackpropagationLocalResponseNormalizationCMSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalResponseNormalizationCM;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  case LayerTypes.LocalContrastNormalization:  	if (ActivationFunctionId != ActivationFunctions.None)  		throw new Exception ("LocalContrastNormaliztion layer cannot have an ActivationFunction' specify ActivationFunctions.None");  	if (MapHeight != PreviousLayer.MapHeight)  		throw new Exception ("MapHeight must be equal to the MapHeight of the previous layer");  	if (MapWidth != PreviousLayer.MapWidth)  		throw new Exception ("MapWidth must be equal to the MapWidth of the previous layer");  	Gaussian2DKernel = MathUtil.CreateGaussian2DKernel (ReceptiveFieldWidth' ReceptiveFieldHeight);  	HasWeights = false;  	WeightCount = 0;  	Weights = null;  	Parallel.For (0' MapCount' map =>  {  		for (int y = 0; y < MapHeight; y++)  			for (int x = 0; x < MapWidth; x++) {  				int position = x + (y * MapWidth) + (map * MapSize);  				bool outOfBounds = false;  				for (int row = -rMid; row <= rMid; row++)  					for (int col = -cMid; col <= cMid; col++) {  						if (col + x < 0)  							outOfBounds = true;  						if (col + x >= MapWidth)  							outOfBounds = true;  						if (row + y < 0)  							outOfBounds = true;  						if (row + y >= MapHeight)  							outOfBounds = true;  						if (!outOfBounds)  							AddConnection (ref Connections [position]' (col + x) + ((row + y) * MapWidth) + (map * MapSize)' -1);  						else  							outOfBounds = false;  					}  			}  	});  	CalculateAction = CalculateLocalContrastNormalization;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateLocalContrastNormalizationParallel;  	else  		BackpropagateAction = BackpropagateLocalContrastNormalizationSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativeLocalContrastNormalization;  	EraseGradientWeights = NoErase;  	UpdateWeights = NoUpdate;  	break;  }  
Missing Default,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following switch statement is missing a default case: switch (LayerType) {  case LayerTypes.AvgPoolingWeightless:  	CalculateAction = CalculateAveragePoolingWeightless;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateAveragePoolingWeightlessParallel;  	else  		BackpropagateAction = BackpropagateAveragePoolingWeightlessSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesAveragePoolingWeightless;  	break;  case LayerTypes.MaxPoolingWeightless:  	CalculateAction = CalculateMaxPoolingWeightless;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateMaxPoolingWeightlessParallel;  	else  		BackpropagateAction = BackpropagateMaxPoolingWeightlessSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesMaxPoolingWeightless;  	break;  case LayerTypes.L2Pooling:  	CalculateAction = CalculateL2Pooling;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateL2PoolingParallel;  	else  		BackpropagateAction = BackpropagateL2PoolingSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesL2Pooling;  	break;  case LayerTypes.StochasticPooling:  	CalculateAction = CalculateStochasticPooling;  	if (PreviousLayer.UseNeuronPartitioner)  		BackpropagateAction = BackpropagateStochasticPoolingParallel;  	else  		BackpropagateAction = BackpropagateStochasticPoolingSerial;  	BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesStochasticPooling;  	break;  }  
Missing Default,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,Layer,The following switch statement is missing a default case: switch (ActivationFunctionId) {  case ActivationFunctions.Abs:  	ActivationFunction = Abs;  	DerivativeActivationFunction = DAbs;  	break;  case ActivationFunctions.AbsTanh:  	ActivationFunction = AbsTanh;  	DerivativeActivationFunction = DAbsTanh;  	break;  case ActivationFunctions.BReLU1:  	ActivationFunction = BReLU1;  	DerivativeActivationFunction = DBReLU1;  	break;  case ActivationFunctions.BReLU6:  	ActivationFunction = BReLU6;  	DerivativeActivationFunction = DBReLU6;  	break;  case ActivationFunctions.Gaussian:  	ActivationFunction = Gaussian;  	DerivativeActivationFunction = DGaussian;  	break;  case ActivationFunctions.Ident:  	ActivationFunction = Ident;  	DerivativeActivationFunction = DIdent;  	break;  case ActivationFunctions.Logistic:  	ActivationFunction = Logistic;  	DerivativeActivationFunction = DLogistic;  	break;  case ActivationFunctions.None:  	ActivationFunction = null;  	DerivativeActivationFunction = null;  	break;  case ActivationFunctions.Tanh:  	ActivationFunction = Tanh;  	DerivativeActivationFunction = DTanh;  	break;  case ActivationFunctions.Ramp:  	ActivationFunction = Ramp;  	DerivativeActivationFunction = DRamp;  	break;  case ActivationFunctions.ReLU:  	ActivationFunction = ReLU;  	DerivativeActivationFunction = DReLU;  	break;  case ActivationFunctions.SoftMax:  	ActivationFunction = Ident;  	DerivativeActivationFunction = DSoftMax;  	break;  case ActivationFunctions.SoftReLU:  	ActivationFunction = SoftReLU;  	DerivativeActivationFunction = DSoftReLU;  	break;  case ActivationFunctions.SoftSign:  	ActivationFunction = SoftSign;  	DerivativeActivationFunction = DSoftSign;  	break;  case ActivationFunctions.Square:  	ActivationFunction = Square;  	DerivativeActivationFunction = DSquare;  	break;  case ActivationFunctions.SquareRoot:  	ActivationFunction = SquareRoot;  	DerivativeActivationFunction = DSquareRoot;  	break;  case ActivationFunctions.STanh:  	ActivationFunction = STanh;  	DerivativeActivationFunction = DSTanh;  	break;  }  
Missing Default,CNNWB.CNN,Layer,C:\repos\supby_cnnwb\CNNWB.CNN\Layer.cs,ChangeTrainingStrategy,The following switch statement is missing a default case: switch (Network.TrainingStrategy) {  case TrainingStrategy.SGD:  	if (UseWeightPartitioner)  		UpdateWeights = delegate (int batchSize) {  			UpdateWeightsSGDParallel (batchSize);  		};  	else  		UpdateWeights = delegate (int batchSize) {  			UpdateWeighsSGDSerial (batchSize);  		};  	break;  case TrainingStrategy.SGDLevenbergMarquardt:  	if (UseWeightPartitioner)  		UpdateWeights = delegate (int batchSize) {  			UpdateWeightsSGDLMParallel (batchSize);  		};  	else  		UpdateWeights = delegate (int batchSize) {  			UpdateWeightsSGDLMSerial (batchSize);  		};  	if ((Network.LossFunction == LossFunctions.CrossEntropy) && (ActivationFunctionId == ActivationFunctions.SoftMax)) {  		if (UseWeightPartitioner)  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	break;  case TrainingStrategy.SGDLevenbergMarquardtModA:  	if (UseWeightPartitioner)  		UpdateWeights = delegate (int batchSize) {  			UpdateWeightsSGDLMParallel (batchSize);  		};  	else  		UpdateWeights = delegate (int batchSize) {  			UpdateWeightsSGDLMSerial (batchSize);  		};  	if ((Network.LossFunction == LossFunctions.CrossEntropy) && (ActivationFunctionId == ActivationFunctions.SoftMax)) {  		if (UseWeightPartitioner)  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxParallel;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxSerial;  	}  	break;  case TrainingStrategy.SGDM:  	if (UseWeightPartitioner)  		UpdateWeights = delegate (int batchSize) {  			UpdateWeightsSGDMParallel (batchSize);  		};  	else  		UpdateWeights = delegate (int batchSize) {  			UpdateWeighsSGDMSerial (batchSize);  		};  	break;  case TrainingStrategy.MiniBatchSGD:  	if (UseWeightPartitioner)  		UpdateWeights = delegate (int batchSize) {  			UpdateWeightsMiniBatchSGDParallel (batchSize);  		};  	else  		UpdateWeights = delegate (int batchSize) {  			UpdateWeightsMiniBatchSGDSerial (batchSize);  		};  	break;  case TrainingStrategy.MiniBatchSGDLevenbergMarquardt:  	if (UseWeightPartitioner)  		UpdateWeights = delegate (int batchSize) {  			UpdateWeightsMiniBatchSGDLMParallel (batchSize);  		};  	else  		UpdateWeights = delegate (int batchSize) {  			UpdateWeightsMiniBatchSGDLMSerial (batchSize);  		};  	if ((Network.LossFunction == LossFunctions.CrossEntropy) && (ActivationFunctionId == ActivationFunctions.SoftMax)) {  		if (UseWeightPartitioner)  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFParallel;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSerial;  	}  	break;  case TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA:  	if (UseWeightPartitioner)  		UpdateWeights = delegate (int batchSize) {  			UpdateWeightsMiniBatchSGDLMParallel (batchSize);  		};  	else  		UpdateWeights = delegate (int batchSize) {  			UpdateWeightsMiniBatchSGDLMSerial (batchSize);  		};  	if ((Network.LossFunction == LossFunctions.CrossEntropy) && (ActivationFunctionId == ActivationFunctions.SoftMax)) {  		if (UseWeightPartitioner)  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxParallel;  		else  			BackpropagateSecondDerivativesAction = BackpropagateSecondDerivativesCCFSoftMaxSerial;  	}  	break;  case TrainingStrategy.MiniBatchSGDM:  	if (UseWeightPartitioner)  		UpdateWeights = delegate (int batchSize) {  			UpdateWeightsMiniBatchSGDMParallel (batchSize);  		};  	else  		UpdateWeights = delegate (int batchSize) {  			UpdateWeightsMiniBatchSGDMSerial (batchSize);  		};  	break;  }  
Missing Default,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,NeuralNetwork,The following switch statement is missing a default case: switch (LossFunction) {  case LossFunctions.MeanSquareError:  	LossFunctionAction = MeanSquareErrorLossFunction;  	GetSampleLoss = GetSampleLossMSE;  	Recognized = ArgMax;  	break;  case LossFunctions.CrossEntropy:  	LossFunctionAction = CrossEntropyLossFunction;  	GetSampleLoss = GetSampleLossCrossEntropy;  	Recognized = ArgMax;  	break;  }  
Missing Default,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The following switch statement is missing a default case: switch (TrainingStrategy) {  case TrainingStrategy.SGD:  case TrainingStrategy.SGDLevenbergMarquardt:  case TrainingStrategy.SGDLevenbergMarquardtModA:  case TrainingStrategy.SGDM:  	switch (LossFunction) {  	case LossFunctions.MeanSquareError:  		if (TrainingRate.Distorted) {  			for (SampleIndex = 0; SampleIndex < DataProvider.TrainingSamplesCount; SampleIndex++) {  				if (RandomGenerator.NextPercentage () < TrainingRate.DistortionPercentage) {  					if (SubstractMean) {  						double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling).SubstractMean (DataProvider);  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = data [i];  					}  					else {  						CurrentSample = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling);  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = (((double)CurrentSample.Image [i] / 255D) * Spread) + Min;  					}  				}  				else {  					if (SubstractMean) {  						double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider);  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = data [i];  					}  					else  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min;  				}  				sampleLabel = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Label;  				// fprop  				for (int i = 1; i < Layers.Length; i++)  					Layers [i].CalculateAction ();  				// Mean Square Error loss  				patternLoss = 0D;  				for (int i = 0; i < ClassCount; i++) {  					if (i == sampleLabel)  						patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output - TrainToValue);  					else  						patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output + TrainToValue);  				}  				patternLoss *= 0.5D;  				totLoss += patternLoss;  				AvgTrainLoss = totLoss / (SampleIndex + 1);  				bestIndex = 0;  				maxValue = LastLayer.Neurons [0].Output;  				for (int i = 1; i < ClassCount; i++) {  					if (LastLayer.Neurons [i].Output > maxValue) {  						maxValue = LastLayer.Neurons [i].Output;  						bestIndex = i;  					}  				}  				if (sampleLabel != bestIndex)  					TrainErrors++;  				if (patternLoss > prevAvgLoss) {  					for (int i = 0; i < ClassCount; i++)  						LastLayer.Neurons [i].D1ErrX = LastLayer.Neurons [i].Output + TrainToValue;  					LastLayer.Neurons [sampleLabel].D1ErrX = LastLayer.Neurons [sampleLabel].Output - TrainToValue;  					// bprop  					for (int i = Layers.Length - 1; i > 1; i--) {  						Layers [i].EraseGradientWeights ();  						Layers [i].BackpropagateAction ();  						Layers [i].UpdateWeights (1);  					}  				}  				if (CurrentTaskState != TaskState.Running)  					if (!CheckStateChange ())  						break;  			}  		}  		else {  			for (SampleIndex = 0; SampleIndex < DataProvider.TrainingSamplesCount; SampleIndex++) {  				if (SubstractMean) {  					double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider);  					for (int i = 0; i < sampleSize; i++)  						Layers [0].Neurons [i].Output = data [i];  				}  				else  					for (int i = 0; i < sampleSize; i++)  						Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min;  				sampleLabel = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Label;  				// fprop  				for (int i = 1; i < Layers.Length; i++)  					Layers [i].CalculateAction ();  				// Mean Square Error loss  				patternLoss = 0D;  				for (int i = 0; i < ClassCount; i++) {  					if (i == sampleLabel)  						patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output - TrainToValue);  					else  						patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output + TrainToValue);  				}  				patternLoss *= 0.5D;  				totLoss += patternLoss;  				AvgTrainLoss = totLoss / (SampleIndex + 1);  				bestIndex = 0;  				maxValue = LastLayer.Neurons [0].Output;  				for (int i = 1; i < ClassCount; i++) {  					if (LastLayer.Neurons [i].Output > maxValue) {  						maxValue = LastLayer.Neurons [i].Output;  						bestIndex = i;  					}  				}  				if (sampleLabel != bestIndex)  					TrainErrors++;  				if (patternLoss > prevAvgLoss) {  					for (int i = 0; i < ClassCount; i++)  						LastLayer.Neurons [i].D1ErrX = LastLayer.Neurons [i].Output + TrainToValue;  					LastLayer.Neurons [sampleLabel].D1ErrX = LastLayer.Neurons [sampleLabel].Output - TrainToValue;  					// bprop  					for (int i = Layers.Length - 1; i > 1; i--) {  						Layers [i].EraseGradientWeights ();  						Layers [i].BackpropagateAction ();  						Layers [i].UpdateWeights (1);  					}  				}  				if (CurrentTaskState != TaskState.Running)  					if (!CheckStateChange ())  						break;  			}  		}  		break;  	case LossFunctions.CrossEntropy:  		if (TrainingRate.Distorted) {  			for (SampleIndex = 0; SampleIndex < DataProvider.TrainingSamplesCount; SampleIndex++) {  				if (RandomGenerator.NextPercentage () < TrainingRate.DistortionPercentage) {  					if (SubstractMean) {  						double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling).SubstractMean (DataProvider);  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = data [i];  					}  					else {  						CurrentSample = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling);  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = (((double)CurrentSample.Image [i] / 255D) * Spread) + Min;  					}  				}  				else {  					if (SubstractMean) {  						double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider);  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = data [i];  					}  					else  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min;  				}  				sampleLabel = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Label;  				// fprop  				for (int i = 1; i < Layers.Length; i++)  					Layers [i].CalculateAction ();  				// Cross Entropy Loss  				patternLoss = -0.5D * MathUtil.Log (LastLayer.Neurons [sampleLabel].Output);  				totLoss += patternLoss;  				AvgTrainLoss = totLoss / (SampleIndex + 1);  				bestIndex = 0;  				maxValue = LastLayer.Neurons [0].Output;  				for (int i = 1; i < ClassCount; i++) {  					if (LastLayer.Neurons [i].Output > maxValue) {  						maxValue = LastLayer.Neurons [i].Output;  						bestIndex = i;  					}  				}  				if (sampleLabel != bestIndex)  					TrainErrors++;  				if (patternLoss > prevAvgLoss) {  					for (int i = 0; i < ClassCount; i++)  						LastLayer.Neurons [i].D1ErrX = LastLayer.Neurons [i].Output;  					LastLayer.Neurons [sampleLabel].D1ErrX = LastLayer.Neurons [sampleLabel].Output - TrainToValue;  					// bprop  					for (int i = Layers.Length - 1; i > 1; i--) {  						Layers [i].EraseGradientWeights ();  						Layers [i].BackpropagateAction ();  						Layers [i].UpdateWeights (1);  					}  				}  				if (CurrentTaskState != TaskState.Running)  					if (!CheckStateChange ())  						break;  			}  		}  		else {  			for (SampleIndex = 0; SampleIndex < DataProvider.TrainingSamplesCount; SampleIndex++) {  				if (SubstractMean) {  					double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider);  					for (int i = 0; i < sampleSize; i++)  						Layers [0].Neurons [i].Output = data [i];  				}  				else  					for (int i = 0; i < sampleSize; i++)  						Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min;  				sampleLabel = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Label;  				// fprop  				for (int i = 1; i < Layers.Length; i++)  					Layers [i].CalculateAction ();  				// Cross Entropy Loss  				patternLoss = -0.5D * MathUtil.Log (LastLayer.Neurons [sampleLabel].Output);  				totLoss += patternLoss;  				AvgTrainLoss = totLoss / (SampleIndex + 1);  				bestIndex = 0;  				maxValue = LastLayer.Neurons [0].Output;  				for (int i = 1; i < ClassCount; i++) {  					if (LastLayer.Neurons [i].Output > maxValue) {  						maxValue = LastLayer.Neurons [i].Output;  						bestIndex = i;  					}  				}  				if (sampleLabel != bestIndex)  					TrainErrors++;  				if (patternLoss > prevAvgLoss) {  					for (int i = 0; i < ClassCount; i++)  						LastLayer.Neurons [i].D1ErrX = LastLayer.Neurons [i].Output;  					LastLayer.Neurons [sampleLabel].D1ErrX = LastLayer.Neurons [sampleLabel].Output - TrainToValue;  					// bprop  					for (int i = Layers.Length - 1; i > 1; i--) {  						Layers [i].EraseGradientWeights ();  						Layers [i].BackpropagateAction ();  						Layers [i].UpdateWeights (1);  					}  				}  				if (CurrentTaskState != TaskState.Running)  					if (!CheckStateChange ())  						break;  			}  		}  		break;  	}  	break;  case TrainingStrategy.MiniBatchSGD:  case TrainingStrategy.MiniBatchSGDLevenbergMarquardt:  case TrainingStrategy.MiniBatchSGDLevenbergMarquardtModA:  case TrainingStrategy.MiniBatchSGDM:  	switch (LossFunction) {  	case LossFunctions.MeanSquareError:  		if (TrainingRate.Distorted) {  			for (SampleIndex = 0; SampleIndex < end; SampleIndex += TrainingRate.BatchSize) {  				for (int i = Layers.Length - 1; i > 1; i--)  					Layers [i].EraseGradientWeights ();  				for (int batchIndex = 0; batchIndex < TrainingRate.BatchSize; batchIndex++) {  					if (RandomGenerator.NextPercentage () < TrainingRate.DistortionPercentage) {  						if (SubstractMean) {  							double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling).SubstractMean (DataProvider);  							for (int i = 0; i < sampleSize; i++)  								Layers [0].Neurons [i].Output = data [i];  						}  						else {  							CurrentSample = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling);  							for (int i = 0; i < sampleSize; i++)  								Layers [0].Neurons [i].Output = (((double)CurrentSample.Image [i] / 255D) * Spread) + Min;  						}  					}  					else {  						if (SubstractMean) {  							double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].SubstractMean (DataProvider);  							for (int i = 0; i < sampleSize; i++)  								Layers [0].Neurons [i].Output = data [i];  						}  						else  							for (int i = 0; i < sampleSize; i++)  								Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Image [i] / 255D) * Spread) + Min;  					}  					sampleLabel = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Label;  					// fprop  					for (int i = 1; i < Layers.Length; i++)  						Layers [i].CalculateAction ();  					// Mean Square Error loss  					patternLoss = 0D;  					for (int i = 0; i < ClassCount; i++) {  						if (i == sampleLabel)  							patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output - TrainToValue);  						else  							patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output + TrainToValue);  					}  					patternLoss *= 0.5D;  					totLoss += patternLoss;  					AvgTrainLoss = totLoss / (SampleIndex + batchIndex + 1);  					bestIndex = 0;  					maxValue = LastLayer.Neurons [0].Output;  					for (int i = 1; i < ClassCount; i++) {  						if (LastLayer.Neurons [i].Output > maxValue) {  							maxValue = LastLayer.Neurons [i].Output;  							bestIndex = i;  						}  					}  					if (sampleLabel != bestIndex)  						TrainErrors++;  					for (int i = 0; i < ClassCount; i++)  						LastLayer.Neurons [i].D1ErrX = LastLayer.Neurons [i].Output + TrainToValue;  					LastLayer.Neurons [sampleLabel].D1ErrX = LastLayer.Neurons [sampleLabel].Output - TrainToValue;  					// bprop  					for (int i = Layers.Length - 1; i > 1; i--)  						Layers [i].BackpropagateAction ();  				}  				for (int i = Layers.Length - 1; i > 1; i--)  					Layers [i].UpdateWeights (TrainingRate.BatchSize);  				if (CurrentTaskState != TaskState.Running)  					if (!CheckStateChange ())  						break;  			}  			if (finalBatch > 0) {  				for (int i = Layers.Length - 1; i > 1; i--)  					Layers [i].EraseGradientWeights ();  				for (SampleIndex = DataProvider.TrainingSamplesCount - finalBatch; SampleIndex < DataProvider.TrainingSamplesCount; SampleIndex++) {  					if (RandomGenerator.NextPercentage () < TrainingRate.DistortionPercentage) {  						if (SubstractMean) {  							double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling).SubstractMean (DataProvider);  							for (int i = 0; i < sampleSize; i++)  								Layers [0].Neurons [i].Output = data [i];  						}  						else {  							CurrentSample = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling);  							for (int i = 0; i < sampleSize; i++)  								Layers [0].Neurons [i].Output = (((double)CurrentSample.Image [i] / 255D) * Spread) + Min;  						}  					}  					else {  						if (SubstractMean) {  							double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider);  							for (int i = 0; i < sampleSize; i++)  								Layers [0].Neurons [i].Output = data [i];  						}  						else  							for (int i = 0; i < sampleSize; i++)  								Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min;  					}  					sampleLabel = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Label;  					// fprop  					for (int i = 1; i < Layers.Length; i++)  						Layers [i].CalculateAction ();  					// Mean Square Error loss  					patternLoss = 0D;  					for (int i = 0; i < ClassCount; i++) {  						if (i == sampleLabel)  							patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output - TrainToValue);  						else  							patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output + TrainToValue);  					}  					patternLoss *= 0.5D;  					totLoss += patternLoss;  					AvgTrainLoss = totLoss / (SampleIndex + 1);  					bestIndex = 0;  					maxValue = LastLayer.Neurons [0].Output;  					for (int i = 1; i < ClassCount; i++) {  						if (LastLayer.Neurons [i].Output > maxValue) {  							maxValue = LastLayer.Neurons [i].Output;  							bestIndex = i;  						}  					}  					if (sampleLabel != bestIndex)  						TrainErrors++;  					for (int i = 0; i < ClassCount; i++)  						LastLayer.Neurons [i].D1ErrX = LastLayer.Neurons [i].Output + TrainToValue;  					LastLayer.Neurons [sampleLabel].D1ErrX = LastLayer.Neurons [sampleLabel].Output - TrainToValue;  					// bprop  					for (int i = Layers.Length - 1; i > 1; i--)  						Layers [i].BackpropagateAction ();  				}  				for (int i = Layers.Length - 1; i > 1; i--)  					Layers [i].UpdateWeights (finalBatch);  				if (CurrentTaskState != TaskState.Running)  					if (!CheckStateChange ())  						break;  			}  		}  		else {  			for (SampleIndex = 0; SampleIndex < end; SampleIndex += TrainingRate.BatchSize) {  				for (int i = Layers.Length - 1; i > 1; i--)  					Layers [i].EraseGradientWeights ();  				for (int batchIndex = 0; batchIndex < TrainingRate.BatchSize; batchIndex++) {  					if (SubstractMean) {  						double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].SubstractMean (DataProvider);  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = data [i];  					}  					else  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Image [i] / 255D) * Spread) + Min;  					sampleLabel = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Label;  					// fprop  					for (int i = 1; i < Layers.Length; i++)  						Layers [i].CalculateAction ();  					// Mean Square Error loss  					patternLoss = 0D;  					for (int i = 0; i < ClassCount; i++) {  						if (i == sampleLabel)  							patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output - TrainToValue);  						else  							patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output + TrainToValue);  					}  					patternLoss *= 0.5D;  					totLoss += patternLoss;  					AvgTrainLoss = totLoss / (SampleIndex + batchIndex + 1);  					bestIndex = 0;  					maxValue = LastLayer.Neurons [0].Output;  					for (int i = 1; i < ClassCount; i++) {  						if (LastLayer.Neurons [i].Output > maxValue) {  							maxValue = LastLayer.Neurons [i].Output;  							bestIndex = i;  						}  					}  					if (sampleLabel != bestIndex)  						TrainErrors++;  					for (int i = 0; i < ClassCount; i++)  						LastLayer.Neurons [i].D1ErrX = LastLayer.Neurons [i].Output + TrainToValue;  					LastLayer.Neurons [sampleLabel].D1ErrX = LastLayer.Neurons [sampleLabel].Output - TrainToValue;  					// bprop  					for (int i = Layers.Length - 1; i > 1; i--)  						Layers [i].BackpropagateAction ();  				}  				for (int i = Layers.Length - 1; i > 1; i--)  					Layers [i].UpdateWeights (TrainingRate.BatchSize);  				if (CurrentTaskState != TaskState.Running)  					if (!CheckStateChange ())  						break;  			}  			if (finalBatch > 0) {  				for (int i = Layers.Length - 1; i > 1; i--)  					Layers [i].EraseGradientWeights ();  				for (SampleIndex = DataProvider.TrainingSamplesCount - finalBatch; SampleIndex < DataProvider.TrainingSamplesCount; SampleIndex++) {  					if (SubstractMean) {  						double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider);  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = data [i];  					}  					else  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min;  					sampleLabel = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Label;  					// fprop  					for (int i = 1; i < Layers.Length; i++)  						Layers [i].CalculateAction ();  					// Mean Square Error loss  					patternLoss = 0D;  					for (int i = 0; i < ClassCount; i++) {  						if (i == sampleLabel)  							patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output - TrainToValue);  						else  							patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output + TrainToValue);  					}  					patternLoss *= 0.5D;  					totLoss += patternLoss;  					AvgTrainLoss = totLoss / (SampleIndex + 1);  					bestIndex = 0;  					maxValue = LastLayer.Neurons [0].Output;  					for (int i = 1; i < ClassCount; i++) {  						if (LastLayer.Neurons [i].Output > maxValue) {  							maxValue = LastLayer.Neurons [i].Output;  							bestIndex = i;  						}  					}  					if (sampleLabel != bestIndex)  						TrainErrors++;  					for (int i = 0; i < ClassCount; i++)  						LastLayer.Neurons [i].D1ErrX = LastLayer.Neurons [i].Output + TrainToValue;  					LastLayer.Neurons [sampleLabel].D1ErrX = LastLayer.Neurons [sampleLabel].Output - TrainToValue;  					// bprop  					for (int i = Layers.Length - 1; i > 1; i--)  						Layers [i].BackpropagateAction ();  				}  			}  			for (int i = Layers.Length - 1; i > 1; i--)  				Layers [i].UpdateWeights (finalBatch);  			if (CurrentTaskState != TaskState.Running)  				if (!CheckStateChange ())  					break;  		}  		break;  	case LossFunctions.CrossEntropy:  		if (TrainingRate.Distorted) {  			for (SampleIndex = 0; SampleIndex < end; SampleIndex += TrainingRate.BatchSize) {  				for (int i = Layers.Length - 1; i > 1; i--)  					Layers [i].EraseGradientWeights ();  				for (int batchIndex = 0; batchIndex < TrainingRate.BatchSize; batchIndex++) {  					if (RandomGenerator.NextPercentage () < TrainingRate.DistortionPercentage) {  						if (SubstractMean) {  							double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling).SubstractMean (DataProvider);  							for (int i = 0; i < sampleSize; i++)  								Layers [0].Neurons [i].Output = data [i];  						}  						else {  							CurrentSample = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling);  							for (int i = 0; i < sampleSize; i++)  								Layers [0].Neurons [i].Output = (((double)CurrentSample.Image [i] / 255D) * Spread) + Min;  						}  					}  					else {  						if (SubstractMean) {  							double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].SubstractMean (DataProvider);  							for (int i = 0; i < sampleSize; i++)  								Layers [0].Neurons [i].Output = data [i];  						}  						else  							for (int i = 0; i < sampleSize; i++)  								Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Image [i] / 255D) * Spread) + Min;  					}  					sampleLabel = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Label;  					// fprop  					for (int i = 1; i < Layers.Length; i++)  						Layers [i].CalculateAction ();  					// Cross Entropy Loss  					patternLoss = -0.5D * MathUtil.Log (LastLayer.Neurons [sampleLabel].Output);  					totLoss += patternLoss;  					AvgTrainLoss = totLoss / (SampleIndex + batchIndex + 1);  					bestIndex = 0;  					maxValue = LastLayer.Neurons [0].Output;  					for (int i = 1; i < ClassCount; i++) {  						if (LastLayer.Neurons [i].Output > maxValue) {  							maxValue = LastLayer.Neurons [i].Output;  							bestIndex = i;  						}  					}  					if (sampleLabel != bestIndex)  						TrainErrors++;  					for (int i = 0; i < ClassCount; i++)  						LastLayer.Neurons [i].D1ErrX = LastLayer.Neurons [i].Output;  					LastLayer.Neurons [sampleLabel].D1ErrX = LastLayer.Neurons [sampleLabel].Output - TrainToValue;  					// bprop  					for (int i = Layers.Length - 1; i > 1; i--)  						Layers [i].BackpropagateAction ();  				}  				for (int i = Layers.Length - 1; i > 1; i--)  					Layers [i].UpdateWeights (TrainingRate.BatchSize);  				if (CurrentTaskState != TaskState.Running)  					if (!CheckStateChange ())  						break;  			}  			if (finalBatch > 0) {  				for (int i = Layers.Length - 1; i > 1; i--)  					Layers [i].EraseGradientWeights ();  				for (SampleIndex = DataProvider.TrainingSamplesCount - finalBatch; SampleIndex < DataProvider.TrainingSamplesCount; SampleIndex++) {  					if (RandomGenerator.NextPercentage () < TrainingRate.DistortionPercentage) {  						if (SubstractMean) {  							double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling).SubstractMean (DataProvider);  							for (int i = 0; i < sampleSize; i++)  								Layers [0].Neurons [i].Output = data [i];  						}  						else {  							CurrentSample = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling);  							for (int i = 0; i < sampleSize; i++)  								Layers [0].Neurons [i].Output = (((double)CurrentSample.Image [i] / 255D) * Spread) + Min;  						}  					}  					else {  						if (SubstractMean) {  							double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider);  							for (int i = 0; i < sampleSize; i++)  								Layers [0].Neurons [i].Output = data [i];  						}  						else  							for (int i = 0; i < sampleSize; i++)  								Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min;  					}  					sampleLabel = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Label;  					// fprop  					for (int i = 1; i < Layers.Length; i++)  						Layers [i].CalculateAction ();  					// Cross Entropy Loss  					patternLoss = -0.5D * MathUtil.Log (LastLayer.Neurons [sampleLabel].Output);  					totLoss += patternLoss;  					AvgTrainLoss = totLoss / (SampleIndex + 1);  					bestIndex = 0;  					maxValue = LastLayer.Neurons [0].Output;  					for (int i = 1; i < ClassCount; i++) {  						if (LastLayer.Neurons [i].Output > maxValue) {  							maxValue = LastLayer.Neurons [i].Output;  							bestIndex = i;  						}  					}  					if (sampleLabel != bestIndex)  						TrainErrors++;  					for (int i = 0; i < ClassCount; i++)  						LastLayer.Neurons [i].D1ErrX = LastLayer.Neurons [i].Output;  					LastLayer.Neurons [sampleLabel].D1ErrX = LastLayer.Neurons [sampleLabel].Output - TrainToValue;  					// bprop  					for (int i = Layers.Length - 1; i > 1; i--)  						Layers [i].BackpropagateAction ();  				}  				for (int i = Layers.Length - 1; i > 1; i--)  					Layers [i].UpdateWeights (finalBatch);  				if (CurrentTaskState != TaskState.Running)  					if (!CheckStateChange ())  						break;  			}  		}  		else {  			for (SampleIndex = 0; SampleIndex < end; SampleIndex += TrainingRate.BatchSize) {  				for (int i = Layers.Length - 1; i > 1; i--)  					Layers [i].EraseGradientWeights ();  				for (int batchIndex = 0; batchIndex < TrainingRate.BatchSize; batchIndex++) {  					if (SubstractMean) {  						double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].SubstractMean (DataProvider);  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = data [i];  					}  					else  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Image [i] / 255D) * Spread) + Min;  					sampleLabel = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Label;  					// fprop  					for (int i = 1; i < Layers.Length; i++)  						Layers [i].CalculateAction ();  					// Cross Entropy Loss  					patternLoss = -0.5D * MathUtil.Log (LastLayer.Neurons [sampleLabel].Output);  					totLoss += patternLoss;  					AvgTrainLoss = totLoss / (SampleIndex + batchIndex + 1);  					bestIndex = 0;  					maxValue = LastLayer.Neurons [0].Output;  					for (int i = 1; i < ClassCount; i++) {  						if (LastLayer.Neurons [i].Output > maxValue) {  							maxValue = LastLayer.Neurons [i].Output;  							bestIndex = i;  						}  					}  					if (sampleLabel != bestIndex)  						TrainErrors++;  					for (int i = 0; i < ClassCount; i++)  						LastLayer.Neurons [i].D1ErrX = LastLayer.Neurons [i].Output;  					LastLayer.Neurons [sampleLabel].D1ErrX = LastLayer.Neurons [sampleLabel].Output - TrainToValue;  					// bprop  					for (int i = Layers.Length - 1; i > 1; i--)  						Layers [i].BackpropagateAction ();  				}  				for (int i = Layers.Length - 1; i > 1; i--)  					Layers [i].UpdateWeights (TrainingRate.BatchSize);  				if (CurrentTaskState != TaskState.Running)  					if (!CheckStateChange ())  						break;  			}  			if (finalBatch > 0) {  				for (int i = Layers.Length - 1; i > 1; i--)  					Layers [i].EraseGradientWeights ();  				for (SampleIndex = DataProvider.TrainingSamplesCount - finalBatch; SampleIndex < DataProvider.TrainingSamplesCount; SampleIndex++) {  					if (SubstractMean) {  						double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider);  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = data [i];  					}  					else  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min;  					sampleLabel = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Label;  					// fprop  					for (int i = 1; i < Layers.Length; i++)  						Layers [i].CalculateAction ();  					// Cross Entropy Loss  					patternLoss = -0.5D * MathUtil.Log (LastLayer.Neurons [sampleLabel].Output);  					totLoss += patternLoss;  					AvgTrainLoss = totLoss / (SampleIndex + 1);  					bestIndex = 0;  					maxValue = LastLayer.Neurons [0].Output;  					for (int i = 1; i < ClassCount; i++) {  						if (LastLayer.Neurons [i].Output > maxValue) {  							maxValue = LastLayer.Neurons [i].Output;  							bestIndex = i;  						}  					}  					if (sampleLabel != bestIndex)  						TrainErrors++;  					for (int i = 0; i < ClassCount; i++)  						LastLayer.Neurons [i].D1ErrX = LastLayer.Neurons [i].Output;  					LastLayer.Neurons [sampleLabel].D1ErrX = LastLayer.Neurons [sampleLabel].Output - TrainToValue;  					// bprop  					for (int i = Layers.Length - 1; i > 1; i--)  						Layers [i].BackpropagateAction ();  				}  				for (int i = Layers.Length - 1; i > 1; i--)  					Layers [i].UpdateWeights (finalBatch);  				if (CurrentTaskState != TaskState.Running)  					if (!CheckStateChange ())  						break;  			}  		}  		break;  	}  	break;  }  
Missing Default,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The following switch statement is missing a default case: switch (LossFunction) {  case LossFunctions.MeanSquareError:  	if (TrainingRate.Distorted) {  		for (SampleIndex = 0; SampleIndex < DataProvider.TrainingSamplesCount; SampleIndex++) {  			if (RandomGenerator.NextPercentage () < TrainingRate.DistortionPercentage) {  				if (SubstractMean) {  					double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling).SubstractMean (DataProvider);  					for (int i = 0; i < sampleSize; i++)  						Layers [0].Neurons [i].Output = data [i];  				}  				else {  					CurrentSample = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling);  					for (int i = 0; i < sampleSize; i++)  						Layers [0].Neurons [i].Output = (((double)CurrentSample.Image [i] / 255D) * Spread) + Min;  				}  			}  			else {  				if (SubstractMean) {  					double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider);  					for (int i = 0; i < sampleSize; i++)  						Layers [0].Neurons [i].Output = data [i];  				}  				else  					for (int i = 0; i < sampleSize; i++)  						Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min;  			}  			sampleLabel = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Label;  			// fprop  			for (int i = 1; i < Layers.Length; i++)  				Layers [i].CalculateAction ();  			// Mean Square Error loss  			patternLoss = 0D;  			for (int i = 0; i < ClassCount; i++) {  				if (i == sampleLabel)  					patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output - TrainToValue);  				else  					patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output + TrainToValue);  			}  			patternLoss *= 0.5D;  			totLoss += patternLoss;  			AvgTrainLoss = totLoss / (SampleIndex + 1);  			bestIndex = 0;  			maxValue = LastLayer.Neurons [0].Output;  			for (int i = 1; i < ClassCount; i++) {  				if (LastLayer.Neurons [i].Output > maxValue) {  					maxValue = LastLayer.Neurons [i].Output;  					bestIndex = i;  				}  			}  			if (sampleLabel != bestIndex)  				TrainErrors++;  			if (patternLoss > prevAvgLoss) {  				for (int i = 0; i < ClassCount; i++)  					LastLayer.Neurons [i].D1ErrX = LastLayer.Neurons [i].Output + TrainToValue;  				LastLayer.Neurons [sampleLabel].D1ErrX = LastLayer.Neurons [sampleLabel].Output - TrainToValue;  				// bprop  				for (int i = Layers.Length - 1; i > 1; i--) {  					Layers [i].EraseGradientWeights ();  					Layers [i].BackpropagateAction ();  					Layers [i].UpdateWeights (1);  				}  			}  			if (CurrentTaskState != TaskState.Running)  				if (!CheckStateChange ())  					break;  		}  	}  	else {  		for (SampleIndex = 0; SampleIndex < DataProvider.TrainingSamplesCount; SampleIndex++) {  			if (SubstractMean) {  				double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider);  				for (int i = 0; i < sampleSize; i++)  					Layers [0].Neurons [i].Output = data [i];  			}  			else  				for (int i = 0; i < sampleSize; i++)  					Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min;  			sampleLabel = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Label;  			// fprop  			for (int i = 1; i < Layers.Length; i++)  				Layers [i].CalculateAction ();  			// Mean Square Error loss  			patternLoss = 0D;  			for (int i = 0; i < ClassCount; i++) {  				if (i == sampleLabel)  					patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output - TrainToValue);  				else  					patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output + TrainToValue);  			}  			patternLoss *= 0.5D;  			totLoss += patternLoss;  			AvgTrainLoss = totLoss / (SampleIndex + 1);  			bestIndex = 0;  			maxValue = LastLayer.Neurons [0].Output;  			for (int i = 1; i < ClassCount; i++) {  				if (LastLayer.Neurons [i].Output > maxValue) {  					maxValue = LastLayer.Neurons [i].Output;  					bestIndex = i;  				}  			}  			if (sampleLabel != bestIndex)  				TrainErrors++;  			if (patternLoss > prevAvgLoss) {  				for (int i = 0; i < ClassCount; i++)  					LastLayer.Neurons [i].D1ErrX = LastLayer.Neurons [i].Output + TrainToValue;  				LastLayer.Neurons [sampleLabel].D1ErrX = LastLayer.Neurons [sampleLabel].Output - TrainToValue;  				// bprop  				for (int i = Layers.Length - 1; i > 1; i--) {  					Layers [i].EraseGradientWeights ();  					Layers [i].BackpropagateAction ();  					Layers [i].UpdateWeights (1);  				}  			}  			if (CurrentTaskState != TaskState.Running)  				if (!CheckStateChange ())  					break;  		}  	}  	break;  case LossFunctions.CrossEntropy:  	if (TrainingRate.Distorted) {  		for (SampleIndex = 0; SampleIndex < DataProvider.TrainingSamplesCount; SampleIndex++) {  			if (RandomGenerator.NextPercentage () < TrainingRate.DistortionPercentage) {  				if (SubstractMean) {  					double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling).SubstractMean (DataProvider);  					for (int i = 0; i < sampleSize; i++)  						Layers [0].Neurons [i].Output = data [i];  				}  				else {  					CurrentSample = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling);  					for (int i = 0; i < sampleSize; i++)  						Layers [0].Neurons [i].Output = (((double)CurrentSample.Image [i] / 255D) * Spread) + Min;  				}  			}  			else {  				if (SubstractMean) {  					double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider);  					for (int i = 0; i < sampleSize; i++)  						Layers [0].Neurons [i].Output = data [i];  				}  				else  					for (int i = 0; i < sampleSize; i++)  						Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min;  			}  			sampleLabel = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Label;  			// fprop  			for (int i = 1; i < Layers.Length; i++)  				Layers [i].CalculateAction ();  			// Cross Entropy Loss  			patternLoss = -0.5D * MathUtil.Log (LastLayer.Neurons [sampleLabel].Output);  			totLoss += patternLoss;  			AvgTrainLoss = totLoss / (SampleIndex + 1);  			bestIndex = 0;  			maxValue = LastLayer.Neurons [0].Output;  			for (int i = 1; i < ClassCount; i++) {  				if (LastLayer.Neurons [i].Output > maxValue) {  					maxValue = LastLayer.Neurons [i].Output;  					bestIndex = i;  				}  			}  			if (sampleLabel != bestIndex)  				TrainErrors++;  			if (patternLoss > prevAvgLoss) {  				for (int i = 0; i < ClassCount; i++)  					LastLayer.Neurons [i].D1ErrX = LastLayer.Neurons [i].Output;  				LastLayer.Neurons [sampleLabel].D1ErrX = LastLayer.Neurons [sampleLabel].Output - TrainToValue;  				// bprop  				for (int i = Layers.Length - 1; i > 1; i--) {  					Layers [i].EraseGradientWeights ();  					Layers [i].BackpropagateAction ();  					Layers [i].UpdateWeights (1);  				}  			}  			if (CurrentTaskState != TaskState.Running)  				if (!CheckStateChange ())  					break;  		}  	}  	else {  		for (SampleIndex = 0; SampleIndex < DataProvider.TrainingSamplesCount; SampleIndex++) {  			if (SubstractMean) {  				double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider);  				for (int i = 0; i < sampleSize; i++)  					Layers [0].Neurons [i].Output = data [i];  			}  			else  				for (int i = 0; i < sampleSize; i++)  					Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min;  			sampleLabel = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Label;  			// fprop  			for (int i = 1; i < Layers.Length; i++)  				Layers [i].CalculateAction ();  			// Cross Entropy Loss  			patternLoss = -0.5D * MathUtil.Log (LastLayer.Neurons [sampleLabel].Output);  			totLoss += patternLoss;  			AvgTrainLoss = totLoss / (SampleIndex + 1);  			bestIndex = 0;  			maxValue = LastLayer.Neurons [0].Output;  			for (int i = 1; i < ClassCount; i++) {  				if (LastLayer.Neurons [i].Output > maxValue) {  					maxValue = LastLayer.Neurons [i].Output;  					bestIndex = i;  				}  			}  			if (sampleLabel != bestIndex)  				TrainErrors++;  			if (patternLoss > prevAvgLoss) {  				for (int i = 0; i < ClassCount; i++)  					LastLayer.Neurons [i].D1ErrX = LastLayer.Neurons [i].Output;  				LastLayer.Neurons [sampleLabel].D1ErrX = LastLayer.Neurons [sampleLabel].Output - TrainToValue;  				// bprop  				for (int i = Layers.Length - 1; i > 1; i--) {  					Layers [i].EraseGradientWeights ();  					Layers [i].BackpropagateAction ();  					Layers [i].UpdateWeights (1);  				}  			}  			if (CurrentTaskState != TaskState.Running)  				if (!CheckStateChange ())  					break;  		}  	}  	break;  }  
Missing Default,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The following switch statement is missing a default case: switch (LossFunction) {  case LossFunctions.MeanSquareError:  	if (TrainingRate.Distorted) {  		for (SampleIndex = 0; SampleIndex < end; SampleIndex += TrainingRate.BatchSize) {  			for (int i = Layers.Length - 1; i > 1; i--)  				Layers [i].EraseGradientWeights ();  			for (int batchIndex = 0; batchIndex < TrainingRate.BatchSize; batchIndex++) {  				if (RandomGenerator.NextPercentage () < TrainingRate.DistortionPercentage) {  					if (SubstractMean) {  						double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling).SubstractMean (DataProvider);  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = data [i];  					}  					else {  						CurrentSample = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling);  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = (((double)CurrentSample.Image [i] / 255D) * Spread) + Min;  					}  				}  				else {  					if (SubstractMean) {  						double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].SubstractMean (DataProvider);  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = data [i];  					}  					else  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Image [i] / 255D) * Spread) + Min;  				}  				sampleLabel = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Label;  				// fprop  				for (int i = 1; i < Layers.Length; i++)  					Layers [i].CalculateAction ();  				// Mean Square Error loss  				patternLoss = 0D;  				for (int i = 0; i < ClassCount; i++) {  					if (i == sampleLabel)  						patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output - TrainToValue);  					else  						patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output + TrainToValue);  				}  				patternLoss *= 0.5D;  				totLoss += patternLoss;  				AvgTrainLoss = totLoss / (SampleIndex + batchIndex + 1);  				bestIndex = 0;  				maxValue = LastLayer.Neurons [0].Output;  				for (int i = 1; i < ClassCount; i++) {  					if (LastLayer.Neurons [i].Output > maxValue) {  						maxValue = LastLayer.Neurons [i].Output;  						bestIndex = i;  					}  				}  				if (sampleLabel != bestIndex)  					TrainErrors++;  				for (int i = 0; i < ClassCount; i++)  					LastLayer.Neurons [i].D1ErrX = LastLayer.Neurons [i].Output + TrainToValue;  				LastLayer.Neurons [sampleLabel].D1ErrX = LastLayer.Neurons [sampleLabel].Output - TrainToValue;  				// bprop  				for (int i = Layers.Length - 1; i > 1; i--)  					Layers [i].BackpropagateAction ();  			}  			for (int i = Layers.Length - 1; i > 1; i--)  				Layers [i].UpdateWeights (TrainingRate.BatchSize);  			if (CurrentTaskState != TaskState.Running)  				if (!CheckStateChange ())  					break;  		}  		if (finalBatch > 0) {  			for (int i = Layers.Length - 1; i > 1; i--)  				Layers [i].EraseGradientWeights ();  			for (SampleIndex = DataProvider.TrainingSamplesCount - finalBatch; SampleIndex < DataProvider.TrainingSamplesCount; SampleIndex++) {  				if (RandomGenerator.NextPercentage () < TrainingRate.DistortionPercentage) {  					if (SubstractMean) {  						double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling).SubstractMean (DataProvider);  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = data [i];  					}  					else {  						CurrentSample = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling);  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = (((double)CurrentSample.Image [i] / 255D) * Spread) + Min;  					}  				}  				else {  					if (SubstractMean) {  						double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider);  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = data [i];  					}  					else  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min;  				}  				sampleLabel = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Label;  				// fprop  				for (int i = 1; i < Layers.Length; i++)  					Layers [i].CalculateAction ();  				// Mean Square Error loss  				patternLoss = 0D;  				for (int i = 0; i < ClassCount; i++) {  					if (i == sampleLabel)  						patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output - TrainToValue);  					else  						patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output + TrainToValue);  				}  				patternLoss *= 0.5D;  				totLoss += patternLoss;  				AvgTrainLoss = totLoss / (SampleIndex + 1);  				bestIndex = 0;  				maxValue = LastLayer.Neurons [0].Output;  				for (int i = 1; i < ClassCount; i++) {  					if (LastLayer.Neurons [i].Output > maxValue) {  						maxValue = LastLayer.Neurons [i].Output;  						bestIndex = i;  					}  				}  				if (sampleLabel != bestIndex)  					TrainErrors++;  				for (int i = 0; i < ClassCount; i++)  					LastLayer.Neurons [i].D1ErrX = LastLayer.Neurons [i].Output + TrainToValue;  				LastLayer.Neurons [sampleLabel].D1ErrX = LastLayer.Neurons [sampleLabel].Output - TrainToValue;  				// bprop  				for (int i = Layers.Length - 1; i > 1; i--)  					Layers [i].BackpropagateAction ();  			}  			for (int i = Layers.Length - 1; i > 1; i--)  				Layers [i].UpdateWeights (finalBatch);  			if (CurrentTaskState != TaskState.Running)  				if (!CheckStateChange ())  					break;  		}  	}  	else {  		for (SampleIndex = 0; SampleIndex < end; SampleIndex += TrainingRate.BatchSize) {  			for (int i = Layers.Length - 1; i > 1; i--)  				Layers [i].EraseGradientWeights ();  			for (int batchIndex = 0; batchIndex < TrainingRate.BatchSize; batchIndex++) {  				if (SubstractMean) {  					double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].SubstractMean (DataProvider);  					for (int i = 0; i < sampleSize; i++)  						Layers [0].Neurons [i].Output = data [i];  				}  				else  					for (int i = 0; i < sampleSize; i++)  						Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Image [i] / 255D) * Spread) + Min;  				sampleLabel = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Label;  				// fprop  				for (int i = 1; i < Layers.Length; i++)  					Layers [i].CalculateAction ();  				// Mean Square Error loss  				patternLoss = 0D;  				for (int i = 0; i < ClassCount; i++) {  					if (i == sampleLabel)  						patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output - TrainToValue);  					else  						patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output + TrainToValue);  				}  				patternLoss *= 0.5D;  				totLoss += patternLoss;  				AvgTrainLoss = totLoss / (SampleIndex + batchIndex + 1);  				bestIndex = 0;  				maxValue = LastLayer.Neurons [0].Output;  				for (int i = 1; i < ClassCount; i++) {  					if (LastLayer.Neurons [i].Output > maxValue) {  						maxValue = LastLayer.Neurons [i].Output;  						bestIndex = i;  					}  				}  				if (sampleLabel != bestIndex)  					TrainErrors++;  				for (int i = 0; i < ClassCount; i++)  					LastLayer.Neurons [i].D1ErrX = LastLayer.Neurons [i].Output + TrainToValue;  				LastLayer.Neurons [sampleLabel].D1ErrX = LastLayer.Neurons [sampleLabel].Output - TrainToValue;  				// bprop  				for (int i = Layers.Length - 1; i > 1; i--)  					Layers [i].BackpropagateAction ();  			}  			for (int i = Layers.Length - 1; i > 1; i--)  				Layers [i].UpdateWeights (TrainingRate.BatchSize);  			if (CurrentTaskState != TaskState.Running)  				if (!CheckStateChange ())  					break;  		}  		if (finalBatch > 0) {  			for (int i = Layers.Length - 1; i > 1; i--)  				Layers [i].EraseGradientWeights ();  			for (SampleIndex = DataProvider.TrainingSamplesCount - finalBatch; SampleIndex < DataProvider.TrainingSamplesCount; SampleIndex++) {  				if (SubstractMean) {  					double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider);  					for (int i = 0; i < sampleSize; i++)  						Layers [0].Neurons [i].Output = data [i];  				}  				else  					for (int i = 0; i < sampleSize; i++)  						Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min;  				sampleLabel = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Label;  				// fprop  				for (int i = 1; i < Layers.Length; i++)  					Layers [i].CalculateAction ();  				// Mean Square Error loss  				patternLoss = 0D;  				for (int i = 0; i < ClassCount; i++) {  					if (i == sampleLabel)  						patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output - TrainToValue);  					else  						patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output + TrainToValue);  				}  				patternLoss *= 0.5D;  				totLoss += patternLoss;  				AvgTrainLoss = totLoss / (SampleIndex + 1);  				bestIndex = 0;  				maxValue = LastLayer.Neurons [0].Output;  				for (int i = 1; i < ClassCount; i++) {  					if (LastLayer.Neurons [i].Output > maxValue) {  						maxValue = LastLayer.Neurons [i].Output;  						bestIndex = i;  					}  				}  				if (sampleLabel != bestIndex)  					TrainErrors++;  				for (int i = 0; i < ClassCount; i++)  					LastLayer.Neurons [i].D1ErrX = LastLayer.Neurons [i].Output + TrainToValue;  				LastLayer.Neurons [sampleLabel].D1ErrX = LastLayer.Neurons [sampleLabel].Output - TrainToValue;  				// bprop  				for (int i = Layers.Length - 1; i > 1; i--)  					Layers [i].BackpropagateAction ();  			}  		}  		for (int i = Layers.Length - 1; i > 1; i--)  			Layers [i].UpdateWeights (finalBatch);  		if (CurrentTaskState != TaskState.Running)  			if (!CheckStateChange ())  				break;  	}  	break;  case LossFunctions.CrossEntropy:  	if (TrainingRate.Distorted) {  		for (SampleIndex = 0; SampleIndex < end; SampleIndex += TrainingRate.BatchSize) {  			for (int i = Layers.Length - 1; i > 1; i--)  				Layers [i].EraseGradientWeights ();  			for (int batchIndex = 0; batchIndex < TrainingRate.BatchSize; batchIndex++) {  				if (RandomGenerator.NextPercentage () < TrainingRate.DistortionPercentage) {  					if (SubstractMean) {  						double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling).SubstractMean (DataProvider);  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = data [i];  					}  					else {  						CurrentSample = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling);  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = (((double)CurrentSample.Image [i] / 255D) * Spread) + Min;  					}  				}  				else {  					if (SubstractMean) {  						double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].SubstractMean (DataProvider);  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = data [i];  					}  					else  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Image [i] / 255D) * Spread) + Min;  				}  				sampleLabel = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Label;  				// fprop  				for (int i = 1; i < Layers.Length; i++)  					Layers [i].CalculateAction ();  				// Cross Entropy Loss  				patternLoss = -0.5D * MathUtil.Log (LastLayer.Neurons [sampleLabel].Output);  				totLoss += patternLoss;  				AvgTrainLoss = totLoss / (SampleIndex + batchIndex + 1);  				bestIndex = 0;  				maxValue = LastLayer.Neurons [0].Output;  				for (int i = 1; i < ClassCount; i++) {  					if (LastLayer.Neurons [i].Output > maxValue) {  						maxValue = LastLayer.Neurons [i].Output;  						bestIndex = i;  					}  				}  				if (sampleLabel != bestIndex)  					TrainErrors++;  				for (int i = 0; i < ClassCount; i++)  					LastLayer.Neurons [i].D1ErrX = LastLayer.Neurons [i].Output;  				LastLayer.Neurons [sampleLabel].D1ErrX = LastLayer.Neurons [sampleLabel].Output - TrainToValue;  				// bprop  				for (int i = Layers.Length - 1; i > 1; i--)  					Layers [i].BackpropagateAction ();  			}  			for (int i = Layers.Length - 1; i > 1; i--)  				Layers [i].UpdateWeights (TrainingRate.BatchSize);  			if (CurrentTaskState != TaskState.Running)  				if (!CheckStateChange ())  					break;  		}  		if (finalBatch > 0) {  			for (int i = Layers.Length - 1; i > 1; i--)  				Layers [i].EraseGradientWeights ();  			for (SampleIndex = DataProvider.TrainingSamplesCount - finalBatch; SampleIndex < DataProvider.TrainingSamplesCount; SampleIndex++) {  				if (RandomGenerator.NextPercentage () < TrainingRate.DistortionPercentage) {  					if (SubstractMean) {  						double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling).SubstractMean (DataProvider);  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = data [i];  					}  					else {  						CurrentSample = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Distorted (DataProvider' TrainingRate.SeverityFactor' TrainingRate.MaxScaling' TrainingRate.MaxRotation' TrainingRate.ElasticSigma' TrainingRate.ElasticScaling);  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = (((double)CurrentSample.Image [i] / 255D) * Spread) + Min;  					}  				}  				else {  					if (SubstractMean) {  						double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider);  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = data [i];  					}  					else  						for (int i = 0; i < sampleSize; i++)  							Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min;  				}  				sampleLabel = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Label;  				// fprop  				for (int i = 1; i < Layers.Length; i++)  					Layers [i].CalculateAction ();  				// Cross Entropy Loss  				patternLoss = -0.5D * MathUtil.Log (LastLayer.Neurons [sampleLabel].Output);  				totLoss += patternLoss;  				AvgTrainLoss = totLoss / (SampleIndex + 1);  				bestIndex = 0;  				maxValue = LastLayer.Neurons [0].Output;  				for (int i = 1; i < ClassCount; i++) {  					if (LastLayer.Neurons [i].Output > maxValue) {  						maxValue = LastLayer.Neurons [i].Output;  						bestIndex = i;  					}  				}  				if (sampleLabel != bestIndex)  					TrainErrors++;  				for (int i = 0; i < ClassCount; i++)  					LastLayer.Neurons [i].D1ErrX = LastLayer.Neurons [i].Output;  				LastLayer.Neurons [sampleLabel].D1ErrX = LastLayer.Neurons [sampleLabel].Output - TrainToValue;  				// bprop  				for (int i = Layers.Length - 1; i > 1; i--)  					Layers [i].BackpropagateAction ();  			}  			for (int i = Layers.Length - 1; i > 1; i--)  				Layers [i].UpdateWeights (finalBatch);  			if (CurrentTaskState != TaskState.Running)  				if (!CheckStateChange ())  					break;  		}  	}  	else {  		for (SampleIndex = 0; SampleIndex < end; SampleIndex += TrainingRate.BatchSize) {  			for (int i = Layers.Length - 1; i > 1; i--)  				Layers [i].EraseGradientWeights ();  			for (int batchIndex = 0; batchIndex < TrainingRate.BatchSize; batchIndex++) {  				if (SubstractMean) {  					double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].SubstractMean (DataProvider);  					for (int i = 0; i < sampleSize; i++)  						Layers [0].Neurons [i].Output = data [i];  				}  				else  					for (int i = 0; i < sampleSize; i++)  						Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Image [i] / 255D) * Spread) + Min;  				sampleLabel = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex + batchIndex]].Label;  				// fprop  				for (int i = 1; i < Layers.Length; i++)  					Layers [i].CalculateAction ();  				// Cross Entropy Loss  				patternLoss = -0.5D * MathUtil.Log (LastLayer.Neurons [sampleLabel].Output);  				totLoss += patternLoss;  				AvgTrainLoss = totLoss / (SampleIndex + batchIndex + 1);  				bestIndex = 0;  				maxValue = LastLayer.Neurons [0].Output;  				for (int i = 1; i < ClassCount; i++) {  					if (LastLayer.Neurons [i].Output > maxValue) {  						maxValue = LastLayer.Neurons [i].Output;  						bestIndex = i;  					}  				}  				if (sampleLabel != bestIndex)  					TrainErrors++;  				for (int i = 0; i < ClassCount; i++)  					LastLayer.Neurons [i].D1ErrX = LastLayer.Neurons [i].Output;  				LastLayer.Neurons [sampleLabel].D1ErrX = LastLayer.Neurons [sampleLabel].Output - TrainToValue;  				// bprop  				for (int i = Layers.Length - 1; i > 1; i--)  					Layers [i].BackpropagateAction ();  			}  			for (int i = Layers.Length - 1; i > 1; i--)  				Layers [i].UpdateWeights (TrainingRate.BatchSize);  			if (CurrentTaskState != TaskState.Running)  				if (!CheckStateChange ())  					break;  		}  		if (finalBatch > 0) {  			for (int i = Layers.Length - 1; i > 1; i--)  				Layers [i].EraseGradientWeights ();  			for (SampleIndex = DataProvider.TrainingSamplesCount - finalBatch; SampleIndex < DataProvider.TrainingSamplesCount; SampleIndex++) {  				if (SubstractMean) {  					double[] data = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].SubstractMean (DataProvider);  					for (int i = 0; i < sampleSize; i++)  						Layers [0].Neurons [i].Output = data [i];  				}  				else  					for (int i = 0; i < sampleSize; i++)  						Layers [0].Neurons [i].Output = (((double)DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Image [i] / 255D) * Spread) + Min;  				sampleLabel = DataProvider.TrainingSamples [DataProvider.RandomTrainingSample [SampleIndex]].Label;  				// fprop  				for (int i = 1; i < Layers.Length; i++)  					Layers [i].CalculateAction ();  				// Cross Entropy Loss  				patternLoss = -0.5D * MathUtil.Log (LastLayer.Neurons [sampleLabel].Output);  				totLoss += patternLoss;  				AvgTrainLoss = totLoss / (SampleIndex + 1);  				bestIndex = 0;  				maxValue = LastLayer.Neurons [0].Output;  				for (int i = 1; i < ClassCount; i++) {  					if (LastLayer.Neurons [i].Output > maxValue) {  						maxValue = LastLayer.Neurons [i].Output;  						bestIndex = i;  					}  				}  				if (sampleLabel != bestIndex)  					TrainErrors++;  				for (int i = 0; i < ClassCount; i++)  					LastLayer.Neurons [i].D1ErrX = LastLayer.Neurons [i].Output;  				LastLayer.Neurons [sampleLabel].D1ErrX = LastLayer.Neurons [sampleLabel].Output - TrainToValue;  				// bprop  				for (int i = Layers.Length - 1; i > 1; i--)  					Layers [i].BackpropagateAction ();  			}  			for (int i = Layers.Length - 1; i > 1; i--)  				Layers [i].UpdateWeights (finalBatch);  			if (CurrentTaskState != TaskState.Running)  				if (!CheckStateChange ())  					break;  		}  	}  	break;  }  
Missing Default,CNNWB.CNN,NeuralNetwork,C:\repos\supby_cnnwb\CNNWB.CNN\NeuralNetwork.cs,TrainingTask,The following switch statement is missing a default case: switch (LossFunction) {  case LossFunctions.MeanSquareError:  	for (SampleIndex = 0; SampleIndex < DataProvider.TestingSamplesCount; SampleIndex++) {  		if (SubstractMean) {  			double[] data = DataProvider.TestingSamples [SampleIndex].SubstractMean (DataProvider);  			for (int i = 0; i < sampleSize; i++)  				Layers [0].Neurons [i].Output = data [i];  		}  		else  			for (int i = 0; i < sampleSize; i++)  				Layers [0].Neurons [i].Output = (((double)DataProvider.TestingSamples [SampleIndex].Image [i] / 255D) * Spread) + Min;  		sampleLabel = DataProvider.TestingSamples [SampleIndex].Label;  		// fprop  		for (int i = 1; i < Layers.Length; i++)  			Layers [i].CalculateAction ();  		// Mean Square Error loss  		patternLoss = 0D;  		for (int i = 0; i < ClassCount; i++) {  			if (i == sampleLabel)  				patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output - TrainToValue);  			else  				patternLoss += MathUtil.Pow2 (LastLayer.Neurons [i].Output + TrainToValue);  		}  		patternLoss *= 0.5D;  		totTestLoss += patternLoss;  		AvgTestLoss = totTestLoss / (SampleIndex + 1);  		bestIndex = 0;  		maxValue = LastLayer.Neurons [0].Output;  		for (int i = 1; i < ClassCount; i++) {  			if (LastLayer.Neurons [i].Output > maxValue) {  				maxValue = LastLayer.Neurons [i].Output;  				bestIndex = i;  			}  		}  		if (sampleLabel != bestIndex)  			TestErrors++;  		if (CurrentTaskState != TaskState.Running)  			if (!CheckStateChange ())  				break;  	}  	break;  case LossFunctions.CrossEntropy:  	for (SampleIndex = 0; SampleIndex < DataProvider.TestingSamplesCount; SampleIndex++) {  		if (SubstractMean) {  			double[] data = DataProvider.TestingSamples [SampleIndex].SubstractMean (DataProvider);  			for (int i = 0; i < sampleSize; i++)  				Layers [0].Neurons [i].Output = data [i];  		}  		else  			for (int i = 0; i < sampleSize; i++)  				Layers [0].Neurons [i].Output = (((double)DataProvider.TestingSamples [SampleIndex].Image [i] / 255D) * Spread) + Min;  		sampleLabel = DataProvider.TestingSamples [SampleIndex].Label;  		// fprop  		for (int i = 1; i < Layers.Length; i++)  			Layers [i].CalculateAction ();  		// Cross Entropy Loss  		totTestLoss += -0.5D * MathUtil.Log (LastLayer.Neurons [sampleLabel].Output);  		AvgTestLoss = totTestLoss / (SampleIndex + 1);  		bestIndex = 0;  		maxValue = LastLayer.Neurons [0].Output;  		for (int i = 1; i < ClassCount; i++) {  			if (LastLayer.Neurons [i].Output > maxValue) {  				maxValue = LastLayer.Neurons [i].Output;  				bestIndex = i;  			}  		}  		if (sampleLabel != bestIndex)  			TestErrors++;  		if (CurrentTaskState != TaskState.Running)  			if (!CheckStateChange ())  				break;  	}  	break;  }  
