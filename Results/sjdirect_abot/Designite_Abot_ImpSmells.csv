Implementation smell,Namespace,Class,File,Method,Description
Complex Method,Abot.Core,CrawlDecisionMaker,C:\repos\sjdirect_abot\Abot\Core\CrawlDecisionMaker.cs,ShouldCrawlPage,Cyclomatic complexity of the method is 10
Complex Method,Abot.Core,CrawlDecisionMaker,C:\repos\sjdirect_abot\Abot\Core\CrawlDecisionMaker.cs,ShouldDownloadPageContent,Cyclomatic complexity of the method is 9
Complex Method,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,Crawl,Cyclomatic complexity of the method is 8
Complex Method,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,ProcessPage,Cyclomatic complexity of the method is 8
Long Parameter List,Abot.Crawler,PoliteWebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\PoliteWebCrawler.cs,PoliteWebCrawler,The method has 9 parameters. Parameters: crawlConfiguration' crawlDecisionMaker' threadManager' scheduler' pageRequester' hyperLinkParser' memoryManager' domainRateLimiter' robotsDotTextFinder
Long Parameter List,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,WebCrawler,The method has 7 parameters. Parameters: crawlConfiguration' crawlDecisionMaker' threadManager' scheduler' pageRequester' hyperLinkParser' memoryManager
Long Parameter List,Abot.Util,BloomFilter<T>,C:\repos\sjdirect_abot\Abot\Util\BloomFilter.cs,BloomFilter,The method has 5 parameters. Parameters: capacity' errorRate' hashFunction' m' k
Long Identifier,Abot.Core,AngleSharpHyperlinkParser,C:\repos\sjdirect_abot\Abot\Core\AngleSharpHyperLinkParser.cs,AngleSharpHyperlinkParser,The length of the parameter isRespectMetaRobotsNoFollowEnabled is 34.
Long Identifier,Abot.Core,AngleSharpHyperlinkParser,C:\repos\sjdirect_abot\Abot\Core\AngleSharpHyperLinkParser.cs,AngleSharpHyperlinkParser,The length of the parameter isRespectAnchorRelNoFollowEnabled is 33.
Long Identifier,Abot.Core,AngleSharpHyperlinkParser,C:\repos\sjdirect_abot\Abot\Core\AngleSharpHyperLinkParser.cs,AngleSharpHyperlinkParser,The length of the parameter isRespectUrlNamedAnchorOrHashbangEnabled is 40.
Long Identifier,Abot.Core,HapHyperLinkParser,C:\repos\sjdirect_abot\Abot\Core\HapHyperLinkParser.cs,HapHyperLinkParser,The length of the parameter isRespectMetaRobotsNoFollowEnabled is 34.
Long Identifier,Abot.Core,HapHyperLinkParser,C:\repos\sjdirect_abot\Abot\Core\HapHyperLinkParser.cs,HapHyperLinkParser,The length of the parameter isRespectAnchorRelNoFollowEnabled is 33.
Long Identifier,Abot.Core,HapHyperLinkParser,C:\repos\sjdirect_abot\Abot\Core\HapHyperLinkParser.cs,HapHyperLinkParser,The length of the parameter isRespectUrlNamedAnchorOrHashbangEnabled is 40.
Long Identifier,Abot.Core,CSQueryHyperlinkParser,C:\repos\sjdirect_abot\Abot\Core\CsQueryHyperLinkParser.cs,CSQueryHyperlinkParser,The length of the parameter isRespectMetaRobotsNoFollowEnabled is 34.
Long Identifier,Abot.Core,CSQueryHyperlinkParser,C:\repos\sjdirect_abot\Abot\Core\CsQueryHyperLinkParser.cs,CSQueryHyperlinkParser,The length of the parameter isRespectAnchorRelNoFollowEnabled is 33.
Long Identifier,Abot.Core,CSQueryHyperlinkParser,C:\repos\sjdirect_abot\Abot\Core\CsQueryHyperLinkParser.cs,CSQueryHyperlinkParser,The length of the parameter isRespectUrlNamedAnchorOrHashbangEnabled is 40.
Long Identifier,Abot.Core,DomainRateLimiter,C:\repos\sjdirect_abot\Abot\Core\DomainRateLimiter.cs,,The length of the parameter _defaultMinCrawlDelayInMillisecs is 32.
Long Identifier,Abot.Core,HyperLinkParser,C:\repos\sjdirect_abot\Abot\Core\HyperLinkParser.cs,HyperLinkParser,The length of the parameter isRespectMetaRobotsNoFollowEnabled is 34.
Long Identifier,Abot.Core,HyperLinkParser,C:\repos\sjdirect_abot\Abot\Core\HyperLinkParser.cs,HyperLinkParser,The length of the parameter isRespectUrlNamedAnchorOrHashbangEnabled is 40.
Long Identifier,Abot.Crawler,PoliteWebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\PoliteWebCrawler.cs,Crawl,The length of the parameter robotsDotTextCrawlDelayInMillisecs is 34.
Long Identifier,Abot.Crawler,PoliteWebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\PoliteWebCrawler.cs,ShouldCrawlPage,The length of the parameter allPathsBelowRootAllowedByRobots is 32.
Long Identifier,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,,The length of the parameter _maxPagesToCrawlLimitReachedOrScheduled is 39.
Long Identifier,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,,The length of the parameter _shouldDownloadPageContentDecisionMaker is 39.
Long Identifier,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,,The length of the parameter _shouldCrawlPageLinksDecisionMaker is 34.
Long Identifier,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,,The length of the parameter _shouldRecrawlPageDecisionMaker is 31.
Long Identifier,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,,The length of the parameter _shouldScheduleLinkDecisionMaker is 32.
Long Identifier,Abot.Util,ThreadManager,C:\repos\sjdirect_abot\Abot\Util\ThreadManager.cs,RunAction,The length of the parameter decrementRunningThreadCountOnCompletion is 39.
Long Statement,Abot.Core,AngleSharpHyperlinkParser,C:\repos\sjdirect_abot\Abot\Core\AngleSharpHyperLinkParser.cs,HasRelNoFollow,The length of the statement  "            return _config.IsRespectAnchorRelNoFollowEnabled && (e.HasAttribute("rel") && e.GetAttribute("rel").ToLower().Trim() == "nofollow"); " is 132.
Long Statement,Abot.Core,WebContentExtractor,C:\repos\sjdirect_abot\Abot\Core\WebContentExtractor.cs,GetCharsetFromBody,The length of the statement  "                //find expression from : http://stackoverflow.com/questions/3458217/how-to-use-regular-expression-to-match-the-charset-string-in-html " is 133.
Long Statement,Abot.Core,WebContentExtractor,C:\repos\sjdirect_abot\Abot\Core\WebContentExtractor.cs,GetCharsetFromBody,The length of the statement  "                Match match = Regex.Match(body' @"<meta(?!\s*(?:name|value)\s*=)(?:[^>]*?content\s*=[\s""']*)?([^>]*?)[\s""';]*charset\s*=[\s""']*([^\s""'/>]*)"' RegexOptions.IgnoreCase); " is 171.
Long Statement,Abot.Core,HapHyperLinkParser,C:\repos\sjdirect_abot\Abot\Core\HapHyperLinkParser.cs,GetMetaRobotsValue,The length of the statement  "            HtmlNode robotsNode = crawledPage.HtmlDocument.DocumentNode.SelectSingleNode("//meta[translate(@name''ABCDEFGHIJKLMNOPQRSTUVWXYZ'''abcdefghijklmnopqrstuvwxyz')='robots']"); " is 172.
Long Statement,Abot.Core,CSQueryHyperlinkParser,C:\repos\sjdirect_abot\Abot\Core\CsQueryHyperLinkParser.cs,HasRelNoFollow,The length of the statement  "            return _config.IsRespectAnchorRelNoFollowEnabled && (e.HasAttribute("rel") && e.GetAttribute("rel").ToLower().Trim() == "nofollow"); " is 132.
Long Statement,Abot.Core,CrawlDecisionMaker,C:\repos\sjdirect_abot\Abot\Core\CrawlDecisionMaker.cs,ShouldCrawlPage,The length of the statement  "            if (pageToCrawl.RedirectedFrom != null && pageToCrawl.RedirectPosition > crawlContext.CrawlConfiguration.HttpRequestMaxAutoRedirects) " is 133.
Long Statement,Abot.Core,CrawlDecisionMaker,C:\repos\sjdirect_abot\Abot\Core\CrawlDecisionMaker.cs,ShouldCrawlPage,The length of the statement  "                return new CrawlDecision { Allow = false' Reason = string.Format("HttpRequestMaxAutoRedirects limit of [{0}] has been reached"' crawlContext.CrawlConfiguration.HttpRequestMaxAutoRedirects) }; " is 191.
Long Statement,Abot.Core,CrawlDecisionMaker,C:\repos\sjdirect_abot\Abot\Core\CrawlDecisionMaker.cs,ShouldCrawlPage,The length of the statement  "                return new CrawlDecision { Allow = false' Reason = string.Format("MaxPagesToCrawl limit of [{0}] has been reached"' crawlContext.CrawlConfiguration.MaxPagesToCrawl) }; " is 167.
Long Statement,Abot.Core,CrawlDecisionMaker,C:\repos\sjdirect_abot\Abot\Core\CrawlDecisionMaker.cs,ShouldCrawlPage,The length of the statement  "                    return new CrawlDecision { Allow = false' Reason = string.Format("MaxPagesToCrawlPerDomain limit of [{0}] has been reached for domain [{1}]"' crawlContext.CrawlConfiguration.MaxPagesToCrawlPerDomain' pageToCrawl.Uri.Authority) }; " is 229.
Long Statement,Abot.Core,CrawlDecisionMaker,C:\repos\sjdirect_abot\Abot\Core\CrawlDecisionMaker.cs,ShouldDownloadPageContent,The length of the statement  "                return new CrawlDecision { Allow = false' Reason = "Content type is not any of the following: " + string.Join("'"' cleanDownloadableContentTypes) }; " is 148.
Long Statement,Abot.Core,CrawlDecisionMaker,C:\repos\sjdirect_abot\Abot\Core\CrawlDecisionMaker.cs,ShouldDownloadPageContent,The length of the statement  "            if (crawlContext.CrawlConfiguration.MaxPageSizeInBytes > 0 && crawledPage.HttpWebResponse.ContentLength > crawlContext.CrawlConfiguration.MaxPageSizeInBytes) " is 157.
Long Statement,Abot.Core,CrawlDecisionMaker,C:\repos\sjdirect_abot\Abot\Core\CrawlDecisionMaker.cs,ShouldDownloadPageContent,The length of the statement  "                return new CrawlDecision { Allow = false' Reason = string.Format("Page size of [{0}] bytes is above the max allowable of [{1}] bytes"' crawledPage.HttpWebResponse.ContentLength' crawlContext.CrawlConfiguration.MaxPageSizeInBytes) }; " is 232.
Long Statement,Abot.Core,DomainRateLimiter,C:\repos\sjdirect_abot\Abot\Core\DomainRateLimiter.cs,AddDomain,The length of the statement  "            GetRateLimter(uri' Math.Max(minCrawlDelayInMillisecs' _defaultMinCrawlDelayInMillisecs));//just calling this method adds the new domain " is 135.
Long Statement,Abot.Core,DomainRateLimiter,C:\repos\sjdirect_abot\Abot\Core\DomainRateLimiter.cs,AddOrUpdateDomain,The length of the statement  "                _logger.DebugFormat("Added/updated domain [{0}] with minCrawlDelayInMillisecs of [{1}] milliseconds"' uri.Authority' delayToUse); " is 129.
Long Statement,Abot.Core,DomainRateLimiter,C:\repos\sjdirect_abot\Abot\Core\DomainRateLimiter.cs,GetRateLimter,The length of the statement  "                    _logger.DebugFormat("Added new domain [{0}] with minCrawlDelayInMillisecs of [{1}] milliseconds"' uri.Authority' minCrawlDelayInMillisecs); " is 139.
Long Statement,Abot.Core,DomainRateLimiter,C:\repos\sjdirect_abot\Abot\Core\DomainRateLimiter.cs,GetRateLimter,The length of the statement  "                    _logger.WarnFormat("Unable to add new domain [{0}] with minCrawlDelayInMillisecs of [{1}] milliseconds"' uri.Authority' minCrawlDelayInMillisecs); " is 146.
Long Statement,Abot.Core,RobotsDotTextFinder,C:\repos\sjdirect_abot\Abot\Core\RobotsDotTextFinder.cs,Find,The length of the statement  "            if (page == null || page.WebException != null || page.HttpWebResponse == null || page.HttpWebResponse.StatusCode != HttpStatusCode.OK) " is 134.
Long Statement,Abot.Core,HyperLinkParser,C:\repos\sjdirect_abot\Abot\Core\HyperLinkParser.cs,GetLinks,The length of the statement  "            _logger.DebugFormat("{0} parsed links from [{1}] in [{2}] milliseconds"' ParserType' crawledPage.Uri' timer.ElapsedMilliseconds); " is 129.
Long Statement,Abot.Core,HyperLinkParser,C:\repos\sjdirect_abot\Abot\Core\HyperLinkParser.cs,GetUris,The length of the statement  "            //Using HttpWebRequest.Address instead of HttpWebResonse.ResponseUri since this is the best practice and mentioned on http://msdn.microsoft.com/en-us/library/system.net.httpwebresponse.responseuri.aspx " is 201.
Long Statement,Abot.Core,HyperLinkParser,C:\repos\sjdirect_abot\Abot\Core\HyperLinkParser.cs,HasRobotsNoFollow,The length of the statement  "                    _logger.InfoFormat("Http header X-Robots-Tag nofollow detected on uri [{0}]' will not crawl links on this page."' crawledPage.Uri); " is 131.
Long Statement,Abot.Core,HyperLinkParser,C:\repos\sjdirect_abot\Abot\Core\HyperLinkParser.cs,HasRobotsNoFollow,The length of the statement  "                    _logger.InfoFormat("Meta Robots nofollow tag detected on uri [{0}]' will not crawl links on this page."' crawledPage.Uri); " is 122.
Long Statement,Abot.Core,PageRequester,C:\repos\sjdirect_abot\Abot\Core\PageRequester.cs,MakeRequest,The length of the statement  "                            _logger.DebugFormat("Links on page [{0}] not crawled' [{1}]"' crawledPage.Uri.AbsoluteUri' shouldDownloadContentDecision.Reason); " is 129.
Long Statement,Abot.Core,PageRequester,C:\repos\sjdirect_abot\Abot\Core\PageRequester.cs,BuildRequestObject,The length of the statement  "                string credentials = Convert.ToBase64String(System.Text.Encoding.ASCII.GetBytes(_config.LoginUser + ":" + _config.LoginPassword)); " is 130.
Long Statement,Abot.Crawler,PoliteWebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\PoliteWebCrawler.cs,Crawl,The length of the statement  "                    robotsDotTextCrawlDelayInSecs = _robotsDotText.GetCrawlDelay(_crawlContext.CrawlConfiguration.RobotsDotTextUserAgentString); " is 124.
Long Statement,Abot.Crawler,PoliteWebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\PoliteWebCrawler.cs,Crawl,The length of the statement  "            //Use whichever value is greater between the actual crawl delay value found' the max allowed crawl delay value or the minimum crawl delay required for every domain " is 163.
Long Statement,Abot.Crawler,PoliteWebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\PoliteWebCrawler.cs,Crawl,The length of the statement  "            if (robotsDotTextCrawlDelayInSecs > 0 && robotsDotTextCrawlDelayInMillisecs > _crawlContext.CrawlConfiguration.MinCrawlDelayPerDomainMilliSeconds) " is 146.
Long Statement,Abot.Crawler,PoliteWebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\PoliteWebCrawler.cs,Crawl,The length of the statement  "                    _logger.WarnFormat("[{0}] robot.txt file directive [Crawl-delay: {1}] is above the value set in the config value MaxRobotsDotTextCrawlDelay' will use MaxRobotsDotTextCrawlDelay value instead."' uri' _crawlContext.CrawlConfiguration.MaxRobotsDotTextCrawlDelayInSeconds); " is 269.
Long Statement,Abot.Crawler,PoliteWebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\PoliteWebCrawler.cs,Crawl,The length of the statement  "                _logger.WarnFormat("[{0}] robot.txt file directive [Crawl-delay: {1}] will be respected."' uri' robotsDotTextCrawlDelayInSecs); " is 127.
Long Statement,Abot.Crawler,PoliteWebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\PoliteWebCrawler.cs,ShouldCrawlPage,The length of the statement  "                allowedByRobots = _robotsDotText.IsUrlAllowed(pageToCrawl.Uri.AbsoluteUri' _crawlContext.CrawlConfiguration.RobotsDotTextUserAgentString); " is 138.
Long Statement,Abot.Crawler,PoliteWebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\PoliteWebCrawler.cs,ShouldCrawlPage,The length of the statement  "            //https://github.com/sjdirect/abot/issues/96 Handle scenario where the root is allowed but all the paths below are disallowed like "disallow: /*" " is 145.
Long Statement,Abot.Crawler,PoliteWebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\PoliteWebCrawler.cs,ShouldCrawlPage,The length of the statement  "                var anyPathOffRoot = pageToCrawl.Uri.AbsoluteUri.EndsWith("/") ? pageToCrawl.Uri.AbsoluteUri + "aaaaa": pageToCrawl.Uri.AbsoluteUri + "/aaaaa"; " is 143.
Long Statement,Abot.Crawler,PoliteWebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\PoliteWebCrawler.cs,ShouldCrawlPage,The length of the statement  "                allPathsBelowRootAllowedByRobots = _robotsDotText.IsUrlAllowed(anyPathOffRoot' _crawlContext.CrawlConfiguration.RobotsDotTextUserAgentString); " is 142.
Long Statement,Abot.Crawler,PoliteWebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\PoliteWebCrawler.cs,ShouldCrawlPage,The length of the statement  "                    string message = string.Format("Page [{0}] [Disallowed by robots.txt file]' however since IsIgnoreRobotsDotTextIfRootDisallowedEnabled is set to true the robots.txt file will be ignored for this site."' pageToCrawl.Uri.AbsoluteUri); " is 232.
Long Statement,Abot.Crawler,PoliteWebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\PoliteWebCrawler.cs,ShouldCrawlPage,The length of the statement  "                    string message = string.Format("All Pages below [{0}] [Disallowed by robots.txt file]' however since IsIgnoreRobotsDotTextIfRootDisallowedEnabled is set to true the robots.txt file will be ignored for this site."' pageToCrawl.Uri.AbsoluteUri); " is 243.
Long Statement,Abot.Crawler,PoliteWebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\PoliteWebCrawler.cs,ShouldCrawlPage,The length of the statement  "                string message = string.Format("Page [{0}] not crawled' [Disallowed by robots.txt file]' set IsRespectRobotsDotText=false in config file if you would like to ignore robots.txt files."' pageToCrawl.Uri.AbsoluteUri); " is 214.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,Crawl,The length of the statement  "                _logger.InfoFormat("Starting memory usage for site [{0}] is [{1}mb]"' uri.AbsoluteUri' _crawlContext.MemoryUsageBeforeCrawlInMb); " is 129.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,Crawl,The length of the statement  "                _logger.InfoFormat("Ending memory usage for site [{0}] is [{1}mb]"' uri.AbsoluteUri' _crawlContext.MemoryUsageAfterCrawlInMb); " is 126.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,Crawl,The length of the statement  "            _logger.InfoFormat("Crawl complete for site [{0}]: Crawled [{1}] pages in [{2}]"' _crawlResult.RootUri.AbsoluteUri' _crawlResult.CrawlContext.CrawledCount' _crawlResult.Elapsed); " is 178.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,FirePageCrawlStartingEvent,The length of the statement  "                _logger.Error("An unhandled exception was thrown by a subscriber of the PageCrawlStarting event for url:" + pageToCrawl.Uri.AbsoluteUri); " is 137.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,FirePageCrawlCompletedEvent,The length of the statement  "                _logger.Error("An unhandled exception was thrown by a subscriber of the PageCrawlCompleted event for url:" + crawledPage.Uri.AbsoluteUri); " is 138.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,FirePageCrawlDisallowedEvent,The length of the statement  "                _logger.Error("An unhandled exception was thrown by a subscriber of the PageCrawlDisallowed event for url:" + pageToCrawl.Uri.AbsoluteUri); " is 139.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,FirePageLinksCrawlDisallowedEvent,The length of the statement  "                _logger.Error("An unhandled exception was thrown by a subscriber of the PageLinksCrawlDisallowed event for url:" + crawledPage.Uri.AbsoluteUri); " is 144.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,FirePageCrawlCompletedEventAsync,The length of the statement  "                //Must be fired synchronously to avoid main thread exiting before completion of event handler for first or last page crawled " is 124.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,FirePageCrawlCompletedEventAsync,The length of the statement  "                    _logger.Error("An unhandled exception was thrown by a subscriber of the PageCrawlCompleted event for url:" + crawledPage.Uri.AbsoluteUri); " is 138.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,VerifyRequiredAvailableMemory,The length of the statement  "                throw new InsufficientMemoryException(string.Format("Process does not have the configured [{0}mb] of available memory to crawl site [{1}]. This is configurable through the minAvailableMemoryRequiredInMb in app.conf or CrawlConfiguration.MinAvailableMemoryRequiredInMb."' _crawlContext.CrawlConfiguration.MinAvailableMemoryRequiredInMb' _crawlContext.RootUri)); " is 360.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,CheckMemoryUsage,The length of the statement  "                string message = string.Format("Process is using [{0}mb] of memory which is above the max configured of [{1}mb] for site [{2}]. This is configurable through the maxMemoryUsageInMb in app.conf or CrawlConfiguration.MaxMemoryUsageInMb."' currentMemoryUsage' _crawlContext.CrawlConfiguration.MaxMemoryUsageInMb' _crawlContext.RootUri); " is 332.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,HandleCrawlTimeout,The length of the statement  "            _logger.InfoFormat("Crawl timeout of [{0}] seconds has been reached for [{1}]"' _crawlContext.CrawlConfiguration.CrawlTimeoutSeconds' _crawlContext.RootUri); " is 157.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,ProcessRedirect,The length of the statement  "                _logger.WarnFormat("Page [{0}] is part of a chain of 20 or more consecutive redirects' redirects for this chain will now be aborted."' crawledPage.Uri); " is 152.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,ProcessRedirect,The length of the statement  "                _logger.DebugFormat("Page [{0}] is requesting that it be redirect to [{1}]"' crawledPage.Uri' crawledPage.RedirectedTo.Uri); " is 124.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,PageSizeIsAboveMax,The length of the statement  "                _logger.InfoFormat("Page [{0}] has a page size of [{1}] bytes which is above the [{2}] byte max' no further processing will occur for this page"' crawledPage.Uri' crawledPage.Content.Bytes.Length' _crawlContext.CrawlConfiguration.MaxPageSizeInBytes); " is 250.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,ShouldCrawlPageLinks,The length of the statement  "                shouldCrawlPageLinksDecision = (_shouldCrawlPageLinksDecisionMaker != null) ? _shouldCrawlPageLinksDecisionMaker.Invoke(crawledPage' _crawlContext) : new CrawlDecision { Allow = true }; " is 185.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,ShouldCrawlPageLinks,The length of the statement  "                _logger.DebugFormat("Links on page [{0}] not crawled' [{1}]"' crawledPage.Uri.AbsoluteUri' shouldCrawlPageLinksDecision.Reason); " is 128.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,ShouldCrawlPage,The length of the statement  "                shouldCrawlPageDecision = (_shouldCrawlPageDecisionMaker != null) ? _shouldCrawlPageDecisionMaker.Invoke(pageToCrawl' _crawlContext) : new CrawlDecision { Allow = true }; " is 170.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,ShouldRecrawlPage,The length of the statement  "                shouldRecrawlPageDecision = (_shouldRecrawlPageDecisionMaker != null) ? _shouldRecrawlPageDecisionMaker.Invoke(crawledPage' _crawlContext) : new CrawlDecision { Allow = true }; " is 176.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,CrawlThePage,The length of the statement  "                _logger.InfoFormat("Page crawl complete' Status:[NA] Url:[{0}] Elapsed:[{1}] Parent:[{2}] Retry:[{3}]"' crawledPage.Uri.AbsoluteUri' crawledPage.Elapsed' crawledPage.ParentUri' crawledPage.RetryCount); " is 201.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,CrawlThePage,The length of the statement  "                _logger.InfoFormat("Page crawl complete' Status:[{0}] Url:[{1}] Elapsed:[{2}] Parent:[{3}] Retry:[{4}]"' Convert.ToInt32(crawledPage.HttpWebResponse.StatusCode)' crawledPage.Uri.AbsoluteUri' crawledPage.Elapsed' crawledPage.ParentUri' crawledPage.RetryCount); " is 259.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,SchedulePageLinks,The length of the statement  "                    try //Added due to a bug in the Uri class related to this (http://stackoverflow.com/questions/2814951/system-uriformatexception-invalid-uri-the-hostname-could-not-be-parsed) " is 173.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,SchedulePageLinks,The length of the statement  "                            _logger.InfoFormat("MaxLinksPerPage has been reached. No more links will be scheduled for current page [{0}]."' crawledPage.Uri); " is 129.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,ShouldScheduleMorePageLink,The length of the statement  "            return _crawlContext.CrawlConfiguration.MaxLinksPerPage == 0 || _crawlContext.CrawlConfiguration.MaxLinksPerPage > linksAdded; " is 126.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,ShouldDownloadPageContent,The length of the statement  "                decision = (_shouldDownloadPageContentDecisionMaker != null) ? _shouldDownloadPageContentDecisionMaker.Invoke(crawledPage' _crawlContext) : new CrawlDecision { Allow = true }; " is 175.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,WaitMinimumRetryDelay,The length of the statement  "                _logger.WarnFormat("pageToCrawl.LastRequest value is null for Url:{0}. Cannot retry without this value."' pageToCrawl.Uri.AbsoluteUri); " is 135.
Long Statement,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,WaitMinimumRetryDelay,The length of the statement  "            //TODO Cannot use RateLimiter since it currently cannot handle dynamic sleep times so using Thread.Sleep in the meantime " is 120.
Long Statement,Abot.Poco,CrawledPage,C:\repos\sjdirect_abot\Abot\Poco\CrawledPage.cs,InitializeHtmlAgilityPackDocument,The length of the statement  "            hapDoc.OptionMaxNestedChildNodes = 5000;//did not make this an externally configurable property since it is really an internal issue to hap " is 139.
Long Statement,Abot.Util,GcMemoryMonitor,C:\repos\sjdirect_abot\Abot\Util\GcMemoryMonitor.cs,GetCurrentUsageInMb,The length of the statement  "            _logger.DebugFormat("GC reporting [{0}mb] currently thought to be allocated' took [{1}] millisecs"' currentUsageInMb' timer.ElapsedMilliseconds); " is 145.
Long Statement,Abot.Util,MemoryManager,C:\repos\sjdirect_abot\Abot\Util\MemoryManager.cs,IsSpaceAvailable,The length of the statement  "                _logger.Warn("MemoryFailPoint is not implemented on this platform. The MemoryManager.IsSpaceAvailable() will just return true."); " is 129.
Long Statement,Abot.Util,RateLimiter,C:\repos\sjdirect_abot\Abot\Util\RateLimiter.cs,ExitTimerCallback,The length of the statement  "            var timeUntilNextCheck = exitTimeValid ? Math.Min(TimeUnitMilliseconds' Math.Max(0' exitTime - Environment.TickCount)) : TimeUnitMilliseconds; " is 142.
Complex Conditional,Abot.Core,CrawlDecisionMaker,C:\repos\sjdirect_abot\Abot\Core\CrawlDecisionMaker.cs,ShouldCrawlPage,The conditional expression  "!pageToCrawl.IsRetry &&                  crawlContext.CrawlConfiguration.MaxPagesToCrawlPerDomain > 0 &&                  crawlContext.CrawlCountByDomain.TryGetValue(pageToCrawl.Uri.Authority' out pagesCrawledInThisDomain) &&                  pagesCrawledInThisDomain > 0"  is complex.
Complex Conditional,Abot.Core,RobotsDotTextFinder,C:\repos\sjdirect_abot\Abot\Core\RobotsDotTextFinder.cs,Find,The conditional expression  "page == null || page.WebException != null || page.HttpWebResponse == null || page.HttpWebResponse.StatusCode != HttpStatusCode.OK"  is complex.
Virtual Method Call from Constructor,Abot.Util,CachedMemoryMonitor,C:\repos\sjdirect_abot\Abot\Util\CachedMemoryMonitor.cs,CachedMemoryMonitor,The constructor "CachedMemoryMonitor" calls a virtual method "UpdateCurrentUsageValue".
Virtual Method Call from Constructor,Abot.Util,CachedMemoryMonitor,C:\repos\sjdirect_abot\Abot\Util\CachedMemoryMonitor.cs,CachedMemoryMonitor,The constructor "CachedMemoryMonitor" calls a virtual method "UpdateCurrentUsageValue".
Empty Catch Block,Abot.Core,WebContentExtractor,C:\repos\sjdirect_abot\Abot\Core\WebContentExtractor.cs,GetEncoding,The method has an empty catch block.
Empty Catch Block,Abot.Core,HyperLinkParser,C:\repos\sjdirect_abot\Abot\Core\HyperLinkParser.cs,GetUris,The method has an empty catch block.
Empty Catch Block,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,ProcessRedirect,The method has an empty catch block.
Empty Catch Block,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,SchedulePageLinks,The method has an empty catch block.
Magic Number,Abot.Core,BloomFilterCrawledUrlRepository,C:\repos\sjdirect_abot\Abot\Core\BloomFilterCrawledUrlRepository.cs,BloomFilterCrawledUrlRepository,The following statement contains a magic number: BloomFilter = bloomFilter ?? new BloomFilter<string>(2000001' 0.001F);
Magic Number,Abot.Core,CompactCrawledUrlRepository,C:\repos\sjdirect_abot\Abot\Core\CompactCrawledUrlRepository.cs,ComputeNumericId,The following statement contains a magic number: for (int i = 0; i < 8; i++)              {                  numericId += (long)md5[i] << (i * 8);              }
Magic Number,Abot.Core,CompactCrawledUrlRepository,C:\repos\sjdirect_abot\Abot\Core\CompactCrawledUrlRepository.cs,ComputeNumericId,The following statement contains a magic number: for (int i = 0; i < 8; i++)              {                  numericId += (long)md5[i] << (i * 8);              }
Magic Number,Abot.Core,WebContentExtractor,C:\repos\sjdirect_abot\Abot\Core\WebContentExtractor.cs,GetCharsetFromHeaders,The following statement contains a magic number: if (ctype != null)              {                  int ind = ctype.IndexOf("charset=");                  if (ind != -1)                      charset = ctype.Substring(ind + 8);              }
Magic Number,Abot.Core,WebContentExtractor,C:\repos\sjdirect_abot\Abot\Core\WebContentExtractor.cs,GetCharsetFromBody,The following statement contains a magic number: if (body != null)              {                  //find expression from : http://stackoverflow.com/questions/3458217/how-to-use-regular-expression-to-match-the-charset-string-in-html                  Match match = Regex.Match(body' @"<meta(?!\s*(?:name|value)\s*=)(?:[^>]*?content\s*=[\s""']*)?([^>]*?)[\s""';]*charset\s*=[\s""']*([^\s""'/>]*)"' RegexOptions.IgnoreCase);                  if (match.Success)                  {                      charset = string.IsNullOrWhiteSpace(match.Groups[2].Value) ? null : match.Groups[2].Value;                  }              }
Magic Number,Abot.Core,WebContentExtractor,C:\repos\sjdirect_abot\Abot\Core\WebContentExtractor.cs,GetCharsetFromBody,The following statement contains a magic number: if (body != null)              {                  //find expression from : http://stackoverflow.com/questions/3458217/how-to-use-regular-expression-to-match-the-charset-string-in-html                  Match match = Regex.Match(body' @"<meta(?!\s*(?:name|value)\s*=)(?:[^>]*?content\s*=[\s""']*)?([^>]*?)[\s""';]*charset\s*=[\s""']*([^\s""'/>]*)"' RegexOptions.IgnoreCase);                  if (match.Success)                  {                      charset = string.IsNullOrWhiteSpace(match.Groups[2].Value) ? null : match.Groups[2].Value;                  }              }
Magic Number,Abot.Core,WebContentExtractor,C:\repos\sjdirect_abot\Abot\Core\WebContentExtractor.cs,GetRawData,The following statement contains a magic number: try              {                  using (Stream rs = webResponse.GetResponseStream())                  {                      byte[] buffer = new byte[1024];                      int read = rs.Read(buffer' 0' buffer.Length);                      while (read > 0)                      {                          rawData.Write(buffer' 0' read);                          read = rs.Read(buffer' 0' buffer.Length);                      }                  }              }              catch (Exception e)              {                  _logger.WarnFormat("Error occurred while downloading content of url {0}"' webResponse.ResponseUri.AbsoluteUri);                  _logger.Warn(e);              }
Magic Number,Abot.Core,DomainRateLimiter,C:\repos\sjdirect_abot\Abot\Core\DomainRateLimiter.cs,DomainRateLimiter,The following statement contains a magic number: if(minCrawlDelayMillisecs > 0)                  _defaultMinCrawlDelayInMillisecs = minCrawlDelayMillisecs + 20;
Magic Number,Abot.Core,DomainRateLimiter,C:\repos\sjdirect_abot\Abot\Core\DomainRateLimiter.cs,RateLimit,The following statement contains a magic number: if(timer.ElapsedMilliseconds > 10)                  _logger.DebugFormat("Rate limited [{0}] [{1}] milliseconds"' uri.AbsoluteUri' timer.ElapsedMilliseconds);
Magic Number,Abot.Core,PageRequester,C:\repos\sjdirect_abot\Abot\Core\PageRequester.cs,BuildRequestObject,The following statement contains a magic number: if (_config.HttpRequestTimeoutInSeconds > 0)                  request.Timeout = _config.HttpRequestTimeoutInSeconds * 1000;
Magic Number,Abot.Crawler,PoliteWebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\PoliteWebCrawler.cs,Crawl,The following statement contains a magic number: if (_crawlContext.CrawlConfiguration.IsRespectRobotsDotTextEnabled)              {                  _robotsDotText = _robotsDotTextFinder.Find(uri);                    if (_robotsDotText != null)                  {                      FireRobotsDotTextParseCompletedAsync(_robotsDotText.Robots);                      FireRobotsDotTextParseCompleted(_robotsDotText.Robots);                        robotsDotTextCrawlDelayInSecs = _robotsDotText.GetCrawlDelay(_crawlContext.CrawlConfiguration.RobotsDotTextUserAgentString);                      robotsDotTextCrawlDelayInMillisecs = robotsDotTextCrawlDelayInSecs * 1000;                  }              }
Magic Number,Abot.Crawler,PoliteWebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\PoliteWebCrawler.cs,Crawl,The following statement contains a magic number: if (robotsDotTextCrawlDelayInSecs > 0 && robotsDotTextCrawlDelayInMillisecs > _crawlContext.CrawlConfiguration.MinCrawlDelayPerDomainMilliSeconds)              {                  if (robotsDotTextCrawlDelayInSecs > _crawlContext.CrawlConfiguration.MaxRobotsDotTextCrawlDelayInSeconds)                  {                      _logger.WarnFormat("[{0}] robot.txt file directive [Crawl-delay: {1}] is above the value set in the config value MaxRobotsDotTextCrawlDelay' will use MaxRobotsDotTextCrawlDelay value instead."' uri' _crawlContext.CrawlConfiguration.MaxRobotsDotTextCrawlDelayInSeconds);                        robotsDotTextCrawlDelayInSecs = _crawlContext.CrawlConfiguration.MaxRobotsDotTextCrawlDelayInSeconds;                      robotsDotTextCrawlDelayInMillisecs = robotsDotTextCrawlDelayInSecs * 1000;                  }                    _logger.WarnFormat("[{0}] robot.txt file directive [Crawl-delay: {1}] will be respected."' uri' robotsDotTextCrawlDelayInSecs);                  _domainRateLimiter.AddDomain(uri' robotsDotTextCrawlDelayInMillisecs);              }
Magic Number,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,Crawl,The following statement contains a magic number: if (_crawlContext.CrawlConfiguration.CrawlTimeoutSeconds > 0)              {                  _timeoutTimer = new Timer(_crawlContext.CrawlConfiguration.CrawlTimeoutSeconds * 1000);                  _timeoutTimer.Elapsed += HandleCrawlTimeout;                  _timeoutTimer.Start();              }
Magic Number,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,CrawlSite,The following statement contains a magic number: while (!_crawlComplete)              {                  RunPreWorkChecks();                    if (_scheduler.Count > 0)                  {                      _threadManager.DoWork(() => ProcessPage(_scheduler.GetNext()));                  }                  else if (!_threadManager.HasRunningThreads())                  {                      _crawlComplete = true;                  }                  else                  {                      _logger.DebugFormat("Waiting for links to be scheduled...");                      Thread.Sleep(2500);                  }              }
Magic Number,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,ProcessRedirect,The following statement contains a magic number: if (crawledPage.RedirectPosition >= 20)                  _logger.WarnFormat("Page [{0}] is part of a chain of 20 or more consecutive redirects' redirects for this chain will now be aborted."' crawledPage.Uri);
Magic Number,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,IsRedirect,The following statement contains a magic number: if (crawledPage.HttpWebResponse != null) {                  isRedirect = (_crawlContext.CrawlConfiguration.IsHttpRequestAutoRedirectsEnabled &&                      crawledPage.HttpWebResponse.ResponseUri != null &&                      crawledPage.HttpWebResponse.ResponseUri.AbsoluteUri != crawledPage.Uri.AbsoluteUri) ||                      (!_crawlContext.CrawlConfiguration.IsHttpRequestAutoRedirectsEnabled &&                      (int) crawledPage.HttpWebResponse.StatusCode >= 300 &&                      (int) crawledPage.HttpWebResponse.StatusCode <= 399);              }
Magic Number,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,IsRedirect,The following statement contains a magic number: if (crawledPage.HttpWebResponse != null) {                  isRedirect = (_crawlContext.CrawlConfiguration.IsHttpRequestAutoRedirectsEnabled &&                      crawledPage.HttpWebResponse.ResponseUri != null &&                      crawledPage.HttpWebResponse.ResponseUri.AbsoluteUri != crawledPage.Uri.AbsoluteUri) ||                      (!_crawlContext.CrawlConfiguration.IsHttpRequestAutoRedirectsEnabled &&                      (int) crawledPage.HttpWebResponse.StatusCode >= 300 &&                      (int) crawledPage.HttpWebResponse.StatusCode <= 399);              }
Magic Number,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,PrintConfigValues,The following statement contains a magic number: string indentString = new string(' '' 2);
Magic Number,Abot.Crawler,WebCrawler,C:\repos\sjdirect_abot\Abot\Crawler\WebCrawler.cs,WaitMinimumRetryDelay,The following statement contains a magic number: if (pageToCrawl.RetryAfter.HasValue)              {                  // Use the time to wait provided by the server instead of the config' if any.                  milliToWait = pageToCrawl.RetryAfter.Value*1000 - milliSinceLastRequest;              }              else              {                  if (!(milliSinceLastRequest < _crawlContext.CrawlConfiguration.MinRetryDelayInMilliseconds)) return;                  milliToWait = _crawlContext.CrawlConfiguration.MinRetryDelayInMilliseconds - milliSinceLastRequest;              }
Magic Number,Abot.Poco,CrawlConfiguration,C:\repos\sjdirect_abot\Abot\Poco\CrawlConfiguration.cs,CrawlConfiguration,The following statement contains a magic number: MaxConcurrentThreads = 10;
Magic Number,Abot.Poco,CrawlConfiguration,C:\repos\sjdirect_abot\Abot\Poco\CrawlConfiguration.cs,CrawlConfiguration,The following statement contains a magic number: MaxPagesToCrawl = 1000;
Magic Number,Abot.Poco,CrawlConfiguration,C:\repos\sjdirect_abot\Abot\Poco\CrawlConfiguration.cs,CrawlConfiguration,The following statement contains a magic number: MaxRobotsDotTextCrawlDelayInSeconds = 5;
Magic Number,Abot.Poco,CrawlConfiguration,C:\repos\sjdirect_abot\Abot\Poco\CrawlConfiguration.cs,CrawlConfiguration,The following statement contains a magic number: HttpRequestMaxAutoRedirects = 7;
Magic Number,Abot.Poco,CrawlConfiguration,C:\repos\sjdirect_abot\Abot\Poco\CrawlConfiguration.cs,CrawlConfiguration,The following statement contains a magic number: MaxCrawlDepth = 100;
Magic Number,Abot.Poco,CrawlConfiguration,C:\repos\sjdirect_abot\Abot\Poco\CrawlConfiguration.cs,CrawlConfiguration,The following statement contains a magic number: HttpServicePointConnectionLimit = 200;
Magic Number,Abot.Poco,CrawlConfiguration,C:\repos\sjdirect_abot\Abot\Poco\CrawlConfiguration.cs,CrawlConfiguration,The following statement contains a magic number: HttpRequestTimeoutInSeconds = 15;
Magic Number,Abot.Poco,CrawledPage,C:\repos\sjdirect_abot\Abot\Poco\CrawledPage.cs,InitializeHtmlAgilityPackDocument,The following statement contains a magic number: hapDoc.OptionMaxNestedChildNodes = 5000;
Magic Number,Abot.Util,BloomFilter<T>,C:\repos\sjdirect_abot\Abot\Util\BloomFilter.cs,BestK,The following statement contains a magic number: return (int)Math.Round(Math.Log(2.0) * BestM(capacity' errorRate) / capacity);
Magic Number,Abot.Util,BloomFilter<T>,C:\repos\sjdirect_abot\Abot\Util\BloomFilter.cs,BestM,The following statement contains a magic number: return (int)Math.Ceiling(capacity * Math.Log(errorRate' (1.0 / Math.Pow(2' Math.Log(2.0)))));
Magic Number,Abot.Util,BloomFilter<T>,C:\repos\sjdirect_abot\Abot\Util\BloomFilter.cs,BestM,The following statement contains a magic number: return (int)Math.Ceiling(capacity * Math.Log(errorRate' (1.0 / Math.Pow(2' Math.Log(2.0)))));
Magic Number,Abot.Util,BloomFilter<T>,C:\repos\sjdirect_abot\Abot\Util\BloomFilter.cs,BestErrorRate,The following statement contains a magic number: return (float)Math.Pow(0.6185' int.MaxValue / capacity);
Magic Number,Abot.Util,BloomFilter<T>,C:\repos\sjdirect_abot\Abot\Util\BloomFilter.cs,HashInt32,The following statement contains a magic number: unchecked  			{  				x = ~x + (x << 15); // x = (x << 15) - x- 1' as (~x) + y is equivalent to y - x - 1 in two's complement representation  				x = x ^ (x >> 12);  				x = x + (x << 2);  				x = x ^ (x >> 4);  				x = x * 2057; // x = (x + (x << 3)) + (x<< 11);  				x = x ^ (x >> 16);  				return (int)x;  			}
Magic Number,Abot.Util,BloomFilter<T>,C:\repos\sjdirect_abot\Abot\Util\BloomFilter.cs,HashInt32,The following statement contains a magic number: unchecked  			{  				x = ~x + (x << 15); // x = (x << 15) - x- 1' as (~x) + y is equivalent to y - x - 1 in two's complement representation  				x = x ^ (x >> 12);  				x = x + (x << 2);  				x = x ^ (x >> 4);  				x = x * 2057; // x = (x + (x << 3)) + (x<< 11);  				x = x ^ (x >> 16);  				return (int)x;  			}
Magic Number,Abot.Util,BloomFilter<T>,C:\repos\sjdirect_abot\Abot\Util\BloomFilter.cs,HashInt32,The following statement contains a magic number: unchecked  			{  				x = ~x + (x << 15); // x = (x << 15) - x- 1' as (~x) + y is equivalent to y - x - 1 in two's complement representation  				x = x ^ (x >> 12);  				x = x + (x << 2);  				x = x ^ (x >> 4);  				x = x * 2057; // x = (x + (x << 3)) + (x<< 11);  				x = x ^ (x >> 16);  				return (int)x;  			}
Magic Number,Abot.Util,BloomFilter<T>,C:\repos\sjdirect_abot\Abot\Util\BloomFilter.cs,HashInt32,The following statement contains a magic number: unchecked  			{  				x = ~x + (x << 15); // x = (x << 15) - x- 1' as (~x) + y is equivalent to y - x - 1 in two's complement representation  				x = x ^ (x >> 12);  				x = x + (x << 2);  				x = x ^ (x >> 4);  				x = x * 2057; // x = (x + (x << 3)) + (x<< 11);  				x = x ^ (x >> 16);  				return (int)x;  			}
Magic Number,Abot.Util,BloomFilter<T>,C:\repos\sjdirect_abot\Abot\Util\BloomFilter.cs,HashInt32,The following statement contains a magic number: unchecked  			{  				x = ~x + (x << 15); // x = (x << 15) - x- 1' as (~x) + y is equivalent to y - x - 1 in two's complement representation  				x = x ^ (x >> 12);  				x = x + (x << 2);  				x = x ^ (x >> 4);  				x = x * 2057; // x = (x + (x << 3)) + (x<< 11);  				x = x ^ (x >> 16);  				return (int)x;  			}
Magic Number,Abot.Util,BloomFilter<T>,C:\repos\sjdirect_abot\Abot\Util\BloomFilter.cs,HashInt32,The following statement contains a magic number: unchecked  			{  				x = ~x + (x << 15); // x = (x << 15) - x- 1' as (~x) + y is equivalent to y - x - 1 in two's complement representation  				x = x ^ (x >> 12);  				x = x + (x << 2);  				x = x ^ (x >> 4);  				x = x * 2057; // x = (x + (x << 3)) + (x<< 11);  				x = x ^ (x >> 16);  				return (int)x;  			}
Magic Number,Abot.Util,BloomFilter<T>,C:\repos\sjdirect_abot\Abot\Util\BloomFilter.cs,HashString,The following statement contains a magic number: for (int i = 0; i < s.Length; i++)  			{  				hash += s[i];  				hash += (hash << 10);  				hash ^= (hash >> 6);  			}
Magic Number,Abot.Util,BloomFilter<T>,C:\repos\sjdirect_abot\Abot\Util\BloomFilter.cs,HashString,The following statement contains a magic number: for (int i = 0; i < s.Length; i++)  			{  				hash += s[i];  				hash += (hash << 10);  				hash ^= (hash >> 6);  			}
Magic Number,Abot.Util,BloomFilter<T>,C:\repos\sjdirect_abot\Abot\Util\BloomFilter.cs,HashString,The following statement contains a magic number: hash += (hash << 3);
Magic Number,Abot.Util,BloomFilter<T>,C:\repos\sjdirect_abot\Abot\Util\BloomFilter.cs,HashString,The following statement contains a magic number: hash ^= (hash >> 11);
Magic Number,Abot.Util,BloomFilter<T>,C:\repos\sjdirect_abot\Abot\Util\BloomFilter.cs,HashString,The following statement contains a magic number: hash += (hash << 15);
Magic Number,Abot.Util,CachedMemoryMonitor,C:\repos\sjdirect_abot\Abot\Util\CachedMemoryMonitor.cs,CachedMemoryMonitor,The following statement contains a magic number: if (cacheExpirationInSeconds < 1)                  cacheExpirationInSeconds = 5;
Magic Number,Abot.Util,CachedMemoryMonitor,C:\repos\sjdirect_abot\Abot\Util\CachedMemoryMonitor.cs,CachedMemoryMonitor,The following statement contains a magic number: _usageRefreshTimer = new Timer(cacheExpirationInSeconds * 1000);
Magic Number,Abot.Util,GcMemoryMonitor,C:\repos\sjdirect_abot\Abot\Util\GcMemoryMonitor.cs,GetCurrentUsageInMb,The following statement contains a magic number: int currentUsageInMb = Convert.ToInt32(GC.GetTotalMemory(false) / (1024 * 1024));
Magic Number,Abot.Util,GcMemoryMonitor,C:\repos\sjdirect_abot\Abot\Util\GcMemoryMonitor.cs,GetCurrentUsageInMb,The following statement contains a magic number: int currentUsageInMb = Convert.ToInt32(GC.GetTotalMemory(false) / (1024 * 1024));
Magic Number,Abot.Util,ThreadManager,C:\repos\sjdirect_abot\Abot\Util\ThreadManager.cs,ThreadManager,The following statement contains a magic number: if ((maxThreads > 100) || (maxThreads < 1))                  throw new ArgumentException("MaxThreads must be from 1 to 100");
