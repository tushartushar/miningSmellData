Implementation smell,Namespace,Class,File,Method,Description
Long Method,kafka4net,Producer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The method has 137 lines of code.
Long Method,kafka4net,Producer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The method has 192 lines of code.
Long Method,kafka4net.ConsumerImpl,Fetcher,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\ConsumerImpl\Fetcher.cs,FetchLoop,The method has 120 lines of code.
Long Method,kafka4net.Internal,PartitionRecoveryMonitor,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The method has 171 lines of code.
Long Method,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,The method has 108 lines of code.
Complex Method,kafka4net,Producer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Producer.cs,CloseAsync,Cyclomatic complexity of the method is 8
Complex Method,kafka4net,Producer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Producer.cs,SendLoop,Cyclomatic complexity of the method is 10
Complex Method,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,ReadHeader,Cyclomatic complexity of the method is 9
Complex Method,kafka4net.Internal,PartitionRecoveryMonitor,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,Cyclomatic complexity of the method is 10
Complex Method,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,Cyclomatic complexity of the method is 16
Long Parameter List,kafka4net,ProducerConfiguration,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\ProducerConfiguration.cs,ProducerConfiguration,The method has 10 parameters. Parameters: topic' batchFlushTime' batchFlushSize' requiredAcks' autoGrowSendBuffers' sendBuffersInitialSize' maxMessageSetSizeInBytes' producerRequestTimeout' partitioner' compressionType
Long Parameter List,kafka4net,ConsumerConfiguration,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\ConsumerConfiguration.cs,ConsumerConfiguration,The method has 11 parameters. Parameters: seedBrokers' topic' startPosition' maxWaitTimeMs' minBytesPerFetch' maxBytesPerFetch' lowWatermark' highWatermark' useFlowControl' stopPosition' scheduler
Long Parameter List,kafka4net.ConsumerImpl,Fetcher,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\ConsumerImpl\Fetcher.cs,Fetcher,The method has 5 parameters. Parameters: cluster' broker' protocol' consumerConfig' cancel
Long Parameter List,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherMessage,The method has 5 parameters. Parameters: id' keyLen' valueLen' offset' partition
Long Parameter List,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherFetchRequest,The method has 6 parameters. Parameters: id' topicCount' partsCount' host' port' brokerId
Long Parameter List,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_HealedPartitions,The method has 6 parameters. Parameters: monitorId' host' port' nodeId' topicName' partitions
Long Parameter List,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataPartitionErrorChange,The method has 5 parameters. Parameters: clusterId' topic' partId' oldCode' newCode
Long Parameter List,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataPartitionIsrChange,The method has 5 parameters. Parameters: clusterId' topic' partId' oldIsrs' newIsrs
Long Parameter List,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataPartitionLeaderChange,The method has 5 parameters. Parameters: clusterId' topic' partId' oldLeader' newLeader
Long Parameter List,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataPartitionReplicasChange,The method has 5 parameters. Parameters: clusterId' topic' partId' oldReplicas' newReplicas
Long Identifier,kafka4net,Consumer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Consumer.cs,,The length of the parameter _outstandingMessageProcessingCount is 34.
Long Identifier,kafka4net.ConsumerImpl,Fetcher,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\ConsumerImpl\Fetcher.cs,Subscribe,The length of the parameter receivedMessagesSubscriptionCleanup is 35.
Long Identifier,kafka4net.Tracing,Opcodes,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,,The length of the parameter MarkSocketAsFailedCorrelationLoopCancelling is 43.
Long Statement,kafka4net,Connection,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Connection.cs,GetClientAsync,The length of the statement  "                            _log.Debug("CorrelationLoop completed with status {0}. {1}"' t.Status' t.Exception==null?"": string.Format("Closing connection because of error. {0}"'t.Exception.Message)); " is 172.
Long Statement,kafka4net,Consumer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Consumer.cs,BuildTopicPartitionsAsync,The length of the statement  "                        string.Join("'"' partitions.Partitions.OrderBy(p => p).Select(p => string.Format("{0}:{1}"' p' partitions.NextOffset(p))))); " is 124.
Long Statement,kafka4net,Consumer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Consumer.cs,BuildTopicPartitionsAsync,The length of the statement  "                startPositionProvider = new TopicPartitionOffsets(partitions.Topic' partitions.GetPartitionsOffset.Where(kv=> origProvider.ShouldConsumePartition(kv.Key))); " is 156.
Long Statement,kafka4net,Consumer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Consumer.cs,OnTopicPartitionComplete,The length of the statement  "                // If we call OnCompleted right now' any last message that may be being sent will not process. Just tell the scheduler to do it in a moment. " is 140.
Long Statement,kafka4net,Logger,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Logger.cs,SetupNLog,The length of the statement  "            var method = logManagerType.GetMethod("GetLogger"' BindingFlags.InvokeMethod | BindingFlags.Public | BindingFlags.Static' null' CallingConventions.Standard' new[] {typeof(string)}' null); " is 187.
Long Statement,kafka4net,Logger,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Logger.cs,SetupLog4Net,The length of the statement  "            var method = logManagerType.GetMethod("GetLogger"' BindingFlags.InvokeMethod | BindingFlags.Public | BindingFlags.Static' null' CallingConventions.Standard' new[] { typeof(string) }' null); " is 189.
Long Statement,kafka4net,Producer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The length of the statement  "                                    _queueSizeEvents.OnNext(new QueueResizeInfo { PartitionId = p.PartitionId' Size = queue.Queue.Size' Capacity = queue.Queue.Capacity }); " is 135.
Long Statement,kafka4net,Producer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The length of the statement  "                                        _queueSizeEvents.OnNext(new QueueResizeInfo { PartitionId = queue.Partition' Size = queue.Queue.Size' Capacity = queue.Queue.Capacity }); " is 137.
Long Statement,kafka4net,Producer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The length of the statement  "                                            _log.Warn("Capacity of send buffer with size {3} not large enough to accept {2} new messages. Increasing capacity from {0} to {1}"' queue.Queue.Capacity' queue.Queue.Capacity + growBy' batchAr.Length' queue.Queue.Size); " is 219.
Long Statement,kafka4net,Producer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The length of the statement  "                                            _queueSizeEvents.OnNext(new QueueResizeInfo { PartitionId = queue.Partition' Size = queue.Queue.Size' Capacity = queue.Queue.Capacity }); " is 137.
Long Statement,kafka4net,Producer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The length of the statement  "                // if we are being told to shut down (already waited for drain)' then send all messages back over OnPermError' and quit. " is 120.
Long Statement,kafka4net,Producer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The length of the statement  "                    _log.Debug("There are {0} partition queues with {1} total messages to send."' queuesToBeSent.Length' queuesToBeSent.Sum(qi=>qi.Queue.Size)); " is 140.
Long Statement,kafka4net,Producer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The length of the statement  "                _log.Debug("Locking queues: '{0}'/[{1}]"' Configuration.Topic' string.Join("'"' queuesToBeSent.Select(q => q.Partition))); " is 122.
Long Statement,kafka4net,Producer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The length of the statement  "                                        _log.Warn("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode); " is 130.
Long Statement,kafka4net,Producer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The length of the statement  "                                    string error = string.Join("' "' permanentErrorPartitions.Select(p => string.Format("{0}:{1}"' p.Partition' p.ErrorCode))); " is 123.
Long Statement,kafka4net,Producer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The length of the statement  "                                //brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError))); " is 168.
Long Statement,kafka4net,Producer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Producer.cs,ToString,The length of the statement  "            return string.Format("'{0}' Batch flush time: {1} Batch flush size: {2}"' Topic' Configuration.BatchFlushTime' Configuration.BatchFlushSize); " is 141.
Long Statement,kafka4net,Cluster,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Cluster.cs,ConnectAsync,The length of the statement  "                _partitionRecoveryMonitor.NewMetadataEvents.Subscribe(MergeTopicMeta' ex => _log.Error(ex' "Error thrown by RecoveryMonitor.NewMetadataEvents!")); " is 146.
Long Statement,kafka4net,Cluster,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Cluster.cs,FetchPartitionOffsetsImplAsync,The length of the statement  "                            throw new AggregateException("Failure when getting offsets info"' requests.Where(r => r.IsFaulted).Select(r => r.Exception)); " is 125.
Long Statement,kafka4net,Cluster,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Cluster.cs,FetchPartitionOffsetsImplAsync,The length of the statement  "                            throw new Exception(string.Format("Partition Errors: [{0}]"' string.Join("'"' partitions.Select(p=>p.Partition + ":" + p.ErrorCode)))); " is 135.
Long Statement,kafka4net,Cluster,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Cluster.cs,FetchPartitionOffsetsImplAsync,The length of the statement  "                        //    throw new Exception(string.Format("Partition Head Offset is -1 for partition(s): [{0}]"' string.Join("'"' partitions.Select(p => p.Partition + ":" + (p.Offsets.Length == 1 ? -1 : p.Offsets[1]))))); " is 203.
Long Statement,kafka4net,Cluster,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Cluster.cs,BuildPartitionStateChangeSubject,The length of the statement  "             _partitionStateChanges.Subscribe(psc => _log.Info("Cluster saw new partition state: {0}-{1}-{2}"' psc.Topic' psc.PartitionId' psc.ErrorCode)' " is 141.
Long Statement,kafka4net,Cluster,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Cluster.cs,MergeTopicMeta,The length of the statement  "                    EtwTrace.Log.MetadataPartitionErrorChange(_id' _.TopicName' _.oldPart.Id' (int)_.oldPart.ErrorCode' (int)_.updatedPart.ErrorCode); " is 130.
Long Statement,kafka4net,Cluster,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Cluster.cs,MergeTopicMeta,The length of the statement  "                    EtwTrace.Log.MetadataPartitionIsrChange(_id' _.TopicName' _.oldPart.Id' string.Join("'"' _.oldPart.Isr)' string.Join("'"' _.updatedPart.Isr)); " is 142.
Long Statement,kafka4net,Cluster,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Cluster.cs,MergeTopicMeta,The length of the statement  "                    EtwTrace.Log.MetadataPartitionReplicasChange(_id' _.TopicName' _.oldPart.Id' string.Join("'"' _.oldPart.Replicas)' string.Join("'"' _.updatedPart.Replicas)); " is 157.
Long Statement,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,Flush,The length of the statement  "            var compressedSize = LZ4Codec.Encode(_uncompressedBuffer' 0' _bufferLen' _compressedBuffer' 0' _compressedBuffer.Length); " is 121.
Long Statement,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,Flush,The length of the statement  "                LittleEndianConverter.Write((uint)(_bufferLen | 1 << 31)' _compressedBuffer' 0); // highest bit set indicates no compression " is 124.
Long Statement,kafka4net.ConsumerImpl,StartAndStopAtExplicitOffsets,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\ConsumerImpl\PositionProviders.cs,ShouldConsumePartition,The length of the statement  "            // we should only consume from this partition if we were told to by the start offsets and also if we are not stopping where we already are. " is 139.
Long Statement,kafka4net.ConsumerImpl,StartAndStopAtExplicitOffsets,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\ConsumerImpl\PositionProviders.cs,ShouldConsumePartition,The length of the statement  "            return _startingOffsets.ShouldConsumePartition(partitionId) && _startingOffsets.NextOffset(partitionId) != _stoppingOffsets.NextOffset(partitionId); " is 148.
Long Statement,kafka4net.ConsumerImpl,TopicPartition,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\ConsumerImpl\TopicPartition.cs,OnNext,The length of the statement  "                    _log.Debug("{0} Skipping message offset {1} as it is less than requested offset {2}"'this'value.Offset'_partitionFetchState.Offset); " is 132.
Long Statement,kafka4net.ConsumerImpl,TopicPartition,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\ConsumerImpl\TopicPartition.cs,OnCompleted,The length of the statement  "            _log.Warn("{0} Recieved OnComplete from Fetcher. Fetcher may have errored out. Waiting for new or updated Fetcher."' this); " is 123.
Long Statement,kafka4net.ConsumerImpl,TopicPartition,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\ConsumerImpl\TopicPartition.cs,Equals,The length of the statement  "            return _cluster == objTopicPartition._cluster && _topic == objTopicPartition._topic && _partitionId == objTopicPartition._partitionId; " is 134.
Long Statement,kafka4net.ConsumerImpl,Fetcher,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\ConsumerImpl\Fetcher.cs,Subscribe,The length of the statement  "                cleanup.Add(Disposable.Create(() => _log.Debug("Fetcher #{0} {1} topicPartition is unsubscribing"' _id' topicPartition))); " is 122.
Long Statement,kafka4net.ConsumerImpl,Fetcher,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\ConsumerImpl\Fetcher.cs,BuildReceivedMessages,The length of the statement  "            Do(msg => EtwTrace.Log.FetcherMessage(_id' msg.Key != null ? msg.Key.Length : -1' msg.Value != null ? msg.Value.Length : -1' msg.Offset' msg.Partition)) " is 152.
Long Statement,kafka4net.ConsumerImpl,Fetcher,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\ConsumerImpl\Fetcher.cs,FetchLoop,The length of the statement  "                        EtwTrace.Log.FetcherFetchRequest(_id' fetchRequest.Topics.Length' fetchRequest.Topics.Sum(td => td.Partitions.Length)' _broker.Host' _broker.Port' _broker.NodeId); " is 163.
Long Statement,kafka4net.ConsumerImpl,Fetcher,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\ConsumerImpl\Fetcher.cs,FetchLoop,The length of the statement  "                        fetch.Topics.SelectMany(t => t.Partitions.Select(p => new PartitionStateChangeEvent(t.Topic' p.Partition' p.ErrorCode))) " is 120.
Long Statement,kafka4net.Internal,PartitionRecoveryMonitor,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The length of the statement  "                    response = await _protocol.MetadataRequest(new TopicRequest { Topics = _failedList.Keys.Select(t => t.Item1).Distinct().ToArray() }' broker' noTransportErrors: true); " is 166.
Long Statement,kafka4net.Internal,PartitionRecoveryMonitor,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The length of the statement  "                        _log.Debug("Out of {0} partitions returned from broker {2}' none of the {3} errored partitions are healed. Current partition states for errored partitions: [{1}]"' " is 163.
Long Statement,kafka4net.Internal,PartitionRecoveryMonitor,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The length of the statement  "                                .SelectMany(t => t.Partitions.Select(p => new { t.TopicName' TopicErrorCode = t.ErrorCode' PartitionId = p.Id' PartitionErrorCode = p.ErrorCode })) " is 147.
Long Statement,kafka4net.Internal,PartitionRecoveryMonitor,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The length of the statement  "                        foreach (var leader in maybeHealedPartitions.GroupBy(p => p.Item3' (i' tuples) => new { Leader = i' Topics = tuples.GroupBy(t => t.Item1) })) " is 141.
Long Statement,kafka4net.Internal,PartitionRecoveryMonitor,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The length of the statement  "                                _log.Error("Got metadata response with partition refering to a broker which is not part of the response: {0}"' response.ToString()); " is 132.
Long Statement,kafka4net.Internal,PartitionRecoveryMonitor,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The length of the statement  "                            MetadataResponse response2 = await _protocol.MetadataRequest(new TopicRequest { Topics = brokerGrp.Select(g=>g.Item1).Distinct().ToArray() }' newBroker' noTransportErrors: true); " is 178.
Long Statement,kafka4net.Internal,PartitionRecoveryMonitor,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The length of the statement  "                                Brokers = response2.Brokers' // we may broadcast more than 1 broker' but it should be ok because discovery of new broker metadata does not cause any actions " is 156.
Long Statement,kafka4net.Internal,PartitionRecoveryMonitor,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The length of the statement  "                                    EtwTrace.Log.RecoveryMonitor_HealedPartitions(_id' newBroker.Host' newBroker.Port' newBroker.NodeId' topic.TopicName' string.Join("'"' topic.Partitions.Select(p => p.Id))); " is 172.
Long Statement,kafka4net.Internal,PartitionRecoveryMonitor,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The length of the statement  "                // we will keep accumulating responses in memory faster than they time out. See https://github.com/ntent-ad/kafka4net/issues/30 " is 127.
Long Statement,kafka4net.Metadata,PartitionMeta,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Metadata\PartitionMeta.cs,ToString,The length of the statement  "            return string.Format("Id: {0} Leader: {1} Error: {2}' Replicas: {3}' Isr: {4}"' Id' Leader' ErrorCode' IntToStringList(Replicas)' IntToStringList(Isr)); " is 152.
Long Statement,kafka4net.Metadata,TopicMeta,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Metadata\TopicMeta.cs,ToString,The length of the statement  "            return string.Format("Topic '{0}' {1} Partitions [{2}]"' TopicName' ErrorCode' string.Join("'"' Partitions.AsEnumerable())); " is 124.
Long Statement,kafka4net.Protocols,ResponseCorrelation,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\ResponseCorrelation.cs,CorrelateResponseLoop,The length of the statement  "                _corelationTable.Values.ForEach(c => c(null' 0' new CorrelationLoopException("Correlation loop closed. Request will never get a response.") { IsRequestedClose = cancel.IsCancellationRequested })); " is 196.
Long Statement,kafka4net.Protocols,ResponseCorrelation,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\ResponseCorrelation.cs,SendAndCorrelateAsync,The length of the statement  "                // TODO: think how buff can be reused. Would it be safe to declare buffer as a member? Is there a guarantee of one write at the time? " is 133.
Long Statement,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,Compress,The length of the statement  "                        var snappy = new KafkaSnappyStream(compressed' CompressionStreamMode.Compress' _snappyUncompressedBuffer' _snappyCompressedBuffer); " is 131.
Long Statement,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,The length of the statement  "                        throw new BrokerException(string.Format("Corrupt message: Crc does not match. Caclulated {0} but got {1}"' computedCrc' crc)); " is 126.
Long Statement,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,The length of the statement  "                    using (var snappyStream = new KafkaSnappyStream(new MemoryStream(value)' CompressionStreamMode.Decompress' _snappyUncompressedBuffer' _snappyCompressedBuffer)) " is 159.
Long Statement,kafka4net.Protocols.Requests,FetchRequest,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Requests\FetchRequest.cs,ToString,The length of the statement  "            return string.Format("MaxTime: {0} MinBytes: {1} [{2}]"' MaxWaitTime' MinBytes' Topics == null ? "null" : string.Join("'"' Topics.AsEnumerable())); " is 147.
Long Statement,kafka4net.Protocols.Requests,TopicData,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Requests\FetchRequest.cs,ToString,The length of the statement  "                return string.Format("Topic: {0} Parts: [{1}]"' Topic' Partitions == null ? "null" : string.Join("' "' Partitions.AsEnumerable())); " is 131.
Long Statement,kafka4net.Protocols.Requests,OffsetRequest,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Requests\OffsetRequest.cs,ToString,The length of the statement  "            return string.Format("{0} Id:Time:MaxNumOffsets [{1}]"' TopicName' Partitions == null ? "null" : string.Join("\n "' Partitions.AsEnumerable())); " is 144.
Long Statement,kafka4net.Protocols.Responses,TopicFetchData,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Responses\FetchResponse.cs,ToString,The length of the statement  "                return string.Format("'{0}' PartId:ErrorCode:HighWatermarkOffset:MessageCount [{1}]"' Topic' Partitions == null ? "null" : string.Join("\n  "' Partitions.Where(p => !onlyPartitionsWithMessages || p.Messages.Length > 0).AsEnumerable())); " is 236.
Long Statement,kafka4net.Protocols.Responses,PartitionFetchData,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Responses\FetchResponse.cs,ToString,The length of the statement  "                return string.Format("{0}:{1}:{2}:{3}"' Partition' ErrorCode' HighWatermarkOffset' Messages == null ? "null" : Messages.Length.ToString()); " is 139.
Long Statement,kafka4net.Protocols.Responses,OffsetResponse,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Responses\OffsetResponse.cs,ToString,The length of the statement  "            return string.Format("'{0}' [{1}]"' TopicName' Partitions == null ? "null" : string.Join("'"' Partitions.Select(p => p.ToString()))); " is 133.
Long Statement,kafka4net.Protocols.Responses,TopicResponse,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Responses\ProducerResponse.cs,ToString,The length of the statement  "                return string.Format("{0} [{1}]"' TopicName' Partitions == null ? "null" : string.Join("\n  "' Partitions.AsEnumerable())); " is 123.
Empty Catch Block,kafka4net,Connection,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Connection.cs,GetClientAsync,The method has an empty catch block.
Empty Catch Block,kafka4net,Connection,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Connection.cs,MarkSocketAsFailed,The method has an empty catch block.
Empty Catch Block,kafka4net,Connection,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Connection.cs,ShutdownAsync,The method has an empty catch block.
Empty Catch Block,kafka4net,Connection,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Connection.cs,ShutdownAsync,The method has an empty catch block.
Empty Catch Block,kafka4net,Consumer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Consumer.cs,Dispose,The method has an empty catch block.
Empty Catch Block,kafka4net,Consumer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Consumer.cs,CloseAsync,The method has an empty catch block.
Empty Catch Block,kafka4net,Cluster,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Cluster.cs,CloseAsyncImpl,The method has an empty catch block.
Empty Catch Block,kafka4net,Cluster,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Cluster.cs,CloseAsyncImpl,The method has an empty catch block.
Magic Number,kafka4net,FletcherHashedMessagePartitioner,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\FletcherHashedMessagePartitioner.cs,Fletcher32HashOptimized,The following statement contains a magic number: var tlen = words > 359 ? 359 : words;
Magic Number,kafka4net,FletcherHashedMessagePartitioner,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\FletcherHashedMessagePartitioner.cs,Fletcher32HashOptimized,The following statement contains a magic number: var tlen = words > 359 ? 359 : words;
Magic Number,kafka4net,FletcherHashedMessagePartitioner,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\FletcherHashedMessagePartitioner.cs,Fletcher32HashOptimized,The following statement contains a magic number: sum1 = (sum1 & 0xffff) + (sum1 >> 16);
Magic Number,kafka4net,FletcherHashedMessagePartitioner,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\FletcherHashedMessagePartitioner.cs,Fletcher32HashOptimized,The following statement contains a magic number: sum2 = (sum2 & 0xffff) + (sum2 >> 16);
Magic Number,kafka4net,FletcherHashedMessagePartitioner,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\FletcherHashedMessagePartitioner.cs,Fletcher32HashOptimized,The following statement contains a magic number: sum1 = (sum1 & 0xffff) + (sum1 >> 16);
Magic Number,kafka4net,FletcherHashedMessagePartitioner,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\FletcherHashedMessagePartitioner.cs,Fletcher32HashOptimized,The following statement contains a magic number: sum2 = (sum2 & 0xffff) + (sum2 >> 16);
Magic Number,kafka4net,FletcherHashedMessagePartitioner,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\FletcherHashedMessagePartitioner.cs,Fletcher32HashOptimized,The following statement contains a magic number: return (sum2 << 16 | sum1);
Magic Number,kafka4net,ProducerConfiguration,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\ProducerConfiguration.cs,ProducerConfiguration,The following statement contains a magic number: BatchFlushTime = batchFlushTime ?? TimeSpan.FromMilliseconds(500);
Magic Number,kafka4net,Connection,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Connection.cs,ParseAddress,The following statement contains a magic number: return seedConnections.Split(''').                  Select(_ => _.Trim()).                  Where(_ => _ != null).                  Select(s =>                  {                      int port = 9092;                      string host = null;                      if (s.Contains(':'))                      {                          var parts = s.Split(new[] { ":" }' StringSplitOptions.RemoveEmptyEntries);                          if (parts.Length == 2)                          {                              host = parts[0];                              port = int.Parse(parts[1]);                          }                      }                      else                      {                          host = s;                      }                      return Tuple.Create(host' port);                  }).ToArray();
Magic Number,kafka4net,Connection,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Connection.cs,ParseAddress,The following statement contains a magic number: return seedConnections.Split(''').                  Select(_ => _.Trim()).                  Where(_ => _ != null).                  Select(s =>                  {                      int port = 9092;                      string host = null;                      if (s.Contains(':'))                      {                          var parts = s.Split(new[] { ":" }' StringSplitOptions.RemoveEmptyEntries);                          if (parts.Length == 2)                          {                              host = parts[0];                              port = int.Parse(parts[1]);                          }                      }                      else                      {                          host = s;                      }                      return Tuple.Create(host' port);                  }).ToArray();
Magic Number,kafka4net,Consumer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Consumer.cs,Dispose,The following statement contains a magic number: _cluster.CloseAsync(TimeSpan.FromSeconds(5)).                          ContinueWith(t => _log.Error(t.Exception' "Error when closing Cluster")' TaskContinuationOptions.OnlyOnFaulted);
Magic Number,kafka4net,Consumer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Consumer.cs,CloseAsync,The following statement contains a magic number: await _cluster.CloseAsync(TimeSpan.FromSeconds(5));
Magic Number,kafka4net,Producer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The following statement contains a magic number: await await _cluster.Scheduler.Ask(async () =>                  {                      await _sync.WaitAsync();                        try                      {                          if (IsConnected)                              return;                            _log.Debug("Connecting producer {1} for topic {0}"' Topic' _id);                          EtwTrace.Log.ProducerStarting(Topic' _id);                          _sendMessagesSubject = new Subject<Message>();                            if (_cluster.State != Cluster.ClusterState.Connected)                          {                              _log.Debug("Connecting cluster");                              await _cluster.ConnectAsync();                              _log.Debug("Connected cluster");                          }                        // Recovery: subscribe to partition offline/online events                      _partitionStateSubsctiption = _cluster.PartitionStateChanges.                              Where(p => p.Topic == Configuration.Topic).                              Synchronize(_allPartitionQueues).                              Subscribe(p =>                              {                                  PartitionQueueInfo queue;                                  if (!_allPartitionQueues.TryGetValue(p.PartitionId' out queue))                                  {                                      queue = new PartitionQueueInfo(Configuration.SendBuffersInitialSize) { Partition = p.PartitionId };                                      _allPartitionQueues.Add(p.PartitionId' queue);                                      _queueSizeEvents.OnNext(new QueueResizeInfo { PartitionId = p.PartitionId' Size = queue.Queue.Size' Capacity = queue.Queue.Capacity });                                  }                                    _log.Info("Detected change in topic/partition '{0}'/{1}/{2} IsOnline {3}->{4}"'                                      Configuration.Topic'                                      p.PartitionId'                                      p.ErrorCode'                                      queue.IsOnline'                                      p.ErrorCode.IsSuccess());                                    queue.IsOnline = p.ErrorCode.IsSuccess();                                  _queueEventWaitHandler.Set();                              });                        // get all of the partitions for this topic. Allows the MessagePartitioner to select a partition.                      var topicPartitions = await _cluster.GetOrFetchMetaForTopicAsync(Configuration.Topic);                          _log.Debug("Producer found {0} partitions for '{1}'"' topicPartitions.Length' Configuration.Topic);                            _sendMessagesSubject.                              Do(msg => msg.PartitionId = Configuration.Partitioner.GetMessagePartition(msg' topicPartitions).Id).                              Buffer(Configuration.BatchFlushTime' Configuration.BatchFlushSize).                              Where(b => b.Count > 0).                              Select(msgs => msgs.GroupBy(msg => msg.PartitionId)).                              ObserveOn(_cluster.Scheduler).                              Subscribe(partitionGroups =>                              {                                  foreach (var batch in partitionGroups)                                  {                                  // partition queue might be created if metadata broadcast fired already                                  PartitionQueueInfo queue;                                      if (!_allPartitionQueues.TryGetValue(batch.Key' out queue))                                      {                                          queue = new PartitionQueueInfo(Configuration.SendBuffersInitialSize) { Partition = batch.Key };                                          _allPartitionQueues.Add(batch.Key' queue);                                          queue.IsOnline = topicPartitions.First(p => p.Id == batch.Key).ErrorCode.IsSuccess();                                          _log.Debug("{0} added new partition queue"' this);                                          _queueSizeEvents.OnNext(new QueueResizeInfo { PartitionId = queue.Partition' Size = queue.Queue.Size' Capacity = queue.Queue.Capacity });                                      }                                    // now queue them up                                  var batchAr = batch.ToArray();                                    // make sure we have space.                                  if (queue.Queue.Size + batchAr.Length > queue.Queue.Capacity)                                      {                                      // try to increase the capacity.                                      if (Configuration.AutoGrowSendBuffers)                                          {                                              var growBy = Math.Max(queue.Queue.Capacity / 2 + 1' 2 * batchAr.Length);                                              _log.Warn("Capacity of send buffer with size {3} not large enough to accept {2} new messages. Increasing capacity from {0} to {1}"' queue.Queue.Capacity' queue.Queue.Capacity + growBy' batchAr.Length' queue.Queue.Size);                                                queue.Queue.Capacity += growBy;                                              _queueSizeEvents.OnNext(new QueueResizeInfo { PartitionId = queue.Partition' Size = queue.Queue.Size' Capacity = queue.Queue.Capacity });                                          }                                          else                                          {                                          // we're full and not allowed to grow. Throw the batch back to the caller' and continue on.                                          var msg = string.Format("Send Buffer Full for partition {0}. "'                                                          Cluster.PartitionStateChanges.Where(                                                              ps => ps.Topic == Configuration.Topic && ps.PartitionId == queue.Partition)                                                              .Take(1)                                                              .Wait());                                              _log.Error(msg);                                              if (OnPermError != null)                                                  Task.Factory.StartNew(() => OnPermError(new Exception(msg)' batchAr));                                              continue;                                          }                                      }                                    // we have the space' add to the queue                                  queue.Queue.Put(batchAr);                                        if (_log.IsDebugEnabled)                                          _log.Debug("Enqueued batch of size {0} for topic '{1}' partition {2}"'                                              batchAr.Length' Configuration.Topic' batch.Key);                                    // After batch enqueued' send wake up signal to sending queue                                  _queueEventWaitHandler.Set();                                  }                              }' e => _log.Fatal(e' "Error in _sendMessagesSubject pipeline")'                                  () => _log.Debug("_sendMessagesSubject complete")                              );                        // start the send loop task                      _cluster.Scheduler.Schedule(() => {                              _sendLoopTask = SendLoop().                                  ContinueWith(t =>                                  {                                      if (t.IsFaulted)                                          _log.Fatal(t.Exception' "SendLoop failed");                                      else                                          _log.Debug("SendLoop complete with status: {0}"' t.Status);                                  });                          });                          _log.Debug("Connected");                          EtwTrace.Log.ProducerStarted(Topic' _id);                      }                      catch (Exception e)                      {                          _log.Error(e' "Exception during connect");                          EtwTrace.Log.ProducerError(e.Message' _id);                          throw;                      }                      finally                      {                          _log.Debug("#{0} Releasing Producer Semaphore."' _id);                          _sync.Release();                      }                  }).ConfigureAwait(false);
Magic Number,kafka4net,Producer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The following statement contains a magic number: await await _cluster.Scheduler.Ask(async () =>                  {                      await _sync.WaitAsync();                        try                      {                          if (IsConnected)                              return;                            _log.Debug("Connecting producer {1} for topic {0}"' Topic' _id);                          EtwTrace.Log.ProducerStarting(Topic' _id);                          _sendMessagesSubject = new Subject<Message>();                            if (_cluster.State != Cluster.ClusterState.Connected)                          {                              _log.Debug("Connecting cluster");                              await _cluster.ConnectAsync();                              _log.Debug("Connected cluster");                          }                        // Recovery: subscribe to partition offline/online events                      _partitionStateSubsctiption = _cluster.PartitionStateChanges.                              Where(p => p.Topic == Configuration.Topic).                              Synchronize(_allPartitionQueues).                              Subscribe(p =>                              {                                  PartitionQueueInfo queue;                                  if (!_allPartitionQueues.TryGetValue(p.PartitionId' out queue))                                  {                                      queue = new PartitionQueueInfo(Configuration.SendBuffersInitialSize) { Partition = p.PartitionId };                                      _allPartitionQueues.Add(p.PartitionId' queue);                                      _queueSizeEvents.OnNext(new QueueResizeInfo { PartitionId = p.PartitionId' Size = queue.Queue.Size' Capacity = queue.Queue.Capacity });                                  }                                    _log.Info("Detected change in topic/partition '{0}'/{1}/{2} IsOnline {3}->{4}"'                                      Configuration.Topic'                                      p.PartitionId'                                      p.ErrorCode'                                      queue.IsOnline'                                      p.ErrorCode.IsSuccess());                                    queue.IsOnline = p.ErrorCode.IsSuccess();                                  _queueEventWaitHandler.Set();                              });                        // get all of the partitions for this topic. Allows the MessagePartitioner to select a partition.                      var topicPartitions = await _cluster.GetOrFetchMetaForTopicAsync(Configuration.Topic);                          _log.Debug("Producer found {0} partitions for '{1}'"' topicPartitions.Length' Configuration.Topic);                            _sendMessagesSubject.                              Do(msg => msg.PartitionId = Configuration.Partitioner.GetMessagePartition(msg' topicPartitions).Id).                              Buffer(Configuration.BatchFlushTime' Configuration.BatchFlushSize).                              Where(b => b.Count > 0).                              Select(msgs => msgs.GroupBy(msg => msg.PartitionId)).                              ObserveOn(_cluster.Scheduler).                              Subscribe(partitionGroups =>                              {                                  foreach (var batch in partitionGroups)                                  {                                  // partition queue might be created if metadata broadcast fired already                                  PartitionQueueInfo queue;                                      if (!_allPartitionQueues.TryGetValue(batch.Key' out queue))                                      {                                          queue = new PartitionQueueInfo(Configuration.SendBuffersInitialSize) { Partition = batch.Key };                                          _allPartitionQueues.Add(batch.Key' queue);                                          queue.IsOnline = topicPartitions.First(p => p.Id == batch.Key).ErrorCode.IsSuccess();                                          _log.Debug("{0} added new partition queue"' this);                                          _queueSizeEvents.OnNext(new QueueResizeInfo { PartitionId = queue.Partition' Size = queue.Queue.Size' Capacity = queue.Queue.Capacity });                                      }                                    // now queue them up                                  var batchAr = batch.ToArray();                                    // make sure we have space.                                  if (queue.Queue.Size + batchAr.Length > queue.Queue.Capacity)                                      {                                      // try to increase the capacity.                                      if (Configuration.AutoGrowSendBuffers)                                          {                                              var growBy = Math.Max(queue.Queue.Capacity / 2 + 1' 2 * batchAr.Length);                                              _log.Warn("Capacity of send buffer with size {3} not large enough to accept {2} new messages. Increasing capacity from {0} to {1}"' queue.Queue.Capacity' queue.Queue.Capacity + growBy' batchAr.Length' queue.Queue.Size);                                                queue.Queue.Capacity += growBy;                                              _queueSizeEvents.OnNext(new QueueResizeInfo { PartitionId = queue.Partition' Size = queue.Queue.Size' Capacity = queue.Queue.Capacity });                                          }                                          else                                          {                                          // we're full and not allowed to grow. Throw the batch back to the caller' and continue on.                                          var msg = string.Format("Send Buffer Full for partition {0}. "'                                                          Cluster.PartitionStateChanges.Where(                                                              ps => ps.Topic == Configuration.Topic && ps.PartitionId == queue.Partition)                                                              .Take(1)                                                              .Wait());                                              _log.Error(msg);                                              if (OnPermError != null)                                                  Task.Factory.StartNew(() => OnPermError(new Exception(msg)' batchAr));                                              continue;                                          }                                      }                                    // we have the space' add to the queue                                  queue.Queue.Put(batchAr);                                        if (_log.IsDebugEnabled)                                          _log.Debug("Enqueued batch of size {0} for topic '{1}' partition {2}"'                                              batchAr.Length' Configuration.Topic' batch.Key);                                    // After batch enqueued' send wake up signal to sending queue                                  _queueEventWaitHandler.Set();                                  }                              }' e => _log.Fatal(e' "Error in _sendMessagesSubject pipeline")'                                  () => _log.Debug("_sendMessagesSubject complete")                              );                        // start the send loop task                      _cluster.Scheduler.Schedule(() => {                              _sendLoopTask = SendLoop().                                  ContinueWith(t =>                                  {                                      if (t.IsFaulted)                                          _log.Fatal(t.Exception' "SendLoop failed");                                      else                                          _log.Debug("SendLoop complete with status: {0}"' t.Status);                                  });                          });                          _log.Debug("Connected");                          EtwTrace.Log.ProducerStarted(Topic' _id);                      }                      catch (Exception e)                      {                          _log.Error(e' "Exception during connect");                          EtwTrace.Log.ProducerError(e.Message' _id);                          throw;                      }                      finally                      {                          _log.Debug("#{0} Releasing Producer Semaphore."' _id);                          _sync.Release();                      }                  }).ConfigureAwait(false);
Magic Number,kafka4net,Producer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The following statement contains a magic number: var sendTasks = queuesToBeSent.                          Select(q => new { partsMeta.First(m => m.Id == q.Partition).Leader' Queue = q }).                          // leader'queue -> leader'queue[]                          GroupBy(q => q.Leader' (i' queues) => new { Leader = i' Queues = queues.Select(q1 => q1.Queue).ToArray() }).                          Select(queues => new { queues.Leader' queues.Queues }).                          Select(async brokerBatch =>                          {                              try                              {                                  //                                  // Walk queuese in zig-zag manner from the tail towards the head until we are withing size limit                                  // The point of zig-zag is fairness: if producing is slow' we do not want to keep sending only first                                   // partition in the queue while starving the last.                                  //                                  var maxSize = Configuration.MaxMessageSetSizeInBytes;                                  var runningSize = 4; // array of messages len                                  const int messageFixedSize =                                          8 + // offset                                          4 + // message size                                           4 + // crc                                          1 + // magic                                          1 + // attributes                                          4 + // size of key array                                          4;  // size of value array                                    brokerBatch.Queues.ForEach(q => q.CountInProgress = 0);                                  var queueCount = brokerBatch.Queues.Length;                                  int finishedQueues=0;                                  for (int i = 0; ; i=(i + 1) % queueCount)                                  {                                      // TODO: prevent enqueing of message of too lage size' which exceeds the limit.                                       // Do it in syncronous part' before msg rich the queue                                      if (i == 0)                                          finishedQueues = 0;                                        var queue = brokerBatch.Queues[i];                                                                            // whole queue is already taken                                      if (queue.Queue.Size == queue.CountInProgress)                                      {                                          finishedQueues++;                                          if (finishedQueues == queueCount)                                              break;                                          continue;                                      }                                        var msg = queue.Queue.PeekSingle(queue.CountInProgress);                                      var keyLen = msg.Key != null ? msg.Key.Length : 0;                                      var msgLen = msg.Value != null ? msg.Value.Length : 0;                                      var msgSize = messageFixedSize + keyLen + msgLen;                                      if (runningSize + msgSize > maxSize)                                          break;                                                                            runningSize += msgSize;                                      queue.CountInProgress++;                                  }                                    var messages = brokerBatch.Queues.SelectMany(q => q.Queue.PeekEnum(q.CountInProgress));                                    // TODO: freeze permanent errors and throw consumer exceptions upon sending to perm error partition                                  var response = await _cluster.SendBatchAsync(brokerBatch.Leader' messages' this);                                    var failedResponsePartitions = response.Topics.                                      Where(t => t.TopicName == Configuration.Topic). // should contain response only for our topic' but just in case...                                      SelectMany(t => t.Partitions).                                      Where(p => !p.ErrorCode.IsSuccess()).ToArray();                                    // some errors' figure out which batches to dismiss from queues                                  var failedPartitionIds = new HashSet<int>(failedResponsePartitions.Select(p => p.Partition));                                    var successPartitionQueues = brokerBatch.Queues.                                      Where(q => !failedPartitionIds.Contains(q.Partition)).                                      ToArray();                                    var permanentErrorPartitions = failedResponsePartitions.                                      Where(p => p.ErrorCode.IsPermanentFailure()).ToArray();                                    var recoverableErrorPartitions = failedResponsePartitions.                                      Except(permanentErrorPartitions).ToArray();                                    var permanentErrorPartitionIds = new HashSet<int>(permanentErrorPartitions.Select(p=>p.Partition));                                    // notify of errors from send response and trigger recovery monitor tracking them                                  recoverableErrorPartitions.                                      ForEach(failedPart =>                                      {                                          _log.Warn("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode);                                          _cluster.NotifyPartitionStateChange(new PartitionStateChangeEvent(Topic'                                              failedPart.Partition' failedPart.ErrorCode));                                      });                                    if (permanentErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled())                                  {                                      EtwTrace.Log.ProducerPermanentFailure(_id' permanentErrorPartitions.Length);                                      string error = string.Join("' "' permanentErrorPartitions.Select(p => string.Format("{0}:{1}"' p.Partition' p.ErrorCode)));                                      error = string.Format("[{0}]"' error);                                      EtwTrace.Log.ProducerPermanentFailureDetails(_id' error);                                  }                                    if (recoverableErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled())                                  {                                      EtwTrace.Log.ProducerRecoverableErrors(_id' recoverableErrorPartitions.Length);                                  }                                    // Pop permanently failed messages from the queue                                  var permanentFailedMessages = brokerBatch.Queues.                                      Where(q => permanentErrorPartitionIds.Contains(q.Partition)).                                      SelectMany(q => q.Queue.GetEnum(q.CountInProgress)).ToArray();                                  // fire permanent error                                  if (OnPermError != null && permanentFailedMessages.Length > 0)                                  {                                      var msg = string.Join("'"' permanentErrorPartitions.Select(p => p.ErrorCode).Distinct());                                      msg = string.Format("Produce request failed with errors: [{0}]"' msg);                                      await Task.Factory.StartNew(() => OnPermError(new BrokerException(msg)' permanentFailedMessages));                                  }                                    // Do nothing with recoverable errors' they will be sent again next time                                                                    var successMessages = successPartitionQueues.SelectMany(q => q.Queue.Get(q.CountInProgress)).ToArray();                                    if (OnSuccess != null && successMessages.Length != 0)                                      await Task.Factory.StartNew(() => OnSuccess(successMessages));                              }                              catch (ThreadAbortException e)                              {                                  // It happen when Watchdog kills stuck thread. We want stack trace to be logged with high priority                                  _log.Fatal(e' "Send has been aborted. Probably hung thread detected");                              }                              catch (Exception e)                              {                                  _log.Debug(e' "Exception while sending batch to topic '{0}' BrokerId {1}"' Configuration.Topic' brokerBatch.Leader);                                  //brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError)));                              }                          });
Magic Number,kafka4net,Producer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The following statement contains a magic number: var sendTasks = queuesToBeSent.                          Select(q => new { partsMeta.First(m => m.Id == q.Partition).Leader' Queue = q }).                          // leader'queue -> leader'queue[]                          GroupBy(q => q.Leader' (i' queues) => new { Leader = i' Queues = queues.Select(q1 => q1.Queue).ToArray() }).                          Select(queues => new { queues.Leader' queues.Queues }).                          Select(async brokerBatch =>                          {                              try                              {                                  //                                  // Walk queuese in zig-zag manner from the tail towards the head until we are withing size limit                                  // The point of zig-zag is fairness: if producing is slow' we do not want to keep sending only first                                   // partition in the queue while starving the last.                                  //                                  var maxSize = Configuration.MaxMessageSetSizeInBytes;                                  var runningSize = 4; // array of messages len                                  const int messageFixedSize =                                          8 + // offset                                          4 + // message size                                           4 + // crc                                          1 + // magic                                          1 + // attributes                                          4 + // size of key array                                          4;  // size of value array                                    brokerBatch.Queues.ForEach(q => q.CountInProgress = 0);                                  var queueCount = brokerBatch.Queues.Length;                                  int finishedQueues=0;                                  for (int i = 0; ; i=(i + 1) % queueCount)                                  {                                      // TODO: prevent enqueing of message of too lage size' which exceeds the limit.                                       // Do it in syncronous part' before msg rich the queue                                      if (i == 0)                                          finishedQueues = 0;                                        var queue = brokerBatch.Queues[i];                                                                            // whole queue is already taken                                      if (queue.Queue.Size == queue.CountInProgress)                                      {                                          finishedQueues++;                                          if (finishedQueues == queueCount)                                              break;                                          continue;                                      }                                        var msg = queue.Queue.PeekSingle(queue.CountInProgress);                                      var keyLen = msg.Key != null ? msg.Key.Length : 0;                                      var msgLen = msg.Value != null ? msg.Value.Length : 0;                                      var msgSize = messageFixedSize + keyLen + msgLen;                                      if (runningSize + msgSize > maxSize)                                          break;                                                                            runningSize += msgSize;                                      queue.CountInProgress++;                                  }                                    var messages = brokerBatch.Queues.SelectMany(q => q.Queue.PeekEnum(q.CountInProgress));                                    // TODO: freeze permanent errors and throw consumer exceptions upon sending to perm error partition                                  var response = await _cluster.SendBatchAsync(brokerBatch.Leader' messages' this);                                    var failedResponsePartitions = response.Topics.                                      Where(t => t.TopicName == Configuration.Topic). // should contain response only for our topic' but just in case...                                      SelectMany(t => t.Partitions).                                      Where(p => !p.ErrorCode.IsSuccess()).ToArray();                                    // some errors' figure out which batches to dismiss from queues                                  var failedPartitionIds = new HashSet<int>(failedResponsePartitions.Select(p => p.Partition));                                    var successPartitionQueues = brokerBatch.Queues.                                      Where(q => !failedPartitionIds.Contains(q.Partition)).                                      ToArray();                                    var permanentErrorPartitions = failedResponsePartitions.                                      Where(p => p.ErrorCode.IsPermanentFailure()).ToArray();                                    var recoverableErrorPartitions = failedResponsePartitions.                                      Except(permanentErrorPartitions).ToArray();                                    var permanentErrorPartitionIds = new HashSet<int>(permanentErrorPartitions.Select(p=>p.Partition));                                    // notify of errors from send response and trigger recovery monitor tracking them                                  recoverableErrorPartitions.                                      ForEach(failedPart =>                                      {                                          _log.Warn("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode);                                          _cluster.NotifyPartitionStateChange(new PartitionStateChangeEvent(Topic'                                              failedPart.Partition' failedPart.ErrorCode));                                      });                                    if (permanentErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled())                                  {                                      EtwTrace.Log.ProducerPermanentFailure(_id' permanentErrorPartitions.Length);                                      string error = string.Join("' "' permanentErrorPartitions.Select(p => string.Format("{0}:{1}"' p.Partition' p.ErrorCode)));                                      error = string.Format("[{0}]"' error);                                      EtwTrace.Log.ProducerPermanentFailureDetails(_id' error);                                  }                                    if (recoverableErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled())                                  {                                      EtwTrace.Log.ProducerRecoverableErrors(_id' recoverableErrorPartitions.Length);                                  }                                    // Pop permanently failed messages from the queue                                  var permanentFailedMessages = brokerBatch.Queues.                                      Where(q => permanentErrorPartitionIds.Contains(q.Partition)).                                      SelectMany(q => q.Queue.GetEnum(q.CountInProgress)).ToArray();                                  // fire permanent error                                  if (OnPermError != null && permanentFailedMessages.Length > 0)                                  {                                      var msg = string.Join("'"' permanentErrorPartitions.Select(p => p.ErrorCode).Distinct());                                      msg = string.Format("Produce request failed with errors: [{0}]"' msg);                                      await Task.Factory.StartNew(() => OnPermError(new BrokerException(msg)' permanentFailedMessages));                                  }                                    // Do nothing with recoverable errors' they will be sent again next time                                                                    var successMessages = successPartitionQueues.SelectMany(q => q.Queue.Get(q.CountInProgress)).ToArray();                                    if (OnSuccess != null && successMessages.Length != 0)                                      await Task.Factory.StartNew(() => OnSuccess(successMessages));                              }                              catch (ThreadAbortException e)                              {                                  // It happen when Watchdog kills stuck thread. We want stack trace to be logged with high priority                                  _log.Fatal(e' "Send has been aborted. Probably hung thread detected");                              }                              catch (Exception e)                              {                                  _log.Debug(e' "Exception while sending batch to topic '{0}' BrokerId {1}"' Configuration.Topic' brokerBatch.Leader);                                  //brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError)));                              }                          });
Magic Number,kafka4net,Producer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The following statement contains a magic number: var sendTasks = queuesToBeSent.                          Select(q => new { partsMeta.First(m => m.Id == q.Partition).Leader' Queue = q }).                          // leader'queue -> leader'queue[]                          GroupBy(q => q.Leader' (i' queues) => new { Leader = i' Queues = queues.Select(q1 => q1.Queue).ToArray() }).                          Select(queues => new { queues.Leader' queues.Queues }).                          Select(async brokerBatch =>                          {                              try                              {                                  //                                  // Walk queuese in zig-zag manner from the tail towards the head until we are withing size limit                                  // The point of zig-zag is fairness: if producing is slow' we do not want to keep sending only first                                   // partition in the queue while starving the last.                                  //                                  var maxSize = Configuration.MaxMessageSetSizeInBytes;                                  var runningSize = 4; // array of messages len                                  const int messageFixedSize =                                          8 + // offset                                          4 + // message size                                           4 + // crc                                          1 + // magic                                          1 + // attributes                                          4 + // size of key array                                          4;  // size of value array                                    brokerBatch.Queues.ForEach(q => q.CountInProgress = 0);                                  var queueCount = brokerBatch.Queues.Length;                                  int finishedQueues=0;                                  for (int i = 0; ; i=(i + 1) % queueCount)                                  {                                      // TODO: prevent enqueing of message of too lage size' which exceeds the limit.                                       // Do it in syncronous part' before msg rich the queue                                      if (i == 0)                                          finishedQueues = 0;                                        var queue = brokerBatch.Queues[i];                                                                            // whole queue is already taken                                      if (queue.Queue.Size == queue.CountInProgress)                                      {                                          finishedQueues++;                                          if (finishedQueues == queueCount)                                              break;                                          continue;                                      }                                        var msg = queue.Queue.PeekSingle(queue.CountInProgress);                                      var keyLen = msg.Key != null ? msg.Key.Length : 0;                                      var msgLen = msg.Value != null ? msg.Value.Length : 0;                                      var msgSize = messageFixedSize + keyLen + msgLen;                                      if (runningSize + msgSize > maxSize)                                          break;                                                                            runningSize += msgSize;                                      queue.CountInProgress++;                                  }                                    var messages = brokerBatch.Queues.SelectMany(q => q.Queue.PeekEnum(q.CountInProgress));                                    // TODO: freeze permanent errors and throw consumer exceptions upon sending to perm error partition                                  var response = await _cluster.SendBatchAsync(brokerBatch.Leader' messages' this);                                    var failedResponsePartitions = response.Topics.                                      Where(t => t.TopicName == Configuration.Topic). // should contain response only for our topic' but just in case...                                      SelectMany(t => t.Partitions).                                      Where(p => !p.ErrorCode.IsSuccess()).ToArray();                                    // some errors' figure out which batches to dismiss from queues                                  var failedPartitionIds = new HashSet<int>(failedResponsePartitions.Select(p => p.Partition));                                    var successPartitionQueues = brokerBatch.Queues.                                      Where(q => !failedPartitionIds.Contains(q.Partition)).                                      ToArray();                                    var permanentErrorPartitions = failedResponsePartitions.                                      Where(p => p.ErrorCode.IsPermanentFailure()).ToArray();                                    var recoverableErrorPartitions = failedResponsePartitions.                                      Except(permanentErrorPartitions).ToArray();                                    var permanentErrorPartitionIds = new HashSet<int>(permanentErrorPartitions.Select(p=>p.Partition));                                    // notify of errors from send response and trigger recovery monitor tracking them                                  recoverableErrorPartitions.                                      ForEach(failedPart =>                                      {                                          _log.Warn("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode);                                          _cluster.NotifyPartitionStateChange(new PartitionStateChangeEvent(Topic'                                              failedPart.Partition' failedPart.ErrorCode));                                      });                                    if (permanentErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled())                                  {                                      EtwTrace.Log.ProducerPermanentFailure(_id' permanentErrorPartitions.Length);                                      string error = string.Join("' "' permanentErrorPartitions.Select(p => string.Format("{0}:{1}"' p.Partition' p.ErrorCode)));                                      error = string.Format("[{0}]"' error);                                      EtwTrace.Log.ProducerPermanentFailureDetails(_id' error);                                  }                                    if (recoverableErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled())                                  {                                      EtwTrace.Log.ProducerRecoverableErrors(_id' recoverableErrorPartitions.Length);                                  }                                    // Pop permanently failed messages from the queue                                  var permanentFailedMessages = brokerBatch.Queues.                                      Where(q => permanentErrorPartitionIds.Contains(q.Partition)).                                      SelectMany(q => q.Queue.GetEnum(q.CountInProgress)).ToArray();                                  // fire permanent error                                  if (OnPermError != null && permanentFailedMessages.Length > 0)                                  {                                      var msg = string.Join("'"' permanentErrorPartitions.Select(p => p.ErrorCode).Distinct());                                      msg = string.Format("Produce request failed with errors: [{0}]"' msg);                                      await Task.Factory.StartNew(() => OnPermError(new BrokerException(msg)' permanentFailedMessages));                                  }                                    // Do nothing with recoverable errors' they will be sent again next time                                                                    var successMessages = successPartitionQueues.SelectMany(q => q.Queue.Get(q.CountInProgress)).ToArray();                                    if (OnSuccess != null && successMessages.Length != 0)                                      await Task.Factory.StartNew(() => OnSuccess(successMessages));                              }                              catch (ThreadAbortException e)                              {                                  // It happen when Watchdog kills stuck thread. We want stack trace to be logged with high priority                                  _log.Fatal(e' "Send has been aborted. Probably hung thread detected");                              }                              catch (Exception e)                              {                                  _log.Debug(e' "Exception while sending batch to topic '{0}' BrokerId {1}"' Configuration.Topic' brokerBatch.Leader);                                  //brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError)));                              }                          });
Magic Number,kafka4net,Producer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The following statement contains a magic number: var sendTasks = queuesToBeSent.                          Select(q => new { partsMeta.First(m => m.Id == q.Partition).Leader' Queue = q }).                          // leader'queue -> leader'queue[]                          GroupBy(q => q.Leader' (i' queues) => new { Leader = i' Queues = queues.Select(q1 => q1.Queue).ToArray() }).                          Select(queues => new { queues.Leader' queues.Queues }).                          Select(async brokerBatch =>                          {                              try                              {                                  //                                  // Walk queuese in zig-zag manner from the tail towards the head until we are withing size limit                                  // The point of zig-zag is fairness: if producing is slow' we do not want to keep sending only first                                   // partition in the queue while starving the last.                                  //                                  var maxSize = Configuration.MaxMessageSetSizeInBytes;                                  var runningSize = 4; // array of messages len                                  const int messageFixedSize =                                          8 + // offset                                          4 + // message size                                           4 + // crc                                          1 + // magic                                          1 + // attributes                                          4 + // size of key array                                          4;  // size of value array                                    brokerBatch.Queues.ForEach(q => q.CountInProgress = 0);                                  var queueCount = brokerBatch.Queues.Length;                                  int finishedQueues=0;                                  for (int i = 0; ; i=(i + 1) % queueCount)                                  {                                      // TODO: prevent enqueing of message of too lage size' which exceeds the limit.                                       // Do it in syncronous part' before msg rich the queue                                      if (i == 0)                                          finishedQueues = 0;                                        var queue = brokerBatch.Queues[i];                                                                            // whole queue is already taken                                      if (queue.Queue.Size == queue.CountInProgress)                                      {                                          finishedQueues++;                                          if (finishedQueues == queueCount)                                              break;                                          continue;                                      }                                        var msg = queue.Queue.PeekSingle(queue.CountInProgress);                                      var keyLen = msg.Key != null ? msg.Key.Length : 0;                                      var msgLen = msg.Value != null ? msg.Value.Length : 0;                                      var msgSize = messageFixedSize + keyLen + msgLen;                                      if (runningSize + msgSize > maxSize)                                          break;                                                                            runningSize += msgSize;                                      queue.CountInProgress++;                                  }                                    var messages = brokerBatch.Queues.SelectMany(q => q.Queue.PeekEnum(q.CountInProgress));                                    // TODO: freeze permanent errors and throw consumer exceptions upon sending to perm error partition                                  var response = await _cluster.SendBatchAsync(brokerBatch.Leader' messages' this);                                    var failedResponsePartitions = response.Topics.                                      Where(t => t.TopicName == Configuration.Topic). // should contain response only for our topic' but just in case...                                      SelectMany(t => t.Partitions).                                      Where(p => !p.ErrorCode.IsSuccess()).ToArray();                                    // some errors' figure out which batches to dismiss from queues                                  var failedPartitionIds = new HashSet<int>(failedResponsePartitions.Select(p => p.Partition));                                    var successPartitionQueues = brokerBatch.Queues.                                      Where(q => !failedPartitionIds.Contains(q.Partition)).                                      ToArray();                                    var permanentErrorPartitions = failedResponsePartitions.                                      Where(p => p.ErrorCode.IsPermanentFailure()).ToArray();                                    var recoverableErrorPartitions = failedResponsePartitions.                                      Except(permanentErrorPartitions).ToArray();                                    var permanentErrorPartitionIds = new HashSet<int>(permanentErrorPartitions.Select(p=>p.Partition));                                    // notify of errors from send response and trigger recovery monitor tracking them                                  recoverableErrorPartitions.                                      ForEach(failedPart =>                                      {                                          _log.Warn("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode);                                          _cluster.NotifyPartitionStateChange(new PartitionStateChangeEvent(Topic'                                              failedPart.Partition' failedPart.ErrorCode));                                      });                                    if (permanentErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled())                                  {                                      EtwTrace.Log.ProducerPermanentFailure(_id' permanentErrorPartitions.Length);                                      string error = string.Join("' "' permanentErrorPartitions.Select(p => string.Format("{0}:{1}"' p.Partition' p.ErrorCode)));                                      error = string.Format("[{0}]"' error);                                      EtwTrace.Log.ProducerPermanentFailureDetails(_id' error);                                  }                                    if (recoverableErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled())                                  {                                      EtwTrace.Log.ProducerRecoverableErrors(_id' recoverableErrorPartitions.Length);                                  }                                    // Pop permanently failed messages from the queue                                  var permanentFailedMessages = brokerBatch.Queues.                                      Where(q => permanentErrorPartitionIds.Contains(q.Partition)).                                      SelectMany(q => q.Queue.GetEnum(q.CountInProgress)).ToArray();                                  // fire permanent error                                  if (OnPermError != null && permanentFailedMessages.Length > 0)                                  {                                      var msg = string.Join("'"' permanentErrorPartitions.Select(p => p.ErrorCode).Distinct());                                      msg = string.Format("Produce request failed with errors: [{0}]"' msg);                                      await Task.Factory.StartNew(() => OnPermError(new BrokerException(msg)' permanentFailedMessages));                                  }                                    // Do nothing with recoverable errors' they will be sent again next time                                                                    var successMessages = successPartitionQueues.SelectMany(q => q.Queue.Get(q.CountInProgress)).ToArray();                                    if (OnSuccess != null && successMessages.Length != 0)                                      await Task.Factory.StartNew(() => OnSuccess(successMessages));                              }                              catch (ThreadAbortException e)                              {                                  // It happen when Watchdog kills stuck thread. We want stack trace to be logged with high priority                                  _log.Fatal(e' "Send has been aborted. Probably hung thread detected");                              }                              catch (Exception e)                              {                                  _log.Debug(e' "Exception while sending batch to topic '{0}' BrokerId {1}"' Configuration.Topic' brokerBatch.Leader);                                  //brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError)));                              }                          });
Magic Number,kafka4net,Producer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The following statement contains a magic number: var sendTasks = queuesToBeSent.                          Select(q => new { partsMeta.First(m => m.Id == q.Partition).Leader' Queue = q }).                          // leader'queue -> leader'queue[]                          GroupBy(q => q.Leader' (i' queues) => new { Leader = i' Queues = queues.Select(q1 => q1.Queue).ToArray() }).                          Select(queues => new { queues.Leader' queues.Queues }).                          Select(async brokerBatch =>                          {                              try                              {                                  //                                  // Walk queuese in zig-zag manner from the tail towards the head until we are withing size limit                                  // The point of zig-zag is fairness: if producing is slow' we do not want to keep sending only first                                   // partition in the queue while starving the last.                                  //                                  var maxSize = Configuration.MaxMessageSetSizeInBytes;                                  var runningSize = 4; // array of messages len                                  const int messageFixedSize =                                          8 + // offset                                          4 + // message size                                           4 + // crc                                          1 + // magic                                          1 + // attributes                                          4 + // size of key array                                          4;  // size of value array                                    brokerBatch.Queues.ForEach(q => q.CountInProgress = 0);                                  var queueCount = brokerBatch.Queues.Length;                                  int finishedQueues=0;                                  for (int i = 0; ; i=(i + 1) % queueCount)                                  {                                      // TODO: prevent enqueing of message of too lage size' which exceeds the limit.                                       // Do it in syncronous part' before msg rich the queue                                      if (i == 0)                                          finishedQueues = 0;                                        var queue = brokerBatch.Queues[i];                                                                            // whole queue is already taken                                      if (queue.Queue.Size == queue.CountInProgress)                                      {                                          finishedQueues++;                                          if (finishedQueues == queueCount)                                              break;                                          continue;                                      }                                        var msg = queue.Queue.PeekSingle(queue.CountInProgress);                                      var keyLen = msg.Key != null ? msg.Key.Length : 0;                                      var msgLen = msg.Value != null ? msg.Value.Length : 0;                                      var msgSize = messageFixedSize + keyLen + msgLen;                                      if (runningSize + msgSize > maxSize)                                          break;                                                                            runningSize += msgSize;                                      queue.CountInProgress++;                                  }                                    var messages = brokerBatch.Queues.SelectMany(q => q.Queue.PeekEnum(q.CountInProgress));                                    // TODO: freeze permanent errors and throw consumer exceptions upon sending to perm error partition                                  var response = await _cluster.SendBatchAsync(brokerBatch.Leader' messages' this);                                    var failedResponsePartitions = response.Topics.                                      Where(t => t.TopicName == Configuration.Topic). // should contain response only for our topic' but just in case...                                      SelectMany(t => t.Partitions).                                      Where(p => !p.ErrorCode.IsSuccess()).ToArray();                                    // some errors' figure out which batches to dismiss from queues                                  var failedPartitionIds = new HashSet<int>(failedResponsePartitions.Select(p => p.Partition));                                    var successPartitionQueues = brokerBatch.Queues.                                      Where(q => !failedPartitionIds.Contains(q.Partition)).                                      ToArray();                                    var permanentErrorPartitions = failedResponsePartitions.                                      Where(p => p.ErrorCode.IsPermanentFailure()).ToArray();                                    var recoverableErrorPartitions = failedResponsePartitions.                                      Except(permanentErrorPartitions).ToArray();                                    var permanentErrorPartitionIds = new HashSet<int>(permanentErrorPartitions.Select(p=>p.Partition));                                    // notify of errors from send response and trigger recovery monitor tracking them                                  recoverableErrorPartitions.                                      ForEach(failedPart =>                                      {                                          _log.Warn("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode);                                          _cluster.NotifyPartitionStateChange(new PartitionStateChangeEvent(Topic'                                              failedPart.Partition' failedPart.ErrorCode));                                      });                                    if (permanentErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled())                                  {                                      EtwTrace.Log.ProducerPermanentFailure(_id' permanentErrorPartitions.Length);                                      string error = string.Join("' "' permanentErrorPartitions.Select(p => string.Format("{0}:{1}"' p.Partition' p.ErrorCode)));                                      error = string.Format("[{0}]"' error);                                      EtwTrace.Log.ProducerPermanentFailureDetails(_id' error);                                  }                                    if (recoverableErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled())                                  {                                      EtwTrace.Log.ProducerRecoverableErrors(_id' recoverableErrorPartitions.Length);                                  }                                    // Pop permanently failed messages from the queue                                  var permanentFailedMessages = brokerBatch.Queues.                                      Where(q => permanentErrorPartitionIds.Contains(q.Partition)).                                      SelectMany(q => q.Queue.GetEnum(q.CountInProgress)).ToArray();                                  // fire permanent error                                  if (OnPermError != null && permanentFailedMessages.Length > 0)                                  {                                      var msg = string.Join("'"' permanentErrorPartitions.Select(p => p.ErrorCode).Distinct());                                      msg = string.Format("Produce request failed with errors: [{0}]"' msg);                                      await Task.Factory.StartNew(() => OnPermError(new BrokerException(msg)' permanentFailedMessages));                                  }                                    // Do nothing with recoverable errors' they will be sent again next time                                                                    var successMessages = successPartitionQueues.SelectMany(q => q.Queue.Get(q.CountInProgress)).ToArray();                                    if (OnSuccess != null && successMessages.Length != 0)                                      await Task.Factory.StartNew(() => OnSuccess(successMessages));                              }                              catch (ThreadAbortException e)                              {                                  // It happen when Watchdog kills stuck thread. We want stack trace to be logged with high priority                                  _log.Fatal(e' "Send has been aborted. Probably hung thread detected");                              }                              catch (Exception e)                              {                                  _log.Debug(e' "Exception while sending batch to topic '{0}' BrokerId {1}"' Configuration.Topic' brokerBatch.Leader);                                  //brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError)));                              }                          });
Magic Number,kafka4net,Producer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The following statement contains a magic number: var sendTasks = queuesToBeSent.                          Select(q => new { partsMeta.First(m => m.Id == q.Partition).Leader' Queue = q }).                          // leader'queue -> leader'queue[]                          GroupBy(q => q.Leader' (i' queues) => new { Leader = i' Queues = queues.Select(q1 => q1.Queue).ToArray() }).                          Select(queues => new { queues.Leader' queues.Queues }).                          Select(async brokerBatch =>                          {                              try                              {                                  //                                  // Walk queuese in zig-zag manner from the tail towards the head until we are withing size limit                                  // The point of zig-zag is fairness: if producing is slow' we do not want to keep sending only first                                   // partition in the queue while starving the last.                                  //                                  var maxSize = Configuration.MaxMessageSetSizeInBytes;                                  var runningSize = 4; // array of messages len                                  const int messageFixedSize =                                          8 + // offset                                          4 + // message size                                           4 + // crc                                          1 + // magic                                          1 + // attributes                                          4 + // size of key array                                          4;  // size of value array                                    brokerBatch.Queues.ForEach(q => q.CountInProgress = 0);                                  var queueCount = brokerBatch.Queues.Length;                                  int finishedQueues=0;                                  for (int i = 0; ; i=(i + 1) % queueCount)                                  {                                      // TODO: prevent enqueing of message of too lage size' which exceeds the limit.                                       // Do it in syncronous part' before msg rich the queue                                      if (i == 0)                                          finishedQueues = 0;                                        var queue = brokerBatch.Queues[i];                                                                            // whole queue is already taken                                      if (queue.Queue.Size == queue.CountInProgress)                                      {                                          finishedQueues++;                                          if (finishedQueues == queueCount)                                              break;                                          continue;                                      }                                        var msg = queue.Queue.PeekSingle(queue.CountInProgress);                                      var keyLen = msg.Key != null ? msg.Key.Length : 0;                                      var msgLen = msg.Value != null ? msg.Value.Length : 0;                                      var msgSize = messageFixedSize + keyLen + msgLen;                                      if (runningSize + msgSize > maxSize)                                          break;                                                                            runningSize += msgSize;                                      queue.CountInProgress++;                                  }                                    var messages = brokerBatch.Queues.SelectMany(q => q.Queue.PeekEnum(q.CountInProgress));                                    // TODO: freeze permanent errors and throw consumer exceptions upon sending to perm error partition                                  var response = await _cluster.SendBatchAsync(brokerBatch.Leader' messages' this);                                    var failedResponsePartitions = response.Topics.                                      Where(t => t.TopicName == Configuration.Topic). // should contain response only for our topic' but just in case...                                      SelectMany(t => t.Partitions).                                      Where(p => !p.ErrorCode.IsSuccess()).ToArray();                                    // some errors' figure out which batches to dismiss from queues                                  var failedPartitionIds = new HashSet<int>(failedResponsePartitions.Select(p => p.Partition));                                    var successPartitionQueues = brokerBatch.Queues.                                      Where(q => !failedPartitionIds.Contains(q.Partition)).                                      ToArray();                                    var permanentErrorPartitions = failedResponsePartitions.                                      Where(p => p.ErrorCode.IsPermanentFailure()).ToArray();                                    var recoverableErrorPartitions = failedResponsePartitions.                                      Except(permanentErrorPartitions).ToArray();                                    var permanentErrorPartitionIds = new HashSet<int>(permanentErrorPartitions.Select(p=>p.Partition));                                    // notify of errors from send response and trigger recovery monitor tracking them                                  recoverableErrorPartitions.                                      ForEach(failedPart =>                                      {                                          _log.Warn("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode);                                          _cluster.NotifyPartitionStateChange(new PartitionStateChangeEvent(Topic'                                              failedPart.Partition' failedPart.ErrorCode));                                      });                                    if (permanentErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled())                                  {                                      EtwTrace.Log.ProducerPermanentFailure(_id' permanentErrorPartitions.Length);                                      string error = string.Join("' "' permanentErrorPartitions.Select(p => string.Format("{0}:{1}"' p.Partition' p.ErrorCode)));                                      error = string.Format("[{0}]"' error);                                      EtwTrace.Log.ProducerPermanentFailureDetails(_id' error);                                  }                                    if (recoverableErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled())                                  {                                      EtwTrace.Log.ProducerRecoverableErrors(_id' recoverableErrorPartitions.Length);                                  }                                    // Pop permanently failed messages from the queue                                  var permanentFailedMessages = brokerBatch.Queues.                                      Where(q => permanentErrorPartitionIds.Contains(q.Partition)).                                      SelectMany(q => q.Queue.GetEnum(q.CountInProgress)).ToArray();                                  // fire permanent error                                  if (OnPermError != null && permanentFailedMessages.Length > 0)                                  {                                      var msg = string.Join("'"' permanentErrorPartitions.Select(p => p.ErrorCode).Distinct());                                      msg = string.Format("Produce request failed with errors: [{0}]"' msg);                                      await Task.Factory.StartNew(() => OnPermError(new BrokerException(msg)' permanentFailedMessages));                                  }                                    // Do nothing with recoverable errors' they will be sent again next time                                                                    var successMessages = successPartitionQueues.SelectMany(q => q.Queue.Get(q.CountInProgress)).ToArray();                                    if (OnSuccess != null && successMessages.Length != 0)                                      await Task.Factory.StartNew(() => OnSuccess(successMessages));                              }                              catch (ThreadAbortException e)                              {                                  // It happen when Watchdog kills stuck thread. We want stack trace to be logged with high priority                                  _log.Fatal(e' "Send has been aborted. Probably hung thread detected");                              }                              catch (Exception e)                              {                                  _log.Debug(e' "Exception while sending batch to topic '{0}' BrokerId {1}"' Configuration.Topic' brokerBatch.Leader);                                  //brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError)));                              }                          });
Magic Number,kafka4net,Cluster,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Cluster.cs,ConnectAsync,The following statement contains a magic number: await Scheduler.Ask(() =>              {                  // we cannot reconnect if we have closed already.                  if (_state == ClusterState.Closed)                      throw new BrokerException("Cluster is already closed. Cannot reconnect. Please create a new Cluster.");                    if (_state != ClusterState.Disconnected)                      return false;                    _log.Debug("Connecting");                    var initBrokers = Connection.ParseAddress(_seedBrokers).                      Select(seed => new BrokerMeta                       {                          Host = seed.Item1'                          Port = seed.Item2'                          NodeId = -99                      }).ToArray();                  EtwTrace.Log.ClusterStarting(_id);                    var initMeta = new MetadataResponse { Topics = new TopicMeta[0]' Brokers = initBrokers };                    MergeTopicMeta(initMeta);                  _state = ClusterState.Connected;                    // start up a recovery monitor to watch for recovered partitions                  _partitionRecoveryMonitor = new PartitionRecoveryMonitor(this' _protocol' _cancel.Token);                  // Merge metadata that recovery monitor discovers                  _partitionRecoveryMonitor.NewMetadataEvents.Subscribe(MergeTopicMeta' ex => _log.Error(ex' "Error thrown by RecoveryMonitor.NewMetadataEvents!"));                  _log.Debug("Connected");                  EtwTrace.Log.ClusterStarted(_id);                  return true;              }).ConfigureAwait(false);
Magic Number,kafka4net,Cluster,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Cluster.cs,FetchPartitionOffsetsImplAsync,The following statement contains a magic number: var ret = await await Scheduler.Ask(async () => {                  var numAttempts = -1;                  while (!_cancel.IsCancellationRequested)                  {                      numAttempts++;                      bool retry;                      Exception exception;                      try                      {                          // get partition list                          var parts = (await GetOrFetchMetaForTopicAsync(topic)).                              Select(p => new OffsetRequest.PartitionData {                                      Id = p.Id'                                      Time = time'                                      MaxNumOffsets = 1                                  }).ToArray();                            // group parts by broker                          var requests = (                              from part in parts                              let broker = FindBrokerMetaForPartitionId(topic' part.Id)                              group part by broker into brokerGrp                              let req = new OffsetRequest { TopicName = topic' Partitions = brokerGrp.ToArray() }                              select _protocol.GetOffsets(req' brokerGrp.Key.Conn)                              ).ToArray();                            await Task.WhenAll(requests);                            // handle recoverable errors' such as tcp transport exceptions (with limited retry)                          // or partition relocation error codes                          if(requests.Any(r => r.IsFaulted))                              throw new AggregateException("Failure when getting offsets info"' requests.Where(r => r.IsFaulted).Select(r => r.Exception));                            var partitions = (from r in requests                                           from part in r.Result.Partitions                                           select part).ToArray();                            if (partitions.Any(p=> !p.ErrorCode.IsSuccess()))                              throw new Exception(string.Format("Partition Errors: [{0}]"' string.Join("'"' partitions.Select(p=>p.Partition + ":" + p.ErrorCode))));                            //if (partitions.Any(p => (p.Offsets.Length == 1 ? -1 : p.Offsets[1])==-1))                          //    throw new Exception(string.Format("Partition Head Offset is -1 for partition(s): [{0}]"' string.Join("'"' partitions.Select(p => p.Partition + ":" + (p.Offsets.Length == 1 ? -1 : p.Offsets[1])))));                            return new TopicPartitionOffsets(topic' partitions.ToDictionary(tp=>tp.Partition' tp=>tp.Offsets.First()));                      }                      catch (Exception ex)                      {                          retry = numAttempts < 4;                          exception = ex;                      }                           if (retry)                      {                          _log.Warn("Could not fetch offsets for topic {0}. Will Retry. Message: {1}"' topic' exception.Message);                          MergeTopicMeta(await FetchMetaWithRetryAsync(topic));                          await Task.Delay(TimeSpan.FromSeconds(1));                      }                      else                      {                          var error = "Could not fetch offsets for topic {0} after 4 attempts! Failing call";                          _log.Fatal(exception' error' topic);                          throw new BrokerException(error' exception);                      }                  }                  throw new TaskCanceledException();              }).ConfigureAwait(false);
Magic Number,kafka4net,Cluster,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Cluster.cs,FetchMetaWithRetryAsync,The following statement contains a magic number: await Task.Delay(500' _cancel.Token);
Magic Number,kafka4net,Cluster,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Cluster.cs,MergeTopicMeta,The following statement contains a magic number: var resolvedSeedBrokers = (                  from seed in _metadata.Brokers                  where seed.NodeId == -99                  from resolved in topicMeta.Brokers                  where resolved.NodeId != -99 &&                      seed.Port == resolved.Port &&                      string.Compare(resolved.Host' seed.Host' true' CultureInfo.InvariantCulture) == 0                  select new { seed' resolved }              ).ToArray();
Magic Number,kafka4net,Cluster,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Cluster.cs,MergeTopicMeta,The following statement contains a magic number: var resolvedSeedBrokers = (                  from seed in _metadata.Brokers                  where seed.NodeId == -99                  from resolved in topicMeta.Brokers                  where resolved.NodeId != -99 &&                      seed.Port == resolved.Port &&                      string.Compare(resolved.Host' seed.Host' true' CultureInfo.InvariantCulture) == 0                  select new { seed' resolved }              ).ToArray();
Magic Number,kafka4net,Cluster,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Cluster.cs,MergeTopicMeta,The following statement contains a magic number: newBrokers.Where(b => b.NodeId != -99).ForEach(b => _newBrokerSubject.OnNext(b));
Magic Number,kafka4net.Compression,KafkaSnappyStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\KafkaSnappyStream.cs,AllocateBuffers,The following statement contains a magic number: uncompressedBuffer = new byte[32 * 1024];
Magic Number,kafka4net.Compression,KafkaSnappyStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\KafkaSnappyStream.cs,AllocateBuffers,The following statement contains a magic number: uncompressedBuffer = new byte[32 * 1024];
Magic Number,kafka4net.Compression,KafkaSnappyStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\KafkaSnappyStream.cs,WriteHeader,The following statement contains a magic number: _base.Write(_versionsHeader' 0' 8);
Magic Number,kafka4net.Compression,KafkaSnappyStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\KafkaSnappyStream.cs,ReadBlock,The following statement contains a magic number: !StreamUtils.ReadAll(_base' _compressedBuffer' 4)
Magic Number,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,Flush,The following statement contains a magic number: LittleEndianConverter.Write((uint)(_bufferLen | 1 << 31)' _compressedBuffer' 0);
Magic Number,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,Flush,The following statement contains a magic number: _base.Write(_compressedBuffer' 0' 4);
Magic Number,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,ReadHeader,The following statement contains a magic number: !StreamUtils.ReadAll(_base' _headerBuffer' 7)
Magic Number,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,ReadHeader,The following statement contains a magic number: var flg = _headerBuffer[4];
Magic Number,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,ReadHeader,The following statement contains a magic number: var version = flg >> 6;
Magic Number,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,ReadHeader,The following statement contains a magic number: var hasBlockChecksum = (flg & (1 << 4)) != 0;
Magic Number,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,ReadHeader,The following statement contains a magic number: var bd = _headerBuffer[5];
Magic Number,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,ReadHeader,The following statement contains a magic number: var maxBlockSizeIndex = (bd >> 4) & 0x7;
Magic Number,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,ReadHeader,The following statement contains a magic number: _hasher.Update(_headerBuffer' 6);
Magic Number,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,ReadHeader,The following statement contains a magic number: var calculatedChecksum = (_hasher.Digest() >> 8) & 0xff;
Magic Number,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,ReadHeader,The following statement contains a magic number: calculatedChecksum != _headerBuffer[6]
Magic Number,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,CreateHeader,The following statement contains a magic number: var buff = new byte[4 + 3];
Magic Number,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,CreateHeader,The following statement contains a magic number: var buff = new byte[4 + 3];
Magic Number,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,CreateHeader,The following statement contains a magic number: var flags = 1 << 6 | 1 << 5;
Magic Number,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,CreateHeader,The following statement contains a magic number: var flags = 1 << 6 | 1 << 5;
Magic Number,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,CreateHeader,The following statement contains a magic number: var bd = 1 << 6;
Magic Number,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,CreateHeader,The following statement contains a magic number: buff[4] = (byte)flags;
Magic Number,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,CreateHeader,The following statement contains a magic number: buff[5] = (byte)bd;
Magic Number,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,CreateHeader,The following statement contains a magic number: hasher.Update(buff' 6);
Magic Number,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,CreateHeader,The following statement contains a magic number: var checksum = hasher.Digest() >> 8 & 0xff;
Magic Number,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,CreateHeader,The following statement contains a magic number: buff[6] = (byte)checksum;
Magic Number,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,ReadBlock,The following statement contains a magic number: !StreamUtils.ReadAll(_base' _uncompressedBuffer' 4)
Magic Number,kafka4net.Compression,Lz4KafkaStream,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,WriteEof,The following statement contains a magic number: _base.Write(_zero32' 0' 4);
Magic Number,kafka4net.ConsumerImpl,TopicPartitionOffsets,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\ConsumerImpl\TopicPartitionOffsets.cs,WriteOffsets,The following statement contains a magic number: var memStream = new MemoryStream(800);
Magic Number,kafka4net.ConsumerImpl,TopicPartition,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\ConsumerImpl\TopicPartition.cs,GetHashCode,The following statement contains a magic number: int hash = 27;
Magic Number,kafka4net.ConsumerImpl,TopicPartition,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\ConsumerImpl\TopicPartition.cs,GetHashCode,The following statement contains a magic number: hash = (13 * hash) + _topic.GetHashCode();
Magic Number,kafka4net.ConsumerImpl,TopicPartition,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\ConsumerImpl\TopicPartition.cs,GetHashCode,The following statement contains a magic number: hash = (13 * hash) + _partitionId.GetHashCode();
Magic Number,kafka4net.Internal,PartitionStateChangeEvent,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Internal\PartitionStateChangeEvent.cs,GetHashCode,The following statement contains a magic number: int hash = 27;
Magic Number,kafka4net.Internal,PartitionStateChangeEvent,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Internal\PartitionStateChangeEvent.cs,GetHashCode,The following statement contains a magic number: hash = (13 * hash) + Topic.GetHashCode();
Magic Number,kafka4net.Internal,PartitionStateChangeEvent,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Internal\PartitionStateChangeEvent.cs,GetHashCode,The following statement contains a magic number: hash = (13 * hash) + PartitionId.GetHashCode();
Magic Number,kafka4net.Internal,PartitionStateChangeEvent,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Internal\PartitionStateChangeEvent.cs,GetHashCode,The following statement contains a magic number: hash = (13 * hash) + ErrorCode.GetHashCode();
Magic Number,kafka4net.Internal,PartitionRecoveryMonitor,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The following statement contains a magic number: await Task.Delay(1000' _cancel);
Magic Number,kafka4net.Internal,PartitionRecoveryMonitor,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The following statement contains a magic number: await Task.Delay(1000' _cancel);
Magic Number,kafka4net.Internal,PartitionRecoveryMonitor,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The following statement contains a magic number: await Task.Delay(3000' _cancel);
Magic Number,kafka4net.Metadata,ComparerImpl,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Metadata\BrokerMeta.cs,Equals,The following statement contains a magic number: x.NodeId != -99 || y.NodeId != -99
Magic Number,kafka4net.Metadata,ComparerImpl,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Metadata\BrokerMeta.cs,Equals,The following statement contains a magic number: x.NodeId != -99 || y.NodeId != -99
Magic Number,kafka4net.Protocols,ResponseCorrelation,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\ResponseCorrelation.cs,CorrelateResponseLoop,The following statement contains a magic number: var buff = new byte[16 * 1024];
Magic Number,kafka4net.Protocols,ResponseCorrelation,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\ResponseCorrelation.cs,CorrelateResponseLoop,The following statement contains a magic number: var buff = new byte[16 * 1024];
Magic Number,kafka4net.Protocols,ResponseCorrelation,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\ResponseCorrelation.cs,CorrelateResponseLoop,The following statement contains a magic number: await ReadBuffer(client' buff' 4' cancel);
Magic Number,kafka4net.Protocols,ResponseCorrelation,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\ResponseCorrelation.cs,FormatBytes,The following statement contains a magic number: return buff.Take(Math.Min(256' len)).Aggregate(new StringBuilder()' (builder' b) => builder.Append(b.ToString("x2"))'                  str => str.ToString());
Magic Number,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,DeserializeMetadataResponse,The following statement contains a magic number: stream.Position += 4;
Magic Number,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,Serialize,The following statement contains a magic number: stream.Write(_zero32' 0' 4);
Magic Number,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,WriteMessageLength,The following statement contains a magic number: var len = buff.Length - 4;
Magic Number,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,Write,The following statement contains a magic number: stream.Write(_zero64' 0' 8);
Magic Number,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,WriteCompressedMessageSet,The following statement contains a magic number: stream.Write(_zero64' 0' 8);
Magic Number,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,Write,The following statement contains a magic number: stream.Write(_minusOne32' 0' 4);
Magic Number,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,WriteArray,The following statement contains a magic number: stream.Write(_minusOne32' 0' 4);
Magic Number,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,Write,The following statement contains a magic number: stream.Write(_minusOne16' 0' 2);
Magic Number,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,WriteRequestHeader,The following statement contains a magic number: stream.Write(_minusOne32' 0' 4);
Magic Number,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,WriteRequestHeader,The following statement contains a magic number: stream.Write(_apiVersion' 0' 2);
Magic Number,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,GetProducerResponse,The following statement contains a magic number: stream.Position += 4;
Magic Number,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,Serialize,The following statement contains a magic number: stream.Write(_minusOne32' 0' 4);
Magic Number,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,Serialize,The following statement contains a magic number: stream.Write(_one32' 0' 4);
Magic Number,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,DeserializeOffsetResponse,The following statement contains a magic number: stream.Position += 4;
Magic Number,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,Serialize,The following statement contains a magic number: stream.Write(_minusOne32' 0' 4);
Magic Number,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,DeserializeFetchResponse,The following statement contains a magic number: stream.Position += 4;
Magic Number,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,The following statement contains a magic number: remainingMessageSetBytes < 8 + 4
Magic Number,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,The following statement contains a magic number: remainingMessageSetBytes < 8 + 4
Magic Number,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,The following statement contains a magic number: remainingMessageSetBytes -= 8 + 4;
Magic Number,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,The following statement contains a magic number: remainingMessageSetBytes -= 8 + 4;
Magic Number,kafka4net.Protocols,Serializer,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ParseCompression,The following statement contains a magic number: return (CompressionType)(attributes & 3);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherCancelSentWakeup,The following statement contains a magic number: Log.WriteEvent(2' id);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherPartitionSubscribed,The following statement contains a magic number: Log.WriteEvent(3' id' partitionId);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherFetchResponse,The following statement contains a magic number: Log.WriteEvent(4' id);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherMessage,The following statement contains a magic number: Log.WriteEvent(5' id' keyLen' valueLen' offset' partition);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherSleep,The following statement contains a magic number: Log.WriteEvent(6' id);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherWakeup,The following statement contains a magic number: Log.WriteEvent(7' id);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherFetchRequest,The following statement contains a magic number: Log.WriteEvent(8' id' topicCount' partsCount' host' port' brokerId);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ConnectionConnecting,The following statement contains a magic number: Log.WriteEvent(101' host' port);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ConnectionConnected,The following statement contains a magic number: Log.WriteEvent(102' host' port);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ConnectionErrored,The following statement contains a magic number: Log.WriteEvent(103' host' port);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ConnectionDisconnected,The following statement contains a magic number: Log.WriteEvent(104' host' port);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ConnectionReplaceClosedClient,The following statement contains a magic number: Log.WriteEvent(107' host' port);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,Connection_MarkSocketAsFailed_CorrelationLoopCancelling,The following statement contains a magic number: Log.WriteEvent(108' host' port);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,Connection_MarkSocketAsFailed_TcpClosing,The following statement contains a magic number: Log.WriteEvent(109' host' port);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationCreate,The following statement contains a magic number: Log.WriteEvent(201);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationStart,The following statement contains a magic number: Log.WriteEvent(202);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationReadingMessageSize,The following statement contains a magic number: Log.WriteEvent(203);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationServerClosedConnection,The following statement contains a magic number: Log.WriteEvent(204);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationReadMessageSize,The following statement contains a magic number: Log.WriteEvent(205' size);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,Correlation_ReadingBodyChunk,The following statement contains a magic number: Log.WriteEvent(206' left);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationReadBodyChunk,The following statement contains a magic number: Log.WriteEvent(207' read' left);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationReadBody,The following statement contains a magic number: Log.WriteEvent(208' size);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationReceivedCorrelationId,The following statement contains a magic number: Log.WriteEvent(209' correlationId);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationExecutingHandler,The following statement contains a magic number: Log.WriteEvent(210);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationExecutedHandler,The following statement contains a magic number: Log.WriteEvent(211);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationError,The following statement contains a magic number: Log.WriteEvent(212' message);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationComplete,The following statement contains a magic number: Log.WriteEvent(213);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationWritingMessage,The following statement contains a magic number: Log.WriteEvent(214' correlationId' length);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_Create,The following statement contains a magic number: Log.WriteEvent(301' monitorId);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_PartitionRecovered,The following statement contains a magic number: Log.WriteEvent(302' monitorId' topic' partitionId);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_PartitionFailed,The following statement contains a magic number: Log.WriteEvent(303' monitorId' topic' partitionId' errorCode);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_PartitionFailedAgain,The following statement contains a magic number: Log.WriteEvent(304' monitorId' topic' partitionId' errorCode);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_RecoveryLoopStarted,The following statement contains a magic number: Log.WriteEvent(305' monitorId' host' port' nodeId);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_SendingPing,The following statement contains a magic number: Log.WriteEvent(306' monitorId' host' port);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_PingResponse,The following statement contains a magic number: Log.WriteEvent(307' monitorId' host' port);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_PingFailed,The following statement contains a magic number: Log.WriteEvent(308' monitorId' host' port' message);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_PossiblyHealedPartitions,The following statement contains a magic number: Log.WriteEvent(309' monitorId' count);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_NoHealedPartitions,The following statement contains a magic number: Log.WriteEvent(310' monitorId);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_CheckingBrokerAccessibility,The following statement contains a magic number: Log.WriteEvent(311' monitorId' host' port' nodeId);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_BrokerIsAccessible,The following statement contains a magic number: Log.WriteEvent(312' monitorId' host' port' nodeId);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_HealedPartitions,The following statement contains a magic number: Log.WriteEvent(313' monitorId' host' port' nodeId' topicName' partitions);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_RecoveryLoopStop,The following statement contains a magic number: Log.WriteEvent(314' monitorId);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolMetadataRequest,The following statement contains a magic number: Log.WriteEvent(400' request);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolMetadataResponse,The following statement contains a magic number: Log.WriteEvent(401' response' host' port' nodeId);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolProduceRequest,The following statement contains a magic number: Log.WriteEvent(402' request' nodeId);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolProduceResponse,The following statement contains a magic number: Log.WriteEvent(403' response' nodeId);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolOffsetRequest,The following statement contains a magic number: Log.WriteEvent(404' request);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolOffsetResponse,The following statement contains a magic number: Log.WriteEvent(405' response);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolFetchRequest,The following statement contains a magic number: Log.WriteEvent(406' request);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolFetchResponse,The following statement contains a magic number: Log.WriteEvent(407' response);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProducerPermanentFailure,The following statement contains a magic number: Log.WriteEvent(500' producerId' partitionCount);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProducerPermanentFailureDetails,The following statement contains a magic number: Log.WriteEvent(501' producerId' error);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProducerRecoverableErrors,The following statement contains a magic number: Log.WriteEvent(502' producerId' partitionCount);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProducerStarting,The following statement contains a magic number: Log.WriteEvent(503' topic' producerId);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProducerStarted,The following statement contains a magic number: Log.WriteEvent(504' topic' producerId);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProducerError,The following statement contains a magic number: Log.WriteEvent(505' error' producerId);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProducerStopping,The following statement contains a magic number: Log.WriteEvent(506' topic' producerId);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProducerStoped,The following statement contains a magic number: Log.WriteEvent(507' topic' producerId);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ClusterStarting,The following statement contains a magic number: Log.WriteEvent(600' clusterId);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ClusterStarted,The following statement contains a magic number: Log.WriteEvent(601' clusterId);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ClusterStopping,The following statement contains a magic number: Log.WriteEvent(602' clusterId);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ClusterError,The following statement contains a magic number: Log.WriteEvent(603' clusterId' error);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ClusterStopped,The following statement contains a magic number: Log.WriteEvent(604' clusterId);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataNewTopic,The following statement contains a magic number: Log.WriteEvent(700' clusterId' topic);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataPartitionErrorChange,The following statement contains a magic number: Log.WriteEvent(702' clusterId' topic' partId'oldCode' newCode);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataPartitionIsrChange,The following statement contains a magic number: Log.WriteEvent(703' clusterId' topic' partId' oldIsrs' newIsrs);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataPartitionLeaderChange,The following statement contains a magic number: Log.WriteEvent(704' clusterId' topic' partId' oldLeader' newLeader);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataPartitionReplicasChange,The following statement contains a magic number: Log.WriteEvent(705' clusterId' topic' partId' oldReplicas' newReplicas);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataTransportError,The following statement contains a magic number: Log.WriteEvent(706' topicName' clusterId' part' leader);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataNewBroker,The following statement contains a magic number: Log.WriteEvent(707' clusterId' host' port' nodeId);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ConsumerStarted,The following statement contains a magic number: Log.WriteEvent(800' consumerId' topic);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ConsumerStopped,The following statement contains a magic number: Log.WriteEvent(801' consumerId' topic);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ConsumerFlowControl,The following statement contains a magic number: Log.WriteEvent(802' isOpen);
Magic Number,kafka4net.Tracing,EtwTrace,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,Marker2,The following statement contains a magic number: Log.WriteEvent(900' marker);
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: s.CanSeek && s.Position + 4 > s.Length
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: return s.ReadByte() << 3 * 8 | s.ReadByte() << 2 * 8 | s.ReadByte() << 8 | s.ReadByte();
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: return s.ReadByte() << 3 * 8 | s.ReadByte() << 2 * 8 | s.ReadByte() << 8 | s.ReadByte();
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: return s.ReadByte() << 3 * 8 | s.ReadByte() << 2 * 8 | s.ReadByte() << 8 | s.ReadByte();
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: return s.ReadByte() << 3 * 8 | s.ReadByte() << 2 * 8 | s.ReadByte() << 8 | s.ReadByte();
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: return s.ReadByte() << 3 * 8 | s.ReadByte() << 2 * 8 | s.ReadByte() << 8 | s.ReadByte();
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: return buff[offset] << 3 * 8 | buff[offset + 1] << 2 * 8 | buff[offset+2] << 8 | buff[offset+3];
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: return buff[offset] << 3 * 8 | buff[offset + 1] << 2 * 8 | buff[offset+2] << 8 | buff[offset+3];
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: return buff[offset] << 3 * 8 | buff[offset + 1] << 2 * 8 | buff[offset+2] << 8 | buff[offset+3];
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: return buff[offset] << 3 * 8 | buff[offset + 1] << 2 * 8 | buff[offset+2] << 8 | buff[offset+3];
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: return buff[offset] << 3 * 8 | buff[offset + 1] << 2 * 8 | buff[offset+2] << 8 | buff[offset+3];
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: return buff[offset] << 3 * 8 | buff[offset + 1] << 2 * 8 | buff[offset+2] << 8 | buff[offset+3];
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: return buff[offset] << 3 * 8 | buff[offset + 1] << 2 * 8 | buff[offset+2] << 8 | buff[offset+3];
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt16,The following statement contains a magic number: s.CanSeek && s.Position + 2 > s.Length
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt16,The following statement contains a magic number: return (short)((s.ReadByte() << 8) | s.ReadByte());
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt64,The following statement contains a magic number: stream.CanSeek && stream.Position + 8 > stream.Length
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt64,The following statement contains a magic number: res = res << 8 | stream.ReadByte();
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt64,The following statement contains a magic number: i < 8
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: stream.WriteByte((byte)(ui >> j * 8 & 0xff));
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: WriteByte(stream' i >> 8 * 3);
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: WriteByte(stream' i >> 8 * 3);
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: WriteByte(stream' i >> 8 * 2);
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: WriteByte(stream' i >> 8 * 2);
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: WriteByte(stream' i >> 8);
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: WriteByte(stream' i >> 8);
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: buff[0] = (byte)(i >> 8 * 3);
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: buff[0] = (byte)(i >> 8 * 3);
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: buff[1] = (byte)((i & 0xff0000) >> 8 * 2);
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: buff[1] = (byte)((i & 0xff0000) >> 8 * 2);
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: buff[2] = (byte)((i & 0xff00) >> 8);
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: buff[2] = (byte)((i & 0xff00) >> 8);
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: buff[3] = (byte)(i & 0xff);
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ToInt32,The following statement contains a magic number: return (buff[0] << 8 * 3) | (buff[1] << 8 * 2) | (buff[2] << 8) | buff[3];
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ToInt32,The following statement contains a magic number: return (buff[0] << 8 * 3) | (buff[1] << 8 * 2) | (buff[2] << 8) | buff[3];
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ToInt32,The following statement contains a magic number: return (buff[0] << 8 * 3) | (buff[1] << 8 * 2) | (buff[2] << 8) | buff[3];
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ToInt32,The following statement contains a magic number: return (buff[0] << 8 * 3) | (buff[1] << 8 * 2) | (buff[2] << 8) | buff[3];
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ToInt32,The following statement contains a magic number: return (buff[0] << 8 * 3) | (buff[1] << 8 * 2) | (buff[2] << 8) | buff[3];
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ToInt32,The following statement contains a magic number: return (buff[0] << 8 * 3) | (buff[1] << 8 * 2) | (buff[2] << 8) | buff[3];
Magic Number,kafka4net.Utils,BigEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ToInt32,The following statement contains a magic number: return (buff[0] << 8 * 3) | (buff[1] << 8 * 2) | (buff[2] << 8) | buff[3];
Magic Number,kafka4net.Utils,Crc32,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: state = (state >> 8) ^ _table[buffer[i] ^ state & 0xff];
Magic Number,kafka4net.Utils,Crc32,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: state = (state >> 8) ^ _table[array[i] ^ state & 0xff];
Magic Number,kafka4net.Utils,Crc32,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: state = (state >> 8) ^ _table[(i >> 8 * 3 & 0xff) ^ state & 0xff];
Magic Number,kafka4net.Utils,Crc32,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: state = (state >> 8) ^ _table[(i >> 8 * 3 & 0xff) ^ state & 0xff];
Magic Number,kafka4net.Utils,Crc32,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: state = (state >> 8) ^ _table[(i >> 8 * 3 & 0xff) ^ state & 0xff];
Magic Number,kafka4net.Utils,Crc32,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: state = (state >> 8) ^ _table[(i >> 8 * 2 & 0xff) ^ state & 0xff];
Magic Number,kafka4net.Utils,Crc32,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: state = (state >> 8) ^ _table[(i >> 8 * 2 & 0xff) ^ state & 0xff];
Magic Number,kafka4net.Utils,Crc32,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: state = (state >> 8) ^ _table[(i >> 8 * 2 & 0xff) ^ state & 0xff];
Magic Number,kafka4net.Utils,Crc32,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: state = (state >> 8) ^ _table[(i >> 8 & 0xff) ^ state & 0xff];
Magic Number,kafka4net.Utils,Crc32,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: state = (state >> 8) ^ _table[(i >> 8 & 0xff) ^ state & 0xff];
Magic Number,kafka4net.Utils,Crc32,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: state = (state >> 8) ^ _table[(i & 0xff) ^ state & 0xff];
Magic Number,kafka4net.Utils,Crc32,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: state = (state >> 8) ^ _table[b ^ state & 0xff];
Magic Number,kafka4net.Utils,Crc32,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\Crc32.cs,InitializeTable,The following statement contains a magic number: var createTable = new UInt32[256];
Magic Number,kafka4net.Utils,Crc32,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\Crc32.cs,InitializeTable,The following statement contains a magic number: j < 8
Magic Number,kafka4net.Utils,Crc32,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\Crc32.cs,InitializeTable,The following statement contains a magic number: i < 256
Magic Number,kafka4net.Utils,DateTimeExtensions,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\DateTimeExtensions.cs,DateTimeToEpochSeconds,The following statement contains a magic number: var t = (date - new DateTimeOffset(1970' 1' 1' 0' 0' 0' TimeSpan.Zero));
Magic Number,kafka4net.Utils,DateTimeExtensions,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\DateTimeExtensions.cs,EpochSecondsToDateTime,The following statement contains a magic number: var date = new DateTimeOffset(1970' 1' 1'0'0'0'TimeSpan.Zero);
Magic Number,kafka4net.Utils,DateTimeExtensions,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\DateTimeExtensions.cs,DateTimeToEpochMilliseconds,The following statement contains a magic number: var t = (date - new DateTimeOffset(1970' 1' 1'0'0'0'TimeSpan.Zero));
Magic Number,kafka4net.Utils,DateTimeExtensions,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\DateTimeExtensions.cs,EpochMillisecondsToDateTime,The following statement contains a magic number: var date = new DateTimeOffset(1970' 1' 1' 0' 0' 0' TimeSpan.Zero);
Magic Number,kafka4net.Utils,LittleEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,ReadUInt32,The following statement contains a magic number: return (uint)(buff[offset+3] << 3 * 8 | buff[offset+2] << 2 * 8 | buff[offset+1] << 8 | buff[offset]);
Magic Number,kafka4net.Utils,LittleEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,ReadUInt32,The following statement contains a magic number: return (uint)(buff[offset+3] << 3 * 8 | buff[offset+2] << 2 * 8 | buff[offset+1] << 8 | buff[offset]);
Magic Number,kafka4net.Utils,LittleEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,ReadUInt32,The following statement contains a magic number: return (uint)(buff[offset+3] << 3 * 8 | buff[offset+2] << 2 * 8 | buff[offset+1] << 8 | buff[offset]);
Magic Number,kafka4net.Utils,LittleEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,ReadUInt32,The following statement contains a magic number: return (uint)(buff[offset+3] << 3 * 8 | buff[offset+2] << 2 * 8 | buff[offset+1] << 8 | buff[offset]);
Magic Number,kafka4net.Utils,LittleEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,ReadUInt32,The following statement contains a magic number: return (uint)(buff[offset+3] << 3 * 8 | buff[offset+2] << 2 * 8 | buff[offset+1] << 8 | buff[offset]);
Magic Number,kafka4net.Utils,LittleEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,ReadUInt32,The following statement contains a magic number: return (uint)(buff[offset+3] << 3 * 8 | buff[offset+2] << 2 * 8 | buff[offset+1] << 8 | buff[offset]);
Magic Number,kafka4net.Utils,LittleEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,ReadUInt32,The following statement contains a magic number: return (uint)(buff[offset+3] << 3 * 8 | buff[offset+2] << 2 * 8 | buff[offset+1] << 8 | buff[offset]);
Magic Number,kafka4net.Utils,LittleEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,Write,The following statement contains a magic number: buff[offset + 1] = (byte)(i >> 8 & 0xff);
Magic Number,kafka4net.Utils,LittleEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,Write,The following statement contains a magic number: buff[offset + 2] = (byte)(i >> 8*2 & 0xff);
Magic Number,kafka4net.Utils,LittleEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,Write,The following statement contains a magic number: buff[offset + 2] = (byte)(i >> 8*2 & 0xff);
Magic Number,kafka4net.Utils,LittleEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,Write,The following statement contains a magic number: buff[offset + 2] = (byte)(i >> 8*2 & 0xff);
Magic Number,kafka4net.Utils,LittleEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,Write,The following statement contains a magic number: buff[offset + 3] = (byte)(i >> 8*3 & 0xff);
Magic Number,kafka4net.Utils,LittleEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,Write,The following statement contains a magic number: buff[offset + 3] = (byte)(i >> 8*3 & 0xff);
Magic Number,kafka4net.Utils,LittleEndianConverter,C:\research\architectureSmells\repos\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,Write,The following statement contains a magic number: buff[offset + 3] = (byte)(i >> 8*3 & 0xff);
