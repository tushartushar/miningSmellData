Implementation smell,Namespace,Class,File,Method,Description
Long Method,kafka4net,Connection,D:\newReposJune17\ntent-ad_kafka4net\src\Connection.cs,GetClientAsync,The method has 61 lines of code.
Long Method,kafka4net,Consumer,D:\newReposJune17\ntent-ad_kafka4net\src\Consumer.cs,Consumer,The method has 64 lines of code.
Long Method,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The method has 113 lines of code.
Long Method,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,CloseAsync,The method has 63 lines of code.
Long Method,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The method has 154 lines of code.
Long Method,kafka4net,Cluster,D:\newReposJune17\ntent-ad_kafka4net\src\Cluster.cs,MergeTopicMeta,The method has 64 lines of code.
Long Method,kafka4net.ConsumerImpl,Fetcher,D:\newReposJune17\ntent-ad_kafka4net\src\ConsumerImpl\Fetcher.cs,FetchLoop,The method has 90 lines of code.
Long Method,kafka4net.Internal,PartitionRecoveryMonitor,D:\newReposJune17\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The method has 146 lines of code.
Long Method,kafka4net.Protocols,ResponseCorrelation,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\ResponseCorrelation.cs,CorrelateResponseLoop,The method has 78 lines of code.
Long Method,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,The method has 89 lines of code.
Complex Method,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,CloseAsync,Cyclomatic complexity of the method is 8
Complex Method,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,Cyclomatic complexity of the method is 10
Complex Method,kafka4net.Compression,Lz4KafkaStream,D:\newReposJune17\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,ReadHeader,Cyclomatic complexity of the method is 8
Complex Method,kafka4net.Internal,PartitionRecoveryMonitor,D:\newReposJune17\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,Cyclomatic complexity of the method is 11
Complex Method,kafka4net.Protocols,ResponseCorrelation,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\ResponseCorrelation.cs,CorrelateResponseLoop,Cyclomatic complexity of the method is 11
Complex Method,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,Compress,Cyclomatic complexity of the method is 10
Complex Method,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,Cyclomatic complexity of the method is 23
Long Parameter List,kafka4net,ProducerConfiguration,D:\newReposJune17\ntent-ad_kafka4net\src\ProducerConfiguration.cs,ProducerConfiguration,The method has 10 parameters.
Long Parameter List,kafka4net,ConsumerConfiguration,D:\newReposJune17\ntent-ad_kafka4net\src\ConsumerConfiguration.cs,ConsumerConfiguration,The method has 11 parameters.
Long Parameter List,kafka4net.Compression,KafkaSnappyStream,D:\newReposJune17\ntent-ad_kafka4net\src\Compression\KafkaSnappyStream.cs,KafkaSnappyStream,The method has 4 parameters.
Long Parameter List,kafka4net.ConsumerImpl,TopicPartition,D:\newReposJune17\ntent-ad_kafka4net\src\ConsumerImpl\TopicPartition.cs,TopicPartition,The method has 4 parameters.
Long Parameter List,kafka4net.ConsumerImpl,Fetcher,D:\newReposJune17\ntent-ad_kafka4net\src\ConsumerImpl\Fetcher.cs,Fetcher,The method has 5 parameters.
Long Parameter List,kafka4net.Protocols,ResponseCorrelation,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\ResponseCorrelation.cs,ReadBuffer,The method has 4 parameters.
Long Parameter List,kafka4net.Protocols,ResponseCorrelation,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\ResponseCorrelation.cs,SendAndCorrelateAsync,The method has 4 parameters.
Long Parameter List,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherMessage,The method has 5 parameters.
Long Parameter List,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherFetchRequest,The method has 6 parameters.
Long Parameter List,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_PartitionFailed,The method has 4 parameters.
Long Parameter List,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_PartitionFailedAgain,The method has 4 parameters.
Long Parameter List,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_RecoveryLoopStarted,The method has 4 parameters.
Long Parameter List,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_PingFailed,The method has 4 parameters.
Long Parameter List,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_CheckingBrokerAccessibility,The method has 4 parameters.
Long Parameter List,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_BrokerIsAccessible,The method has 4 parameters.
Long Parameter List,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_HealedPartitions,The method has 6 parameters.
Long Parameter List,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolMetadataResponse,The method has 4 parameters.
Long Parameter List,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataPartitionErrorChange,The method has 5 parameters.
Long Parameter List,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataPartitionIsrChange,The method has 5 parameters.
Long Parameter List,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataPartitionLeaderChange,The method has 5 parameters.
Long Parameter List,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataPartitionReplicasChange,The method has 5 parameters.
Long Parameter List,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataTransportError,The method has 4 parameters.
Long Parameter List,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataNewBroker,The method has 4 parameters.
Long Parameter List,kafka4net.Utils,Crc32,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The method has 4 parameters.
Long Parameter List,kafka4net.Utils,CircularBuffer,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\CircularBuffer.cs,CopyTo,The method has 4 parameters.
Long Identifier,kafka4net.ConsumerImpl,Fetcher,D:\newReposJune17\ntent-ad_kafka4net\src\ConsumerImpl\Fetcher.cs,Subscribe,The length of the parameter receivedMessagesSubscriptionCleanup is 35.
Long Statement,kafka4net,FletcherHashedMessagePartitioner,D:\newReposJune17\ntent-ad_kafka4net\src\FletcherHashedMessagePartitioner.cs,GetMessagePartition,The length of the statement  "	var index = message.Key == null ? _rnd.Value.Next (allPartitions.Length) : Fletcher32HashOptimized (message.Key) % allPartitions.Length; " is 136.
Long Statement,kafka4net,Connection,D:\newReposJune17\ntent-ad_kafka4net\src\Connection.cs,GetClientAsync,The length of the statement  "				_log.Debug ("CorrelationLoop completed with status {0}. {1}"' t.Status' t.Exception == null ? "" : string.Format ("Closing connection because of error. {0}"' t.Exception.Message)); " is 180.
Long Statement,kafka4net,Consumer,D:\newReposJune17\ntent-ad_kafka4net\src\Consumer.cs,BuildTopicPartitionsAsync,The length of the statement  "			_log.Debug ("Consumer for topic {0} got time->offset resolved for location {1}. parts: [{2}]"' Topic' startPositionProvider' string.Join ("'"' partitions.Partitions.OrderBy (p => p).Select (p => string.Format ("{0}:{1}"' p' partitions.NextOffset (p))))); " is 254.
Long Statement,kafka4net,Consumer,D:\newReposJune17\ntent-ad_kafka4net\src\Consumer.cs,BuildTopicPartitionsAsync,The length of the statement  "		startPositionProvider = new TopicPartitionOffsets (partitions.Topic' partitions.GetPartitionsOffset.Where (kv => origProvider.ShouldConsumePartition (kv.Key))); " is 160.
Long Statement,kafka4net,Consumer,D:\newReposJune17\ntent-ad_kafka4net\src\Consumer.cs,OnTopicPartitionComplete,The length of the statement  "		// If we call OnCompleted right now' any last message that may be being sent will not process. Just tell the scheduler to do it in a moment. " is 140.
Long Statement,kafka4net,Consumer,D:\newReposJune17\ntent-ad_kafka4net\src\Consumer.cs,Dispose,The length of the statement  "			_cluster.CloseAsync (TimeSpan.FromSeconds (5)).ContinueWith (t => _log.Error (t.Exception' "Error when closing Cluster")' TaskContinuationOptions.OnlyOnFaulted); " is 161.
Long Statement,kafka4net,ErrorCodeClassification,D:\newReposJune17\ntent-ad_kafka4net\src\ErrorCode.cs,IsPermanentFailure,The length of the statement  "	return errorCode == ErrorCode.MessageSetSizeTooLarge || errorCode == ErrorCode.MessageSizeTooLarge || errorCode == ErrorCode.InvalidMessageSize || errorCode == ErrorCode.InvalidMessage || errorCode == ErrorCode.OffsetOutOfRange; " is 228.
Long Statement,kafka4net,Logger,D:\newReposJune17\ntent-ad_kafka4net\src\Logger.cs,SetupNLog,The length of the statement  "	var method = logManagerType.GetMethod ("GetLogger"' BindingFlags.InvokeMethod | BindingFlags.Public | BindingFlags.Static' null' CallingConventions.Standard' new[] { " is 165.
Long Statement,kafka4net,Logger,D:\newReposJune17\ntent-ad_kafka4net\src\Logger.cs,SetupLog4Net,The length of the statement  "	var method = logManagerType.GetMethod ("GetLogger"' BindingFlags.InvokeMethod | BindingFlags.Public | BindingFlags.Static' null' CallingConventions.Standard' new[] { " is 165.
Long Statement,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The length of the statement  "				_partitionStateSubsctiption = _cluster.PartitionStateChanges.Where (p => p.Topic == Configuration.Topic).Synchronize (_allPartitionQueues).Subscribe (p => { " is 156.
Long Statement,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The length of the statement  "					_log.Info ("Detected change in topic/partition '{0}'/{1}/{2} IsOnline {3}->{4}"' Configuration.Topic' p.PartitionId' p.ErrorCode' queue.IsOnline' p.ErrorCode.IsSuccess ()); " is 172.
Long Statement,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The length of the statement  "				_sendMessagesSubject.Do (msg => msg.PartitionId = Configuration.Partitioner.GetMessagePartition (msg' topicPartitions).Id).Buffer (Configuration.BatchFlushTime' Configuration.BatchFlushSize).Where (b => b.Count > 0).Select (msgs => msgs.GroupBy (msg => msg.PartitionId)).ObserveOn (_cluster.Scheduler).Subscribe (partitionGroups => { " is 333.
Long Statement,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The length of the statement  "								_log.Warn ("Capacity of send buffer with size {3} not large enough to accept {2} new messages. Increasing capacity from {0} to {1}"' queue.Queue.Capacity' queue.Queue.Capacity + growBy' batchAr.Length' queue.Queue.Size); " is 220.
Long Statement,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The length of the statement  "								var msg = string.Format ("Send Buffer Full for partition {0}. "' Cluster.PartitionStateChanges.Where (ps => ps.Topic == Configuration.Topic && ps.PartitionId == queue.Partition).Take (1).Wait ()); " is 196.
Long Statement,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The length of the statement  "							_log.Debug ("Enqueued batch of size {0} for topic '{1}' partition {2}"' batchAr.Length' Configuration.Topic' batch.Key); " is 120.
Long Statement,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The length of the statement  "		// if we are being told to shut down (already waited for drain)' then send all messages back over OnPermError' and quit. " is 120.
Long Statement,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The length of the statement  "			_log.Debug ("There are {0} partition queues with {1} total messages to send."' queuesToBeSent.Length' queuesToBeSent.Sum (qi => qi.Queue.Size)); " is 144.
Long Statement,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The length of the statement  "		_log.Debug ("Locking queues: '{0}'/[{1}]"' Configuration.Topic' string.Join ("'"' queuesToBeSent.Select (q => q.Partition))); " is 125.
Long Statement,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The length of the statement  "					var failedResponsePartitions = response.Topics.Where (t => t.TopicName == Configuration.Topic).// should contain response only for our topic' but just in case... " is 161.
Long Statement,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The length of the statement  "						_log.Warn ("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode); " is 131.
Long Statement,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The length of the statement  "						_cluster.NotifyPartitionStateChange (new PartitionStateChangeEvent (Topic' failedPart.Partition' failedPart.ErrorCode)); " is 120.
Long Statement,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The length of the statement  "						string error = string.Join ("' "' permanentErrorPartitions.Select (p => string.Format ("{0}:{1}"' p.Partition' p.ErrorCode))); " is 126.
Long Statement,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The length of the statement  "					var permanentFailedMessages = brokerBatch.Queues.Where (q => permanentErrorPartitionIds.Contains (q.Partition)).SelectMany (q => q.Queue.GetEnum (q.CountInProgress)).ToArray (); " is 177.
Long Statement,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The length of the statement  "					//brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError))); " is 168.
Long Statement,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The length of the statement  "				_log.Debug ("Unlocked queue '{0}'/{1}"' Configuration.Topic' string.Join ("'"' queuesToBeSent.Select (q => q.Partition))); " is 122.
Long Statement,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,ToString,The length of the statement  "	return string.Format ("'{0}' Batch flush time: {1} Batch flush size: {2}"' Topic' Configuration.BatchFlushTime' Configuration.BatchFlushSize); " is 142.
Long Statement,kafka4net,Cluster,D:\newReposJune17\ntent-ad_kafka4net\src\Cluster.cs,HandleTransportError,The length of the statement  "		_partitionStateChangesSubject.OnNext (new PartitionStateChangeEvent (p.TopicName' p.part.Id' ErrorCode.TransportError)); " is 120.
Long Statement,kafka4net,Cluster,D:\newReposJune17\ntent-ad_kafka4net\src\Cluster.cs,ConnectAsync,The length of the statement  "		_partitionRecoveryMonitor.NewMetadataEvents.Subscribe (MergeTopicMeta' ex => _log.Error (ex' "Error thrown by RecoveryMonitor.NewMetadataEvents!")); " is 148.
Long Statement,kafka4net,Cluster,D:\newReposJune17\ntent-ad_kafka4net\src\Cluster.cs,CloseAsync,The length of the statement  "	var success = await await Scheduler.Ask (async () => await CloseAsyncImpl ().TimeoutAfter (timeout)).ConfigureAwait (false); " is 124.
Long Statement,kafka4net,Cluster,D:\newReposJune17\ntent-ad_kafka4net\src\Cluster.cs,FetchPartitionOffsetsImplAsync,The length of the statement  "					throw new AggregateException ("Failure when getting offsets info"' requests.Where (r => r.IsFaulted).Select (r => r.Exception)); " is 128.
Long Statement,kafka4net,Cluster,D:\newReposJune17\ntent-ad_kafka4net\src\Cluster.cs,FetchPartitionOffsetsImplAsync,The length of the statement  "					throw new Exception (string.Format ("Partition Errors: [{0}]"' string.Join ("'"' partitions.Select (p => p.Partition + ":" + p.ErrorCode)))); " is 141.
Long Statement,kafka4net,Cluster,D:\newReposJune17\ntent-ad_kafka4net\src\Cluster.cs,FetchPartitionOffsetsImplAsync,The length of the statement  "				//    throw new Exception(string.Format("Partition Head Offset is -1 for partition(s): [{0}]"' string.Join("'"' partitions.Select(p => p.Partition + ":" + (p.Offsets.Length == 1 ? -1 : p.Offsets[1]))))); " is 203.
Long Statement,kafka4net,Cluster,D:\newReposJune17\ntent-ad_kafka4net\src\Cluster.cs,BuildPartitionStateChangeSubject,The length of the statement  "	_partitionStateChanges.Subscribe (psc => _log.Info ("Cluster saw new partition state: {0}-{1}-{2}"' psc.Topic' psc.PartitionId' psc.ErrorCode)' ex => _log.Fatal (ex' "GetFetcherChages saw ERROR from PartitionStateChanges")); " is 224.
Long Statement,kafka4net,Cluster,D:\newReposJune17\ntent-ad_kafka4net\src\Cluster.cs,GetFetcherChanges,The length of the statement  "	}).Do (f => _log.Debug ("GetFetcherChanges returning {1} fetcher {0}"' f' f == null ? "null" : "new")' ex => _log.Error (ex' "GetFetcherChages saw ERROR returning new fetcher.")' () => _log.Error ("GetFetcherChanges saw COMPLETE from returning new fetcher.")); " is 260.
Long Statement,kafka4net,Cluster,D:\newReposJune17\ntent-ad_kafka4net\src\Cluster.cs,GetAnyClientAsync,The length of the statement  "		Select (t => t.ToObservable ().Catch (Observable.Empty<Tuple<Connection' TcpClient>> ())).ToObservable ().Merge ().FirstAsync (); " is 129.
Long Statement,kafka4net,Cluster,D:\newReposJune17\ntent-ad_kafka4net\src\Cluster.cs,MergeTopicMeta,The length of the statement  "			EtwTrace.Log.MetadataPartitionErrorChange (_id' _.TopicName' _.oldPart.Id' (int)_.oldPart.ErrorCode' (int)_.updatedPart.ErrorCode); " is 131.
Long Statement,kafka4net,Cluster,D:\newReposJune17\ntent-ad_kafka4net\src\Cluster.cs,MergeTopicMeta,The length of the statement  "			EtwTrace.Log.MetadataPartitionIsrChange (_id' _.TopicName' _.oldPart.Id' string.Join ("'"' _.oldPart.Isr)' string.Join ("'"' _.updatedPart.Isr)); " is 145.
Long Statement,kafka4net,Cluster,D:\newReposJune17\ntent-ad_kafka4net\src\Cluster.cs,MergeTopicMeta,The length of the statement  "			EtwTrace.Log.MetadataPartitionReplicasChange (_id' _.TopicName' _.oldPart.Id' string.Join ("'"' _.oldPart.Replicas)' string.Join ("'"' _.updatedPart.Replicas)); " is 160.
Long Statement,kafka4net,Cluster,D:\newReposJune17\ntent-ad_kafka4net\src\Cluster.cs,MergeTopicMeta,The length of the statement  "	where resolved.NodeId != -99 && seed.Port == resolved.Port && string.Compare (resolved.Host' seed.Host' true' CultureInfo.InvariantCulture) == 0 " is 144.
Long Statement,kafka4net,Cluster,D:\newReposJune17\ntent-ad_kafka4net\src\Cluster.cs,MergeTopicMeta,The length of the statement  "	topicMeta.Topics.SelectMany (t => t.Partitions.Select (part => new PartitionStateChangeEvent (t.TopicName' part.Id' part.ErrorCode))).ForEach (tp => _partitionStateChangesSubject.OnNext (tp)); " is 192.
Long Statement,kafka4net.Compression,Lz4KafkaStream,D:\newReposJune17\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,Flush,The length of the statement  "	var compressedSize = LZ4Codec.Encode (_uncompressedBuffer' 0' _bufferLen' _compressedBuffer' 0' _compressedBuffer.Length); " is 122.
Long Statement,kafka4net.Compression,Lz4KafkaStream,D:\newReposJune17\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,ReadBlock,The length of the statement  "	var decodedSize = LZ4Codec.Decode (_compressedBuffer' 0' blockSize' _uncompressedBuffer' 0' _uncompressedBuffer.Length); " is 120.
Long Statement,kafka4net.ConsumerImpl,StartAndStopAtExplicitOffsets,D:\newReposJune17\ntent-ad_kafka4net\src\ConsumerImpl\PositionProviders.cs,ShouldConsumePartition,The length of the statement  "	// we should only consume from this partition if we were told to by the start offsets and also if we are not stopping where we already are. " is 139.
Long Statement,kafka4net.ConsumerImpl,StartAndStopAtExplicitOffsets,D:\newReposJune17\ntent-ad_kafka4net\src\ConsumerImpl\PositionProviders.cs,ShouldConsumePartition,The length of the statement  "	return _startingOffsets.ShouldConsumePartition (partitionId) && _startingOffsets.NextOffset (partitionId) != _stoppingOffsets.NextOffset (partitionId); " is 151.
Long Statement,kafka4net.ConsumerImpl,TopicPartition,D:\newReposJune17\ntent-ad_kafka4net\src\ConsumerImpl\TopicPartition.cs,Subscribe,The length of the statement  "	_fetcherChangesSubscription = _cluster.GetFetcherChanges (_topic' _partitionId' consumer.Configuration).Do (fetchers.Add).Subscribe (OnNewFetcher' OnFetcherChangesError' OnFetcherChangesComplete); " is 196.
Long Statement,kafka4net.ConsumerImpl,TopicPartition,D:\newReposJune17\ntent-ad_kafka4net\src\ConsumerImpl\TopicPartition.cs,OnNext,The length of the statement  "				_log.Info ("{0} was expecting offset {1} but received larger offset {2}"' this' _partitionFetchState.Offset' value.Offset); " is 123.
Long Statement,kafka4net.ConsumerImpl,TopicPartition,D:\newReposJune17\ntent-ad_kafka4net\src\ConsumerImpl\TopicPartition.cs,OnNext,The length of the statement  "			_log.Debug ("{0} Skipping message offset {1} as it is less than requested offset {2}"' this' value.Offset' _partitionFetchState.Offset); " is 136.
Long Statement,kafka4net.ConsumerImpl,TopicPartition,D:\newReposJune17\ntent-ad_kafka4net\src\ConsumerImpl\TopicPartition.cs,OnCompleted,The length of the statement  "	_log.Warn ("{0} Recieved OnComplete from Fetcher. Fetcher may have errored out. Waiting for new or updated Fetcher."' this); " is 124.
Long Statement,kafka4net.ConsumerImpl,TopicPartition,D:\newReposJune17\ntent-ad_kafka4net\src\ConsumerImpl\TopicPartition.cs,Equals,The length of the statement  "	return _cluster == objTopicPartition._cluster && _topic == objTopicPartition._topic && _partitionId == objTopicPartition._partitionId; " is 134.
Long Statement,kafka4net.ConsumerImpl,Fetcher,D:\newReposJune17\ntent-ad_kafka4net\src\ConsumerImpl\Fetcher.cs,Subscribe,The length of the statement  "	var receivedMessagesSubscriptionCleanup = ReceivedMessages.Where (rm => rm.Topic == topicPartition.Topic && rm.Partition == topicPartition.PartitionId).Subscribe (topicPartition); " is 179.
Long Statement,kafka4net.ConsumerImpl,Fetcher,D:\newReposJune17\ntent-ad_kafka4net\src\ConsumerImpl\Fetcher.cs,Subscribe,The length of the statement  "	var flowControlCleanup = topicPartition.FlowControl.// we need to wake up from waiting loop any time flow control hits low watermark and becomes enabled " is 152.
Long Statement,kafka4net.ConsumerImpl,Fetcher,D:\newReposJune17\ntent-ad_kafka4net\src\ConsumerImpl\Fetcher.cs,Subscribe,The length of the statement  "		cleanup.Add (Disposable.Create (() => _log.Debug ("Fetcher #{0} {1} topicPartition is unsubscribing"' _id' topicPartition))); " is 125.
Long Statement,kafka4net.ConsumerImpl,Fetcher,D:\newReposJune17\ntent-ad_kafka4net\src\ConsumerImpl\Fetcher.cs,BuildReceivedMessages,The length of the statement  "	}).Do (msg => EtwTrace.Log.FetcherMessage (_id' msg.Key != null ? msg.Key.Length : -1' msg.Value != null ? msg.Value.Length : -1' msg.Offset' msg.Partition)).Do (_ => { " is 168.
Long Statement,kafka4net.ConsumerImpl,Fetcher,D:\newReposJune17\ntent-ad_kafka4net\src\ConsumerImpl\Fetcher.cs,BuildReceivedMessages,The length of the statement  "	}' err => _log.Warn ("Error in ReceivedMessages stream from broker {0}. Message: {1}"' _broker' err.Message)' () => _log.Debug ("ReceivedMessages stream for broker {0} is complete."' _broker)).Publish ().RefCount (); " is 216.
Long Statement,kafka4net.ConsumerImpl,Fetcher,D:\newReposJune17\ntent-ad_kafka4net\src\ConsumerImpl\Fetcher.cs,FetchLoop,The length of the statement  "				EtwTrace.Log.FetcherFetchRequest (_id' fetchRequest.Topics.Length' fetchRequest.Topics.Sum (td => td.Partitions.Length)' _broker.Host' _broker.Port' _broker.NodeId); " is 165.
Long Statement,kafka4net.ConsumerImpl,Fetcher,D:\newReposJune17\ntent-ad_kafka4net\src\ConsumerImpl\Fetcher.cs,FetchLoop,The length of the statement  "				fetch.Topics.SelectMany (t => t.Partitions.Select (p => new PartitionStateChangeEvent (t.Topic' p.Partition' p.ErrorCode))).Where (ps => !ps.ErrorCode.IsSuccess ()).ForEach (ps => _cluster.NotifyPartitionStateChange (ps)); " is 222.
Long Statement,kafka4net.Internal,PartitionRecoveryMonitor,D:\newReposJune17\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The length of the statement  "				_log.Debug ("Out of {0} partitions returned from broker {2}' none of the {3} errored partitions are healed. Current partition states for errored partitions: [{1}]"' response.Topics.SelectMany (t => t.Partitions).Count ()' string.Join ("'"' response.Topics.SelectMany (t => t.Partitions.Select (p => new { " is 304.
Long Statement,kafka4net.Internal,PartitionRecoveryMonitor,D:\newReposJune17\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The length of the statement  "				})).Where (p => _failedList.ContainsKey (new Tuple<string' int> (p.TopicName' p.PartitionId))).Select (p => string.Format ("{0}:{1}:{2}:{3}"' p.TopicName' p.TopicErrorCode' p.PartitionId' p.PartitionErrorCode)))' broker' _failedList.Count); " is 240.
Long Statement,kafka4net.Internal,PartitionRecoveryMonitor,D:\newReposJune17\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The length of the statement  "					_log.Error ("Got metadata response with partition refering to a broker which is not part of the response: {0}"' response.ToString ()); " is 134.
Long Statement,kafka4net.Internal,PartitionRecoveryMonitor,D:\newReposJune17\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The length of the statement  "					// we may broadcast more than 1 broker' but it should be ok because discovery of new broker metadata does not cause any actions " is 127.
Long Statement,kafka4net.Internal,PartitionRecoveryMonitor,D:\newReposJune17\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The length of the statement  "						EtwTrace.Log.RecoveryMonitor_HealedPartitions (_id' newBroker.Host' newBroker.Port' newBroker.NodeId' topic.TopicName' string.Join ("'"' topic.Partitions.Select (p => p.Id))); " is 175.
Long Statement,kafka4net.Internal,PartitionRecoveryMonitor,D:\newReposJune17\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The length of the statement  "		// we will keep accumulating responses in memory faster than they time out. See https://github.com/ntent-ad/kafka4net/issues/30 " is 127.
Long Statement,kafka4net.Metadata,PartitionMeta,D:\newReposJune17\ntent-ad_kafka4net\src\Metadata\PartitionMeta.cs,ToString,The length of the statement  "	return string.Format ("Id: {0} Leader: {1} Error: {2}' Replicas: {3}' Isr: {4}"' Id' Leader' ErrorCode' IntToStringList (Replicas)' IntToStringList (Isr)); " is 155.
Long Statement,kafka4net.Metadata,TopicMeta,D:\newReposJune17\ntent-ad_kafka4net\src\Metadata\TopicMeta.cs,ToString,The length of the statement  "	return string.Format ("Topic '{0}' {1} Partitions [{2}]"' TopicName' ErrorCode' string.Join ("'"' Partitions.AsEnumerable ())); " is 127.
Long Statement,kafka4net.Protocols,ResponseCorrelation,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\ResponseCorrelation.cs,CorrelateResponseLoop,The length of the statement  "		_corelationTable.Values.ForEach (c => c (null' 0' new CorrelationLoopException ("Correlation loop closed. Request will never get a response.") { " is 144.
Long Statement,kafka4net.Protocols,ResponseCorrelation,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\ResponseCorrelation.cs,SendAndCorrelateAsync,The length of the statement  "		// TODO: think how buff can be reused. Would it be safe to declare buffer as a member? Is there a guarantee of one write at the time? " is 133.
Long Statement,kafka4net.Protocols,ResponseCorrelation,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\ResponseCorrelation.cs,FormatBytes,The length of the statement  "	return buff.Take (Math.Min (256' len)).Aggregate (new StringBuilder ()' (builder' b) => builder.Append (b.ToString ("x2"))' str => str.ToString ()); " is 148.
Long Statement,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,Compress,The length of the statement  "		var snappy = new KafkaSnappyStream (compressed' CompressionStreamMode.Compress' _snappyUncompressedBuffer' _snappyCompressedBuffer); " is 132.
Long Statement,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,The length of the statement  "				throw new BrokerException (string.Format ("Corrupt message: Crc does not match. Caclulated {0} but got {1}"' computedCrc' crc)); " is 128.
Long Statement,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,The length of the statement  "			using (var snappyStream = new KafkaSnappyStream (new MemoryStream (value)' CompressionStreamMode.Decompress' _snappyUncompressedBuffer' _snappyCompressedBuffer)) { " is 163.
Long Statement,kafka4net.Protocols,Protocol,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Protocol.cs,Produce,The length of the statement  "	var response = await conn.Correlation.SendAndCorrelateAsync (id => Serializer.Serialize (request' id)' Serializer.GetProducerResponse' client' CancellationToken.None); " is 167.
Long Statement,kafka4net.Protocols,Protocol,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Protocol.cs,MetadataRequest,The length of the statement  "	var response = await conn.Correlation.SendAndCorrelateAsync (id => Serializer.Serialize (request' id)' Serializer.DeserializeMetadataResponse' tcp' CancellationToken.None); " is 172.
Long Statement,kafka4net.Protocols,Protocol,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Protocol.cs,MetadataRequest,The length of the statement  "		_etw.ProtocolMetadataResponse (response.ToString ()' broker != null ? broker.Host : ""' broker != null ? broker.Port : -1' broker != null ? broker.NodeId : -1); " is 160.
Long Statement,kafka4net.Protocols,Protocol,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Protocol.cs,GetOffsets,The length of the statement  "	var response = await conn.Correlation.SendAndCorrelateAsync (id => Serializer.Serialize (req' id)' Serializer.DeserializeOffsetResponse' tcp' CancellationToken.None); " is 166.
Long Statement,kafka4net.Protocols,Protocol,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Protocol.cs,Fetch,The length of the statement  "	var response = await conn.Correlation.SendAndCorrelateAsync (id => Serializer.Serialize (req' id)' Serializer.DeserializeFetchResponse' tcp' /*cancel.Token*/CancellationToken.None); " is 181.
Long Statement,kafka4net.Protocols.Requests,FetchRequest,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Requests\FetchRequest.cs,ToString,The length of the statement  "	return string.Format ("MaxTime: {0} MinBytes: {1} [{2}]"' MaxWaitTime' MinBytes' Topics == null ? "null" : string.Join ("'"' Topics.AsEnumerable ())); " is 150.
Long Statement,kafka4net.Protocols.Requests,OffsetRequest,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Requests\OffsetRequest.cs,ToString,The length of the statement  "	return string.Format ("{0} Id:Time:MaxNumOffsets [{1}]"' TopicName' Partitions == null ? "null" : string.Join ("\n "' Partitions.AsEnumerable ())); " is 147.
Long Statement,kafka4net.Protocols.Responses,MetadataResponse,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Responses\MetadataResponse.cs,ToString,The length of the statement  "	return string.Format ("Brokers: [{0}]' TopicMeta: [{1}]"' Brokers == null ? "null" : string.Join ("'"' Brokers.AsEnumerable ())' Topics == null ? "null" : string.Join ("\n"' Topics.AsEnumerable ())); " is 199.
Long Statement,kafka4net.Protocols.Responses,OffsetResponse,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Responses\OffsetResponse.cs,ToString,The length of the statement  "	return string.Format ("'{0}' [{1}]"' TopicName' Partitions == null ? "null" : string.Join ("'"' Partitions.Select (p => p.ToString ()))); " is 137.
Empty Catch Block,kafka4net,Connection,D:\newReposJune17\ntent-ad_kafka4net\src\Connection.cs,GetClientAsync,The method has an empty catch block.
Empty Catch Block,kafka4net,Connection,D:\newReposJune17\ntent-ad_kafka4net\src\Connection.cs,MarkSocketAsFailed,The method has an empty catch block.
Empty Catch Block,kafka4net,Connection,D:\newReposJune17\ntent-ad_kafka4net\src\Connection.cs,ShutdownAsync,The method has an empty catch block.
Empty Catch Block,kafka4net,Connection,D:\newReposJune17\ntent-ad_kafka4net\src\Connection.cs,ShutdownAsync,The method has an empty catch block.
Empty Catch Block,kafka4net,Consumer,D:\newReposJune17\ntent-ad_kafka4net\src\Consumer.cs,Dispose,The method has an empty catch block.
Empty Catch Block,kafka4net,Consumer,D:\newReposJune17\ntent-ad_kafka4net\src\Consumer.cs,CloseAsync,The method has an empty catch block.
Empty Catch Block,kafka4net,Cluster,D:\newReposJune17\ntent-ad_kafka4net\src\Cluster.cs,CloseAsyncImpl,The method has an empty catch block.
Empty Catch Block,kafka4net,Cluster,D:\newReposJune17\ntent-ad_kafka4net\src\Cluster.cs,CloseAsyncImpl,The method has an empty catch block.
Magic Number,kafka4net,FletcherHashedMessagePartitioner,D:\newReposJune17\ntent-ad_kafka4net\src\FletcherHashedMessagePartitioner.cs,Fletcher32HashOptimized,The following statement contains a magic number: while (words != 0) {  	var tlen = words > 359 ? 359 : words;  	words -= tlen;  	do {  		sum2 += sum1 += msg [i++];  	} while (--tlen != 0);  	sum1 = (sum1 & 0xffff) + (sum1 >> 16);  	sum2 = (sum2 & 0xffff) + (sum2 >> 16);  }  
Magic Number,kafka4net,FletcherHashedMessagePartitioner,D:\newReposJune17\ntent-ad_kafka4net\src\FletcherHashedMessagePartitioner.cs,Fletcher32HashOptimized,The following statement contains a magic number: while (words != 0) {  	var tlen = words > 359 ? 359 : words;  	words -= tlen;  	do {  		sum2 += sum1 += msg [i++];  	} while (--tlen != 0);  	sum1 = (sum1 & 0xffff) + (sum1 >> 16);  	sum2 = (sum2 & 0xffff) + (sum2 >> 16);  }  
Magic Number,kafka4net,FletcherHashedMessagePartitioner,D:\newReposJune17\ntent-ad_kafka4net\src\FletcherHashedMessagePartitioner.cs,Fletcher32HashOptimized,The following statement contains a magic number: while (words != 0) {  	var tlen = words > 359 ? 359 : words;  	words -= tlen;  	do {  		sum2 += sum1 += msg [i++];  	} while (--tlen != 0);  	sum1 = (sum1 & 0xffff) + (sum1 >> 16);  	sum2 = (sum2 & 0xffff) + (sum2 >> 16);  }  
Magic Number,kafka4net,FletcherHashedMessagePartitioner,D:\newReposJune17\ntent-ad_kafka4net\src\FletcherHashedMessagePartitioner.cs,Fletcher32HashOptimized,The following statement contains a magic number: while (words != 0) {  	var tlen = words > 359 ? 359 : words;  	words -= tlen;  	do {  		sum2 += sum1 += msg [i++];  	} while (--tlen != 0);  	sum1 = (sum1 & 0xffff) + (sum1 >> 16);  	sum2 = (sum2 & 0xffff) + (sum2 >> 16);  }  
Magic Number,kafka4net,FletcherHashedMessagePartitioner,D:\newReposJune17\ntent-ad_kafka4net\src\FletcherHashedMessagePartitioner.cs,Fletcher32HashOptimized,The following statement contains a magic number: sum1 = (sum1 & 0xffff) + (sum1 >> 16);  
Magic Number,kafka4net,FletcherHashedMessagePartitioner,D:\newReposJune17\ntent-ad_kafka4net\src\FletcherHashedMessagePartitioner.cs,Fletcher32HashOptimized,The following statement contains a magic number: sum2 = (sum2 & 0xffff) + (sum2 >> 16);  
Magic Number,kafka4net,FletcherHashedMessagePartitioner,D:\newReposJune17\ntent-ad_kafka4net\src\FletcherHashedMessagePartitioner.cs,Fletcher32HashOptimized,The following statement contains a magic number: sum1 = (sum1 & 0xffff) + (sum1 >> 16);  
Magic Number,kafka4net,FletcherHashedMessagePartitioner,D:\newReposJune17\ntent-ad_kafka4net\src\FletcherHashedMessagePartitioner.cs,Fletcher32HashOptimized,The following statement contains a magic number: sum2 = (sum2 & 0xffff) + (sum2 >> 16);  
Magic Number,kafka4net,FletcherHashedMessagePartitioner,D:\newReposJune17\ntent-ad_kafka4net\src\FletcherHashedMessagePartitioner.cs,Fletcher32HashOptimized,The following statement contains a magic number: return (sum2 << 16 | sum1);  
Magic Number,kafka4net,ProducerConfiguration,D:\newReposJune17\ntent-ad_kafka4net\src\ProducerConfiguration.cs,ProducerConfiguration,The following statement contains a magic number: BatchFlushTime = batchFlushTime ?? TimeSpan.FromMilliseconds (500);  
Magic Number,kafka4net,Connection,D:\newReposJune17\ntent-ad_kafka4net\src\Connection.cs,ParseAddress,The following statement contains a magic number: return seedConnections.Split (''').Select (_ => _.Trim ()).Where (_ => _ != null).Select (s => {  	int port = 9092;  	string host = null;  	if (s.Contains (':')) {  		var parts = s.Split (new[] {  			":"  		}' StringSplitOptions.RemoveEmptyEntries);  		if (parts.Length == 2) {  			host = parts [0];  			port = int.Parse (parts [1]);  		}  	} else {  		host = s;  	}  	return Tuple.Create (host' port);  }).ToArray ();  
Magic Number,kafka4net,Connection,D:\newReposJune17\ntent-ad_kafka4net\src\Connection.cs,ParseAddress,The following statement contains a magic number: return seedConnections.Split (''').Select (_ => _.Trim ()).Where (_ => _ != null).Select (s => {  	int port = 9092;  	string host = null;  	if (s.Contains (':')) {  		var parts = s.Split (new[] {  			":"  		}' StringSplitOptions.RemoveEmptyEntries);  		if (parts.Length == 2) {  			host = parts [0];  			port = int.Parse (parts [1]);  		}  	} else {  		host = s;  	}  	return Tuple.Create (host' port);  }).ToArray ();  
Magic Number,kafka4net,Connection,D:\newReposJune17\ntent-ad_kafka4net\src\Connection.cs,ParseAddress,The following statement contains a magic number: if (s.Contains (':')) {  	var parts = s.Split (new[] {  		":"  	}' StringSplitOptions.RemoveEmptyEntries);  	if (parts.Length == 2) {  		host = parts [0];  		port = int.Parse (parts [1]);  	}  } else {  	host = s;  }  
Magic Number,kafka4net,Connection,D:\newReposJune17\ntent-ad_kafka4net\src\Connection.cs,ParseAddress,The following statement contains a magic number: if (parts.Length == 2) {  	host = parts [0];  	port = int.Parse (parts [1]);  }  
Magic Number,kafka4net,Consumer,D:\newReposJune17\ntent-ad_kafka4net\src\Consumer.cs,Dispose,The following statement contains a magic number: try {  	// close and release the connections in the Cluster.  	if (_cluster != null && _cluster.State != Cluster.ClusterState.Disconnected)  		_cluster.CloseAsync (TimeSpan.FromSeconds (5)).ContinueWith (t => _log.Error (t.Exception' "Error when closing Cluster")' TaskContinuationOptions.OnlyOnFaulted);  } // ReSharper disable once EmptyGeneralCatchClause  catch (Exception e) {  	_log.Error (e' "Error in Dispose");  }  
Magic Number,kafka4net,Consumer,D:\newReposJune17\ntent-ad_kafka4net\src\Consumer.cs,Dispose,The following statement contains a magic number: if (_cluster != null && _cluster.State != Cluster.ClusterState.Disconnected)  	_cluster.CloseAsync (TimeSpan.FromSeconds (5)).ContinueWith (t => _log.Error (t.Exception' "Error when closing Cluster")' TaskContinuationOptions.OnlyOnFaulted);  
Magic Number,kafka4net,Consumer,D:\newReposJune17\ntent-ad_kafka4net\src\Consumer.cs,Dispose,The following statement contains a magic number: _cluster.CloseAsync (TimeSpan.FromSeconds (5)).ContinueWith (t => _log.Error (t.Exception' "Error when closing Cluster")' TaskContinuationOptions.OnlyOnFaulted);  
Magic Number,kafka4net,Consumer,D:\newReposJune17\ntent-ad_kafka4net\src\Consumer.cs,CloseAsync,The following statement contains a magic number: try {  	// close and release the connections in the Cluster.  	if (_cluster != null && _cluster.State != Cluster.ClusterState.Disconnected)  		await _cluster.CloseAsync (TimeSpan.FromSeconds (5));  } // ReSharper disable once EmptyGeneralCatchClause  catch (Exception e) {  	_log.Error (e' "Error when closing Cluster");  }  
Magic Number,kafka4net,Consumer,D:\newReposJune17\ntent-ad_kafka4net\src\Consumer.cs,CloseAsync,The following statement contains a magic number: if (_cluster != null && _cluster.State != Cluster.ClusterState.Disconnected)  	await _cluster.CloseAsync (TimeSpan.FromSeconds (5));  
Magic Number,kafka4net,Consumer,D:\newReposJune17\ntent-ad_kafka4net\src\Consumer.cs,CloseAsync,The following statement contains a magic number: await _cluster.CloseAsync (TimeSpan.FromSeconds (5));  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The following statement contains a magic number: try {  	await await _cluster.Scheduler.Ask (async () => {  		await _sync.WaitAsync ();  		try {  			if (IsConnected)  				return;  			_log.Debug ("Connecting producer {1} for topic {0}"' Topic' _id);  			EtwTrace.Log.ProducerStarting (Topic' _id);  			_sendMessagesSubject = new Subject<Message> ();  			if (_cluster.State != Cluster.ClusterState.Connected) {  				_log.Debug ("Connecting cluster");  				await _cluster.ConnectAsync ();  				_log.Debug ("Connected cluster");  			}  			// Recovery: subscribe to partition offline/online events  			_partitionStateSubsctiption = _cluster.PartitionStateChanges.Where (p => p.Topic == Configuration.Topic).Synchronize (_allPartitionQueues).Subscribe (p => {  				PartitionQueueInfo queue;  				if (!_allPartitionQueues.TryGetValue (p.PartitionId' out queue)) {  					queue = new PartitionQueueInfo (Configuration.SendBuffersInitialSize) {  						Partition = p.PartitionId  					};  					_allPartitionQueues.Add (p.PartitionId' queue);  					_queueSizeEvents.OnNext (new QueueResizeInfo {  						PartitionId = p.PartitionId'  						Size = queue.Queue.Size'  						Capacity = queue.Queue.Capacity  					});  				}  				_log.Info ("Detected change in topic/partition '{0}'/{1}/{2} IsOnline {3}->{4}"' Configuration.Topic' p.PartitionId' p.ErrorCode' queue.IsOnline' p.ErrorCode.IsSuccess ());  				queue.IsOnline = p.ErrorCode.IsSuccess ();  				_queueEventWaitHandler.Set ();  			});  			// get all of the partitions for this topic. Allows the MessagePartitioner to select a partition.  			var topicPartitions = await _cluster.GetOrFetchMetaForTopicAsync (Configuration.Topic);  			_log.Debug ("Producer found {0} partitions for '{1}'"' topicPartitions.Length' Configuration.Topic);  			_sendMessagesSubject.Do (msg => msg.PartitionId = Configuration.Partitioner.GetMessagePartition (msg' topicPartitions).Id).Buffer (Configuration.BatchFlushTime' Configuration.BatchFlushSize).Where (b => b.Count > 0).Select (msgs => msgs.GroupBy (msg => msg.PartitionId)).ObserveOn (_cluster.Scheduler).Subscribe (partitionGroups => {  				foreach (var batch in partitionGroups) {  					// partition queue might be created if metadata broadcast fired already  					PartitionQueueInfo queue;  					if (!_allPartitionQueues.TryGetValue (batch.Key' out queue)) {  						queue = new PartitionQueueInfo (Configuration.SendBuffersInitialSize) {  							Partition = batch.Key  						};  						_allPartitionQueues.Add (batch.Key' queue);  						queue.IsOnline = topicPartitions.First (p => p.Id == batch.Key).ErrorCode.IsSuccess ();  						_log.Debug ("{0} added new partition queue"' this);  						_queueSizeEvents.OnNext (new QueueResizeInfo {  							PartitionId = queue.Partition'  							Size = queue.Queue.Size'  							Capacity = queue.Queue.Capacity  						});  					}  					// now queue them up  					var batchAr = batch.ToArray ();  					// make sure we have space.  					if (queue.Queue.Size + batchAr.Length > queue.Queue.Capacity) {  						// try to increase the capacity.  						if (Configuration.AutoGrowSendBuffers) {  							var growBy = Math.Max (queue.Queue.Capacity / 2 + 1' 2 * batchAr.Length);  							_log.Warn ("Capacity of send buffer with size {3} not large enough to accept {2} new messages. Increasing capacity from {0} to {1}"' queue.Queue.Capacity' queue.Queue.Capacity + growBy' batchAr.Length' queue.Queue.Size);  							queue.Queue.Capacity += growBy;  							_queueSizeEvents.OnNext (new QueueResizeInfo {  								PartitionId = queue.Partition'  								Size = queue.Queue.Size'  								Capacity = queue.Queue.Capacity  							});  						} else {  							// we're full and not allowed to grow. Throw the batch back to the caller' and continue on.  							var msg = string.Format ("Send Buffer Full for partition {0}. "' Cluster.PartitionStateChanges.Where (ps => ps.Topic == Configuration.Topic && ps.PartitionId == queue.Partition).Take (1).Wait ());  							_log.Error (msg);  							if (OnPermError != null)  								Task.Factory.StartNew (() => OnPermError (new Exception (msg)' batchAr));  							continue;  						}  					}  					// we have the space' add to the queue  					queue.Queue.Put (batchAr);  					if (_log.IsDebugEnabled)  						_log.Debug ("Enqueued batch of size {0} for topic '{1}' partition {2}"' batchAr.Length' Configuration.Topic' batch.Key);  					// After batch enqueued' send wake up signal to sending queue  					_queueEventWaitHandler.Set ();  				}  			}' e => _log.Fatal (e' "Error in _sendMessagesSubject pipeline")' () => _log.Debug ("_sendMessagesSubject complete"));  			// start the send loop task  			_cluster.Scheduler.Schedule (() => {  				_sendLoopTask = SendLoop ().ContinueWith (t => {  					if (t.IsFaulted)  						_log.Fatal (t.Exception' "SendLoop failed");  					else  						_log.Debug ("SendLoop complete with status: {0}"' t.Status);  				});  			});  			_log.Debug ("Connected");  			EtwTrace.Log.ProducerStarted (Topic' _id);  		} catch (Exception e) {  			_log.Error (e' "Exception during connect");  			EtwTrace.Log.ProducerError (e.Message' _id);  			throw;  		} finally {  			_log.Debug ("#{0} Releasing Producer Semaphore."' _id);  			_sync.Release ();  		}  	}).ConfigureAwait (false);  } finally {  	SynchronizationContext.SetSynchronizationContext (oldctx);  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The following statement contains a magic number: try {  	await await _cluster.Scheduler.Ask (async () => {  		await _sync.WaitAsync ();  		try {  			if (IsConnected)  				return;  			_log.Debug ("Connecting producer {1} for topic {0}"' Topic' _id);  			EtwTrace.Log.ProducerStarting (Topic' _id);  			_sendMessagesSubject = new Subject<Message> ();  			if (_cluster.State != Cluster.ClusterState.Connected) {  				_log.Debug ("Connecting cluster");  				await _cluster.ConnectAsync ();  				_log.Debug ("Connected cluster");  			}  			// Recovery: subscribe to partition offline/online events  			_partitionStateSubsctiption = _cluster.PartitionStateChanges.Where (p => p.Topic == Configuration.Topic).Synchronize (_allPartitionQueues).Subscribe (p => {  				PartitionQueueInfo queue;  				if (!_allPartitionQueues.TryGetValue (p.PartitionId' out queue)) {  					queue = new PartitionQueueInfo (Configuration.SendBuffersInitialSize) {  						Partition = p.PartitionId  					};  					_allPartitionQueues.Add (p.PartitionId' queue);  					_queueSizeEvents.OnNext (new QueueResizeInfo {  						PartitionId = p.PartitionId'  						Size = queue.Queue.Size'  						Capacity = queue.Queue.Capacity  					});  				}  				_log.Info ("Detected change in topic/partition '{0}'/{1}/{2} IsOnline {3}->{4}"' Configuration.Topic' p.PartitionId' p.ErrorCode' queue.IsOnline' p.ErrorCode.IsSuccess ());  				queue.IsOnline = p.ErrorCode.IsSuccess ();  				_queueEventWaitHandler.Set ();  			});  			// get all of the partitions for this topic. Allows the MessagePartitioner to select a partition.  			var topicPartitions = await _cluster.GetOrFetchMetaForTopicAsync (Configuration.Topic);  			_log.Debug ("Producer found {0} partitions for '{1}'"' topicPartitions.Length' Configuration.Topic);  			_sendMessagesSubject.Do (msg => msg.PartitionId = Configuration.Partitioner.GetMessagePartition (msg' topicPartitions).Id).Buffer (Configuration.BatchFlushTime' Configuration.BatchFlushSize).Where (b => b.Count > 0).Select (msgs => msgs.GroupBy (msg => msg.PartitionId)).ObserveOn (_cluster.Scheduler).Subscribe (partitionGroups => {  				foreach (var batch in partitionGroups) {  					// partition queue might be created if metadata broadcast fired already  					PartitionQueueInfo queue;  					if (!_allPartitionQueues.TryGetValue (batch.Key' out queue)) {  						queue = new PartitionQueueInfo (Configuration.SendBuffersInitialSize) {  							Partition = batch.Key  						};  						_allPartitionQueues.Add (batch.Key' queue);  						queue.IsOnline = topicPartitions.First (p => p.Id == batch.Key).ErrorCode.IsSuccess ();  						_log.Debug ("{0} added new partition queue"' this);  						_queueSizeEvents.OnNext (new QueueResizeInfo {  							PartitionId = queue.Partition'  							Size = queue.Queue.Size'  							Capacity = queue.Queue.Capacity  						});  					}  					// now queue them up  					var batchAr = batch.ToArray ();  					// make sure we have space.  					if (queue.Queue.Size + batchAr.Length > queue.Queue.Capacity) {  						// try to increase the capacity.  						if (Configuration.AutoGrowSendBuffers) {  							var growBy = Math.Max (queue.Queue.Capacity / 2 + 1' 2 * batchAr.Length);  							_log.Warn ("Capacity of send buffer with size {3} not large enough to accept {2} new messages. Increasing capacity from {0} to {1}"' queue.Queue.Capacity' queue.Queue.Capacity + growBy' batchAr.Length' queue.Queue.Size);  							queue.Queue.Capacity += growBy;  							_queueSizeEvents.OnNext (new QueueResizeInfo {  								PartitionId = queue.Partition'  								Size = queue.Queue.Size'  								Capacity = queue.Queue.Capacity  							});  						} else {  							// we're full and not allowed to grow. Throw the batch back to the caller' and continue on.  							var msg = string.Format ("Send Buffer Full for partition {0}. "' Cluster.PartitionStateChanges.Where (ps => ps.Topic == Configuration.Topic && ps.PartitionId == queue.Partition).Take (1).Wait ());  							_log.Error (msg);  							if (OnPermError != null)  								Task.Factory.StartNew (() => OnPermError (new Exception (msg)' batchAr));  							continue;  						}  					}  					// we have the space' add to the queue  					queue.Queue.Put (batchAr);  					if (_log.IsDebugEnabled)  						_log.Debug ("Enqueued batch of size {0} for topic '{1}' partition {2}"' batchAr.Length' Configuration.Topic' batch.Key);  					// After batch enqueued' send wake up signal to sending queue  					_queueEventWaitHandler.Set ();  				}  			}' e => _log.Fatal (e' "Error in _sendMessagesSubject pipeline")' () => _log.Debug ("_sendMessagesSubject complete"));  			// start the send loop task  			_cluster.Scheduler.Schedule (() => {  				_sendLoopTask = SendLoop ().ContinueWith (t => {  					if (t.IsFaulted)  						_log.Fatal (t.Exception' "SendLoop failed");  					else  						_log.Debug ("SendLoop complete with status: {0}"' t.Status);  				});  			});  			_log.Debug ("Connected");  			EtwTrace.Log.ProducerStarted (Topic' _id);  		} catch (Exception e) {  			_log.Error (e' "Exception during connect");  			EtwTrace.Log.ProducerError (e.Message' _id);  			throw;  		} finally {  			_log.Debug ("#{0} Releasing Producer Semaphore."' _id);  			_sync.Release ();  		}  	}).ConfigureAwait (false);  } finally {  	SynchronizationContext.SetSynchronizationContext (oldctx);  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The following statement contains a magic number: await await _cluster.Scheduler.Ask (async () => {  	await _sync.WaitAsync ();  	try {  		if (IsConnected)  			return;  		_log.Debug ("Connecting producer {1} for topic {0}"' Topic' _id);  		EtwTrace.Log.ProducerStarting (Topic' _id);  		_sendMessagesSubject = new Subject<Message> ();  		if (_cluster.State != Cluster.ClusterState.Connected) {  			_log.Debug ("Connecting cluster");  			await _cluster.ConnectAsync ();  			_log.Debug ("Connected cluster");  		}  		// Recovery: subscribe to partition offline/online events  		_partitionStateSubsctiption = _cluster.PartitionStateChanges.Where (p => p.Topic == Configuration.Topic).Synchronize (_allPartitionQueues).Subscribe (p => {  			PartitionQueueInfo queue;  			if (!_allPartitionQueues.TryGetValue (p.PartitionId' out queue)) {  				queue = new PartitionQueueInfo (Configuration.SendBuffersInitialSize) {  					Partition = p.PartitionId  				};  				_allPartitionQueues.Add (p.PartitionId' queue);  				_queueSizeEvents.OnNext (new QueueResizeInfo {  					PartitionId = p.PartitionId'  					Size = queue.Queue.Size'  					Capacity = queue.Queue.Capacity  				});  			}  			_log.Info ("Detected change in topic/partition '{0}'/{1}/{2} IsOnline {3}->{4}"' Configuration.Topic' p.PartitionId' p.ErrorCode' queue.IsOnline' p.ErrorCode.IsSuccess ());  			queue.IsOnline = p.ErrorCode.IsSuccess ();  			_queueEventWaitHandler.Set ();  		});  		// get all of the partitions for this topic. Allows the MessagePartitioner to select a partition.  		var topicPartitions = await _cluster.GetOrFetchMetaForTopicAsync (Configuration.Topic);  		_log.Debug ("Producer found {0} partitions for '{1}'"' topicPartitions.Length' Configuration.Topic);  		_sendMessagesSubject.Do (msg => msg.PartitionId = Configuration.Partitioner.GetMessagePartition (msg' topicPartitions).Id).Buffer (Configuration.BatchFlushTime' Configuration.BatchFlushSize).Where (b => b.Count > 0).Select (msgs => msgs.GroupBy (msg => msg.PartitionId)).ObserveOn (_cluster.Scheduler).Subscribe (partitionGroups => {  			foreach (var batch in partitionGroups) {  				// partition queue might be created if metadata broadcast fired already  				PartitionQueueInfo queue;  				if (!_allPartitionQueues.TryGetValue (batch.Key' out queue)) {  					queue = new PartitionQueueInfo (Configuration.SendBuffersInitialSize) {  						Partition = batch.Key  					};  					_allPartitionQueues.Add (batch.Key' queue);  					queue.IsOnline = topicPartitions.First (p => p.Id == batch.Key).ErrorCode.IsSuccess ();  					_log.Debug ("{0} added new partition queue"' this);  					_queueSizeEvents.OnNext (new QueueResizeInfo {  						PartitionId = queue.Partition'  						Size = queue.Queue.Size'  						Capacity = queue.Queue.Capacity  					});  				}  				// now queue them up  				var batchAr = batch.ToArray ();  				// make sure we have space.  				if (queue.Queue.Size + batchAr.Length > queue.Queue.Capacity) {  					// try to increase the capacity.  					if (Configuration.AutoGrowSendBuffers) {  						var growBy = Math.Max (queue.Queue.Capacity / 2 + 1' 2 * batchAr.Length);  						_log.Warn ("Capacity of send buffer with size {3} not large enough to accept {2} new messages. Increasing capacity from {0} to {1}"' queue.Queue.Capacity' queue.Queue.Capacity + growBy' batchAr.Length' queue.Queue.Size);  						queue.Queue.Capacity += growBy;  						_queueSizeEvents.OnNext (new QueueResizeInfo {  							PartitionId = queue.Partition'  							Size = queue.Queue.Size'  							Capacity = queue.Queue.Capacity  						});  					} else {  						// we're full and not allowed to grow. Throw the batch back to the caller' and continue on.  						var msg = string.Format ("Send Buffer Full for partition {0}. "' Cluster.PartitionStateChanges.Where (ps => ps.Topic == Configuration.Topic && ps.PartitionId == queue.Partition).Take (1).Wait ());  						_log.Error (msg);  						if (OnPermError != null)  							Task.Factory.StartNew (() => OnPermError (new Exception (msg)' batchAr));  						continue;  					}  				}  				// we have the space' add to the queue  				queue.Queue.Put (batchAr);  				if (_log.IsDebugEnabled)  					_log.Debug ("Enqueued batch of size {0} for topic '{1}' partition {2}"' batchAr.Length' Configuration.Topic' batch.Key);  				// After batch enqueued' send wake up signal to sending queue  				_queueEventWaitHandler.Set ();  			}  		}' e => _log.Fatal (e' "Error in _sendMessagesSubject pipeline")' () => _log.Debug ("_sendMessagesSubject complete"));  		// start the send loop task  		_cluster.Scheduler.Schedule (() => {  			_sendLoopTask = SendLoop ().ContinueWith (t => {  				if (t.IsFaulted)  					_log.Fatal (t.Exception' "SendLoop failed");  				else  					_log.Debug ("SendLoop complete with status: {0}"' t.Status);  			});  		});  		_log.Debug ("Connected");  		EtwTrace.Log.ProducerStarted (Topic' _id);  	} catch (Exception e) {  		_log.Error (e' "Exception during connect");  		EtwTrace.Log.ProducerError (e.Message' _id);  		throw;  	} finally {  		_log.Debug ("#{0} Releasing Producer Semaphore."' _id);  		_sync.Release ();  	}  }).ConfigureAwait (false);  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The following statement contains a magic number: await await _cluster.Scheduler.Ask (async () => {  	await _sync.WaitAsync ();  	try {  		if (IsConnected)  			return;  		_log.Debug ("Connecting producer {1} for topic {0}"' Topic' _id);  		EtwTrace.Log.ProducerStarting (Topic' _id);  		_sendMessagesSubject = new Subject<Message> ();  		if (_cluster.State != Cluster.ClusterState.Connected) {  			_log.Debug ("Connecting cluster");  			await _cluster.ConnectAsync ();  			_log.Debug ("Connected cluster");  		}  		// Recovery: subscribe to partition offline/online events  		_partitionStateSubsctiption = _cluster.PartitionStateChanges.Where (p => p.Topic == Configuration.Topic).Synchronize (_allPartitionQueues).Subscribe (p => {  			PartitionQueueInfo queue;  			if (!_allPartitionQueues.TryGetValue (p.PartitionId' out queue)) {  				queue = new PartitionQueueInfo (Configuration.SendBuffersInitialSize) {  					Partition = p.PartitionId  				};  				_allPartitionQueues.Add (p.PartitionId' queue);  				_queueSizeEvents.OnNext (new QueueResizeInfo {  					PartitionId = p.PartitionId'  					Size = queue.Queue.Size'  					Capacity = queue.Queue.Capacity  				});  			}  			_log.Info ("Detected change in topic/partition '{0}'/{1}/{2} IsOnline {3}->{4}"' Configuration.Topic' p.PartitionId' p.ErrorCode' queue.IsOnline' p.ErrorCode.IsSuccess ());  			queue.IsOnline = p.ErrorCode.IsSuccess ();  			_queueEventWaitHandler.Set ();  		});  		// get all of the partitions for this topic. Allows the MessagePartitioner to select a partition.  		var topicPartitions = await _cluster.GetOrFetchMetaForTopicAsync (Configuration.Topic);  		_log.Debug ("Producer found {0} partitions for '{1}'"' topicPartitions.Length' Configuration.Topic);  		_sendMessagesSubject.Do (msg => msg.PartitionId = Configuration.Partitioner.GetMessagePartition (msg' topicPartitions).Id).Buffer (Configuration.BatchFlushTime' Configuration.BatchFlushSize).Where (b => b.Count > 0).Select (msgs => msgs.GroupBy (msg => msg.PartitionId)).ObserveOn (_cluster.Scheduler).Subscribe (partitionGroups => {  			foreach (var batch in partitionGroups) {  				// partition queue might be created if metadata broadcast fired already  				PartitionQueueInfo queue;  				if (!_allPartitionQueues.TryGetValue (batch.Key' out queue)) {  					queue = new PartitionQueueInfo (Configuration.SendBuffersInitialSize) {  						Partition = batch.Key  					};  					_allPartitionQueues.Add (batch.Key' queue);  					queue.IsOnline = topicPartitions.First (p => p.Id == batch.Key).ErrorCode.IsSuccess ();  					_log.Debug ("{0} added new partition queue"' this);  					_queueSizeEvents.OnNext (new QueueResizeInfo {  						PartitionId = queue.Partition'  						Size = queue.Queue.Size'  						Capacity = queue.Queue.Capacity  					});  				}  				// now queue them up  				var batchAr = batch.ToArray ();  				// make sure we have space.  				if (queue.Queue.Size + batchAr.Length > queue.Queue.Capacity) {  					// try to increase the capacity.  					if (Configuration.AutoGrowSendBuffers) {  						var growBy = Math.Max (queue.Queue.Capacity / 2 + 1' 2 * batchAr.Length);  						_log.Warn ("Capacity of send buffer with size {3} not large enough to accept {2} new messages. Increasing capacity from {0} to {1}"' queue.Queue.Capacity' queue.Queue.Capacity + growBy' batchAr.Length' queue.Queue.Size);  						queue.Queue.Capacity += growBy;  						_queueSizeEvents.OnNext (new QueueResizeInfo {  							PartitionId = queue.Partition'  							Size = queue.Queue.Size'  							Capacity = queue.Queue.Capacity  						});  					} else {  						// we're full and not allowed to grow. Throw the batch back to the caller' and continue on.  						var msg = string.Format ("Send Buffer Full for partition {0}. "' Cluster.PartitionStateChanges.Where (ps => ps.Topic == Configuration.Topic && ps.PartitionId == queue.Partition).Take (1).Wait ());  						_log.Error (msg);  						if (OnPermError != null)  							Task.Factory.StartNew (() => OnPermError (new Exception (msg)' batchAr));  						continue;  					}  				}  				// we have the space' add to the queue  				queue.Queue.Put (batchAr);  				if (_log.IsDebugEnabled)  					_log.Debug ("Enqueued batch of size {0} for topic '{1}' partition {2}"' batchAr.Length' Configuration.Topic' batch.Key);  				// After batch enqueued' send wake up signal to sending queue  				_queueEventWaitHandler.Set ();  			}  		}' e => _log.Fatal (e' "Error in _sendMessagesSubject pipeline")' () => _log.Debug ("_sendMessagesSubject complete"));  		// start the send loop task  		_cluster.Scheduler.Schedule (() => {  			_sendLoopTask = SendLoop ().ContinueWith (t => {  				if (t.IsFaulted)  					_log.Fatal (t.Exception' "SendLoop failed");  				else  					_log.Debug ("SendLoop complete with status: {0}"' t.Status);  			});  		});  		_log.Debug ("Connected");  		EtwTrace.Log.ProducerStarted (Topic' _id);  	} catch (Exception e) {  		_log.Error (e' "Exception during connect");  		EtwTrace.Log.ProducerError (e.Message' _id);  		throw;  	} finally {  		_log.Debug ("#{0} Releasing Producer Semaphore."' _id);  		_sync.Release ();  	}  }).ConfigureAwait (false);  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The following statement contains a magic number: try {  	if (IsConnected)  		return;  	_log.Debug ("Connecting producer {1} for topic {0}"' Topic' _id);  	EtwTrace.Log.ProducerStarting (Topic' _id);  	_sendMessagesSubject = new Subject<Message> ();  	if (_cluster.State != Cluster.ClusterState.Connected) {  		_log.Debug ("Connecting cluster");  		await _cluster.ConnectAsync ();  		_log.Debug ("Connected cluster");  	}  	// Recovery: subscribe to partition offline/online events  	_partitionStateSubsctiption = _cluster.PartitionStateChanges.Where (p => p.Topic == Configuration.Topic).Synchronize (_allPartitionQueues).Subscribe (p => {  		PartitionQueueInfo queue;  		if (!_allPartitionQueues.TryGetValue (p.PartitionId' out queue)) {  			queue = new PartitionQueueInfo (Configuration.SendBuffersInitialSize) {  				Partition = p.PartitionId  			};  			_allPartitionQueues.Add (p.PartitionId' queue);  			_queueSizeEvents.OnNext (new QueueResizeInfo {  				PartitionId = p.PartitionId'  				Size = queue.Queue.Size'  				Capacity = queue.Queue.Capacity  			});  		}  		_log.Info ("Detected change in topic/partition '{0}'/{1}/{2} IsOnline {3}->{4}"' Configuration.Topic' p.PartitionId' p.ErrorCode' queue.IsOnline' p.ErrorCode.IsSuccess ());  		queue.IsOnline = p.ErrorCode.IsSuccess ();  		_queueEventWaitHandler.Set ();  	});  	// get all of the partitions for this topic. Allows the MessagePartitioner to select a partition.  	var topicPartitions = await _cluster.GetOrFetchMetaForTopicAsync (Configuration.Topic);  	_log.Debug ("Producer found {0} partitions for '{1}'"' topicPartitions.Length' Configuration.Topic);  	_sendMessagesSubject.Do (msg => msg.PartitionId = Configuration.Partitioner.GetMessagePartition (msg' topicPartitions).Id).Buffer (Configuration.BatchFlushTime' Configuration.BatchFlushSize).Where (b => b.Count > 0).Select (msgs => msgs.GroupBy (msg => msg.PartitionId)).ObserveOn (_cluster.Scheduler).Subscribe (partitionGroups => {  		foreach (var batch in partitionGroups) {  			// partition queue might be created if metadata broadcast fired already  			PartitionQueueInfo queue;  			if (!_allPartitionQueues.TryGetValue (batch.Key' out queue)) {  				queue = new PartitionQueueInfo (Configuration.SendBuffersInitialSize) {  					Partition = batch.Key  				};  				_allPartitionQueues.Add (batch.Key' queue);  				queue.IsOnline = topicPartitions.First (p => p.Id == batch.Key).ErrorCode.IsSuccess ();  				_log.Debug ("{0} added new partition queue"' this);  				_queueSizeEvents.OnNext (new QueueResizeInfo {  					PartitionId = queue.Partition'  					Size = queue.Queue.Size'  					Capacity = queue.Queue.Capacity  				});  			}  			// now queue them up  			var batchAr = batch.ToArray ();  			// make sure we have space.  			if (queue.Queue.Size + batchAr.Length > queue.Queue.Capacity) {  				// try to increase the capacity.  				if (Configuration.AutoGrowSendBuffers) {  					var growBy = Math.Max (queue.Queue.Capacity / 2 + 1' 2 * batchAr.Length);  					_log.Warn ("Capacity of send buffer with size {3} not large enough to accept {2} new messages. Increasing capacity from {0} to {1}"' queue.Queue.Capacity' queue.Queue.Capacity + growBy' batchAr.Length' queue.Queue.Size);  					queue.Queue.Capacity += growBy;  					_queueSizeEvents.OnNext (new QueueResizeInfo {  						PartitionId = queue.Partition'  						Size = queue.Queue.Size'  						Capacity = queue.Queue.Capacity  					});  				} else {  					// we're full and not allowed to grow. Throw the batch back to the caller' and continue on.  					var msg = string.Format ("Send Buffer Full for partition {0}. "' Cluster.PartitionStateChanges.Where (ps => ps.Topic == Configuration.Topic && ps.PartitionId == queue.Partition).Take (1).Wait ());  					_log.Error (msg);  					if (OnPermError != null)  						Task.Factory.StartNew (() => OnPermError (new Exception (msg)' batchAr));  					continue;  				}  			}  			// we have the space' add to the queue  			queue.Queue.Put (batchAr);  			if (_log.IsDebugEnabled)  				_log.Debug ("Enqueued batch of size {0} for topic '{1}' partition {2}"' batchAr.Length' Configuration.Topic' batch.Key);  			// After batch enqueued' send wake up signal to sending queue  			_queueEventWaitHandler.Set ();  		}  	}' e => _log.Fatal (e' "Error in _sendMessagesSubject pipeline")' () => _log.Debug ("_sendMessagesSubject complete"));  	// start the send loop task  	_cluster.Scheduler.Schedule (() => {  		_sendLoopTask = SendLoop ().ContinueWith (t => {  			if (t.IsFaulted)  				_log.Fatal (t.Exception' "SendLoop failed");  			else  				_log.Debug ("SendLoop complete with status: {0}"' t.Status);  		});  	});  	_log.Debug ("Connected");  	EtwTrace.Log.ProducerStarted (Topic' _id);  } catch (Exception e) {  	_log.Error (e' "Exception during connect");  	EtwTrace.Log.ProducerError (e.Message' _id);  	throw;  } finally {  	_log.Debug ("#{0} Releasing Producer Semaphore."' _id);  	_sync.Release ();  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The following statement contains a magic number: try {  	if (IsConnected)  		return;  	_log.Debug ("Connecting producer {1} for topic {0}"' Topic' _id);  	EtwTrace.Log.ProducerStarting (Topic' _id);  	_sendMessagesSubject = new Subject<Message> ();  	if (_cluster.State != Cluster.ClusterState.Connected) {  		_log.Debug ("Connecting cluster");  		await _cluster.ConnectAsync ();  		_log.Debug ("Connected cluster");  	}  	// Recovery: subscribe to partition offline/online events  	_partitionStateSubsctiption = _cluster.PartitionStateChanges.Where (p => p.Topic == Configuration.Topic).Synchronize (_allPartitionQueues).Subscribe (p => {  		PartitionQueueInfo queue;  		if (!_allPartitionQueues.TryGetValue (p.PartitionId' out queue)) {  			queue = new PartitionQueueInfo (Configuration.SendBuffersInitialSize) {  				Partition = p.PartitionId  			};  			_allPartitionQueues.Add (p.PartitionId' queue);  			_queueSizeEvents.OnNext (new QueueResizeInfo {  				PartitionId = p.PartitionId'  				Size = queue.Queue.Size'  				Capacity = queue.Queue.Capacity  			});  		}  		_log.Info ("Detected change in topic/partition '{0}'/{1}/{2} IsOnline {3}->{4}"' Configuration.Topic' p.PartitionId' p.ErrorCode' queue.IsOnline' p.ErrorCode.IsSuccess ());  		queue.IsOnline = p.ErrorCode.IsSuccess ();  		_queueEventWaitHandler.Set ();  	});  	// get all of the partitions for this topic. Allows the MessagePartitioner to select a partition.  	var topicPartitions = await _cluster.GetOrFetchMetaForTopicAsync (Configuration.Topic);  	_log.Debug ("Producer found {0} partitions for '{1}'"' topicPartitions.Length' Configuration.Topic);  	_sendMessagesSubject.Do (msg => msg.PartitionId = Configuration.Partitioner.GetMessagePartition (msg' topicPartitions).Id).Buffer (Configuration.BatchFlushTime' Configuration.BatchFlushSize).Where (b => b.Count > 0).Select (msgs => msgs.GroupBy (msg => msg.PartitionId)).ObserveOn (_cluster.Scheduler).Subscribe (partitionGroups => {  		foreach (var batch in partitionGroups) {  			// partition queue might be created if metadata broadcast fired already  			PartitionQueueInfo queue;  			if (!_allPartitionQueues.TryGetValue (batch.Key' out queue)) {  				queue = new PartitionQueueInfo (Configuration.SendBuffersInitialSize) {  					Partition = batch.Key  				};  				_allPartitionQueues.Add (batch.Key' queue);  				queue.IsOnline = topicPartitions.First (p => p.Id == batch.Key).ErrorCode.IsSuccess ();  				_log.Debug ("{0} added new partition queue"' this);  				_queueSizeEvents.OnNext (new QueueResizeInfo {  					PartitionId = queue.Partition'  					Size = queue.Queue.Size'  					Capacity = queue.Queue.Capacity  				});  			}  			// now queue them up  			var batchAr = batch.ToArray ();  			// make sure we have space.  			if (queue.Queue.Size + batchAr.Length > queue.Queue.Capacity) {  				// try to increase the capacity.  				if (Configuration.AutoGrowSendBuffers) {  					var growBy = Math.Max (queue.Queue.Capacity / 2 + 1' 2 * batchAr.Length);  					_log.Warn ("Capacity of send buffer with size {3} not large enough to accept {2} new messages. Increasing capacity from {0} to {1}"' queue.Queue.Capacity' queue.Queue.Capacity + growBy' batchAr.Length' queue.Queue.Size);  					queue.Queue.Capacity += growBy;  					_queueSizeEvents.OnNext (new QueueResizeInfo {  						PartitionId = queue.Partition'  						Size = queue.Queue.Size'  						Capacity = queue.Queue.Capacity  					});  				} else {  					// we're full and not allowed to grow. Throw the batch back to the caller' and continue on.  					var msg = string.Format ("Send Buffer Full for partition {0}. "' Cluster.PartitionStateChanges.Where (ps => ps.Topic == Configuration.Topic && ps.PartitionId == queue.Partition).Take (1).Wait ());  					_log.Error (msg);  					if (OnPermError != null)  						Task.Factory.StartNew (() => OnPermError (new Exception (msg)' batchAr));  					continue;  				}  			}  			// we have the space' add to the queue  			queue.Queue.Put (batchAr);  			if (_log.IsDebugEnabled)  				_log.Debug ("Enqueued batch of size {0} for topic '{1}' partition {2}"' batchAr.Length' Configuration.Topic' batch.Key);  			// After batch enqueued' send wake up signal to sending queue  			_queueEventWaitHandler.Set ();  		}  	}' e => _log.Fatal (e' "Error in _sendMessagesSubject pipeline")' () => _log.Debug ("_sendMessagesSubject complete"));  	// start the send loop task  	_cluster.Scheduler.Schedule (() => {  		_sendLoopTask = SendLoop ().ContinueWith (t => {  			if (t.IsFaulted)  				_log.Fatal (t.Exception' "SendLoop failed");  			else  				_log.Debug ("SendLoop complete with status: {0}"' t.Status);  		});  	});  	_log.Debug ("Connected");  	EtwTrace.Log.ProducerStarted (Topic' _id);  } catch (Exception e) {  	_log.Error (e' "Exception during connect");  	EtwTrace.Log.ProducerError (e.Message' _id);  	throw;  } finally {  	_log.Debug ("#{0} Releasing Producer Semaphore."' _id);  	_sync.Release ();  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The following statement contains a magic number: _sendMessagesSubject.Do (msg => msg.PartitionId = Configuration.Partitioner.GetMessagePartition (msg' topicPartitions).Id).Buffer (Configuration.BatchFlushTime' Configuration.BatchFlushSize).Where (b => b.Count > 0).Select (msgs => msgs.GroupBy (msg => msg.PartitionId)).ObserveOn (_cluster.Scheduler).Subscribe (partitionGroups => {  	foreach (var batch in partitionGroups) {  		// partition queue might be created if metadata broadcast fired already  		PartitionQueueInfo queue;  		if (!_allPartitionQueues.TryGetValue (batch.Key' out queue)) {  			queue = new PartitionQueueInfo (Configuration.SendBuffersInitialSize) {  				Partition = batch.Key  			};  			_allPartitionQueues.Add (batch.Key' queue);  			queue.IsOnline = topicPartitions.First (p => p.Id == batch.Key).ErrorCode.IsSuccess ();  			_log.Debug ("{0} added new partition queue"' this);  			_queueSizeEvents.OnNext (new QueueResizeInfo {  				PartitionId = queue.Partition'  				Size = queue.Queue.Size'  				Capacity = queue.Queue.Capacity  			});  		}  		// now queue them up  		var batchAr = batch.ToArray ();  		// make sure we have space.  		if (queue.Queue.Size + batchAr.Length > queue.Queue.Capacity) {  			// try to increase the capacity.  			if (Configuration.AutoGrowSendBuffers) {  				var growBy = Math.Max (queue.Queue.Capacity / 2 + 1' 2 * batchAr.Length);  				_log.Warn ("Capacity of send buffer with size {3} not large enough to accept {2} new messages. Increasing capacity from {0} to {1}"' queue.Queue.Capacity' queue.Queue.Capacity + growBy' batchAr.Length' queue.Queue.Size);  				queue.Queue.Capacity += growBy;  				_queueSizeEvents.OnNext (new QueueResizeInfo {  					PartitionId = queue.Partition'  					Size = queue.Queue.Size'  					Capacity = queue.Queue.Capacity  				});  			} else {  				// we're full and not allowed to grow. Throw the batch back to the caller' and continue on.  				var msg = string.Format ("Send Buffer Full for partition {0}. "' Cluster.PartitionStateChanges.Where (ps => ps.Topic == Configuration.Topic && ps.PartitionId == queue.Partition).Take (1).Wait ());  				_log.Error (msg);  				if (OnPermError != null)  					Task.Factory.StartNew (() => OnPermError (new Exception (msg)' batchAr));  				continue;  			}  		}  		// we have the space' add to the queue  		queue.Queue.Put (batchAr);  		if (_log.IsDebugEnabled)  			_log.Debug ("Enqueued batch of size {0} for topic '{1}' partition {2}"' batchAr.Length' Configuration.Topic' batch.Key);  		// After batch enqueued' send wake up signal to sending queue  		_queueEventWaitHandler.Set ();  	}  }' e => _log.Fatal (e' "Error in _sendMessagesSubject pipeline")' () => _log.Debug ("_sendMessagesSubject complete"));  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The following statement contains a magic number: _sendMessagesSubject.Do (msg => msg.PartitionId = Configuration.Partitioner.GetMessagePartition (msg' topicPartitions).Id).Buffer (Configuration.BatchFlushTime' Configuration.BatchFlushSize).Where (b => b.Count > 0).Select (msgs => msgs.GroupBy (msg => msg.PartitionId)).ObserveOn (_cluster.Scheduler).Subscribe (partitionGroups => {  	foreach (var batch in partitionGroups) {  		// partition queue might be created if metadata broadcast fired already  		PartitionQueueInfo queue;  		if (!_allPartitionQueues.TryGetValue (batch.Key' out queue)) {  			queue = new PartitionQueueInfo (Configuration.SendBuffersInitialSize) {  				Partition = batch.Key  			};  			_allPartitionQueues.Add (batch.Key' queue);  			queue.IsOnline = topicPartitions.First (p => p.Id == batch.Key).ErrorCode.IsSuccess ();  			_log.Debug ("{0} added new partition queue"' this);  			_queueSizeEvents.OnNext (new QueueResizeInfo {  				PartitionId = queue.Partition'  				Size = queue.Queue.Size'  				Capacity = queue.Queue.Capacity  			});  		}  		// now queue them up  		var batchAr = batch.ToArray ();  		// make sure we have space.  		if (queue.Queue.Size + batchAr.Length > queue.Queue.Capacity) {  			// try to increase the capacity.  			if (Configuration.AutoGrowSendBuffers) {  				var growBy = Math.Max (queue.Queue.Capacity / 2 + 1' 2 * batchAr.Length);  				_log.Warn ("Capacity of send buffer with size {3} not large enough to accept {2} new messages. Increasing capacity from {0} to {1}"' queue.Queue.Capacity' queue.Queue.Capacity + growBy' batchAr.Length' queue.Queue.Size);  				queue.Queue.Capacity += growBy;  				_queueSizeEvents.OnNext (new QueueResizeInfo {  					PartitionId = queue.Partition'  					Size = queue.Queue.Size'  					Capacity = queue.Queue.Capacity  				});  			} else {  				// we're full and not allowed to grow. Throw the batch back to the caller' and continue on.  				var msg = string.Format ("Send Buffer Full for partition {0}. "' Cluster.PartitionStateChanges.Where (ps => ps.Topic == Configuration.Topic && ps.PartitionId == queue.Partition).Take (1).Wait ());  				_log.Error (msg);  				if (OnPermError != null)  					Task.Factory.StartNew (() => OnPermError (new Exception (msg)' batchAr));  				continue;  			}  		}  		// we have the space' add to the queue  		queue.Queue.Put (batchAr);  		if (_log.IsDebugEnabled)  			_log.Debug ("Enqueued batch of size {0} for topic '{1}' partition {2}"' batchAr.Length' Configuration.Topic' batch.Key);  		// After batch enqueued' send wake up signal to sending queue  		_queueEventWaitHandler.Set ();  	}  }' e => _log.Fatal (e' "Error in _sendMessagesSubject pipeline")' () => _log.Debug ("_sendMessagesSubject complete"));  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The following statement contains a magic number: foreach (var batch in partitionGroups) {  	// partition queue might be created if metadata broadcast fired already  	PartitionQueueInfo queue;  	if (!_allPartitionQueues.TryGetValue (batch.Key' out queue)) {  		queue = new PartitionQueueInfo (Configuration.SendBuffersInitialSize) {  			Partition = batch.Key  		};  		_allPartitionQueues.Add (batch.Key' queue);  		queue.IsOnline = topicPartitions.First (p => p.Id == batch.Key).ErrorCode.IsSuccess ();  		_log.Debug ("{0} added new partition queue"' this);  		_queueSizeEvents.OnNext (new QueueResizeInfo {  			PartitionId = queue.Partition'  			Size = queue.Queue.Size'  			Capacity = queue.Queue.Capacity  		});  	}  	// now queue them up  	var batchAr = batch.ToArray ();  	// make sure we have space.  	if (queue.Queue.Size + batchAr.Length > queue.Queue.Capacity) {  		// try to increase the capacity.  		if (Configuration.AutoGrowSendBuffers) {  			var growBy = Math.Max (queue.Queue.Capacity / 2 + 1' 2 * batchAr.Length);  			_log.Warn ("Capacity of send buffer with size {3} not large enough to accept {2} new messages. Increasing capacity from {0} to {1}"' queue.Queue.Capacity' queue.Queue.Capacity + growBy' batchAr.Length' queue.Queue.Size);  			queue.Queue.Capacity += growBy;  			_queueSizeEvents.OnNext (new QueueResizeInfo {  				PartitionId = queue.Partition'  				Size = queue.Queue.Size'  				Capacity = queue.Queue.Capacity  			});  		} else {  			// we're full and not allowed to grow. Throw the batch back to the caller' and continue on.  			var msg = string.Format ("Send Buffer Full for partition {0}. "' Cluster.PartitionStateChanges.Where (ps => ps.Topic == Configuration.Topic && ps.PartitionId == queue.Partition).Take (1).Wait ());  			_log.Error (msg);  			if (OnPermError != null)  				Task.Factory.StartNew (() => OnPermError (new Exception (msg)' batchAr));  			continue;  		}  	}  	// we have the space' add to the queue  	queue.Queue.Put (batchAr);  	if (_log.IsDebugEnabled)  		_log.Debug ("Enqueued batch of size {0} for topic '{1}' partition {2}"' batchAr.Length' Configuration.Topic' batch.Key);  	// After batch enqueued' send wake up signal to sending queue  	_queueEventWaitHandler.Set ();  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The following statement contains a magic number: foreach (var batch in partitionGroups) {  	// partition queue might be created if metadata broadcast fired already  	PartitionQueueInfo queue;  	if (!_allPartitionQueues.TryGetValue (batch.Key' out queue)) {  		queue = new PartitionQueueInfo (Configuration.SendBuffersInitialSize) {  			Partition = batch.Key  		};  		_allPartitionQueues.Add (batch.Key' queue);  		queue.IsOnline = topicPartitions.First (p => p.Id == batch.Key).ErrorCode.IsSuccess ();  		_log.Debug ("{0} added new partition queue"' this);  		_queueSizeEvents.OnNext (new QueueResizeInfo {  			PartitionId = queue.Partition'  			Size = queue.Queue.Size'  			Capacity = queue.Queue.Capacity  		});  	}  	// now queue them up  	var batchAr = batch.ToArray ();  	// make sure we have space.  	if (queue.Queue.Size + batchAr.Length > queue.Queue.Capacity) {  		// try to increase the capacity.  		if (Configuration.AutoGrowSendBuffers) {  			var growBy = Math.Max (queue.Queue.Capacity / 2 + 1' 2 * batchAr.Length);  			_log.Warn ("Capacity of send buffer with size {3} not large enough to accept {2} new messages. Increasing capacity from {0} to {1}"' queue.Queue.Capacity' queue.Queue.Capacity + growBy' batchAr.Length' queue.Queue.Size);  			queue.Queue.Capacity += growBy;  			_queueSizeEvents.OnNext (new QueueResizeInfo {  				PartitionId = queue.Partition'  				Size = queue.Queue.Size'  				Capacity = queue.Queue.Capacity  			});  		} else {  			// we're full and not allowed to grow. Throw the batch back to the caller' and continue on.  			var msg = string.Format ("Send Buffer Full for partition {0}. "' Cluster.PartitionStateChanges.Where (ps => ps.Topic == Configuration.Topic && ps.PartitionId == queue.Partition).Take (1).Wait ());  			_log.Error (msg);  			if (OnPermError != null)  				Task.Factory.StartNew (() => OnPermError (new Exception (msg)' batchAr));  			continue;  		}  	}  	// we have the space' add to the queue  	queue.Queue.Put (batchAr);  	if (_log.IsDebugEnabled)  		_log.Debug ("Enqueued batch of size {0} for topic '{1}' partition {2}"' batchAr.Length' Configuration.Topic' batch.Key);  	// After batch enqueued' send wake up signal to sending queue  	_queueEventWaitHandler.Set ();  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The following statement contains a magic number: if (queue.Queue.Size + batchAr.Length > queue.Queue.Capacity) {  	// try to increase the capacity.  	if (Configuration.AutoGrowSendBuffers) {  		var growBy = Math.Max (queue.Queue.Capacity / 2 + 1' 2 * batchAr.Length);  		_log.Warn ("Capacity of send buffer with size {3} not large enough to accept {2} new messages. Increasing capacity from {0} to {1}"' queue.Queue.Capacity' queue.Queue.Capacity + growBy' batchAr.Length' queue.Queue.Size);  		queue.Queue.Capacity += growBy;  		_queueSizeEvents.OnNext (new QueueResizeInfo {  			PartitionId = queue.Partition'  			Size = queue.Queue.Size'  			Capacity = queue.Queue.Capacity  		});  	} else {  		// we're full and not allowed to grow. Throw the batch back to the caller' and continue on.  		var msg = string.Format ("Send Buffer Full for partition {0}. "' Cluster.PartitionStateChanges.Where (ps => ps.Topic == Configuration.Topic && ps.PartitionId == queue.Partition).Take (1).Wait ());  		_log.Error (msg);  		if (OnPermError != null)  			Task.Factory.StartNew (() => OnPermError (new Exception (msg)' batchAr));  		continue;  	}  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The following statement contains a magic number: if (queue.Queue.Size + batchAr.Length > queue.Queue.Capacity) {  	// try to increase the capacity.  	if (Configuration.AutoGrowSendBuffers) {  		var growBy = Math.Max (queue.Queue.Capacity / 2 + 1' 2 * batchAr.Length);  		_log.Warn ("Capacity of send buffer with size {3} not large enough to accept {2} new messages. Increasing capacity from {0} to {1}"' queue.Queue.Capacity' queue.Queue.Capacity + growBy' batchAr.Length' queue.Queue.Size);  		queue.Queue.Capacity += growBy;  		_queueSizeEvents.OnNext (new QueueResizeInfo {  			PartitionId = queue.Partition'  			Size = queue.Queue.Size'  			Capacity = queue.Queue.Capacity  		});  	} else {  		// we're full and not allowed to grow. Throw the batch back to the caller' and continue on.  		var msg = string.Format ("Send Buffer Full for partition {0}. "' Cluster.PartitionStateChanges.Where (ps => ps.Topic == Configuration.Topic && ps.PartitionId == queue.Partition).Take (1).Wait ());  		_log.Error (msg);  		if (OnPermError != null)  			Task.Factory.StartNew (() => OnPermError (new Exception (msg)' batchAr));  		continue;  	}  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The following statement contains a magic number: if (Configuration.AutoGrowSendBuffers) {  	var growBy = Math.Max (queue.Queue.Capacity / 2 + 1' 2 * batchAr.Length);  	_log.Warn ("Capacity of send buffer with size {3} not large enough to accept {2} new messages. Increasing capacity from {0} to {1}"' queue.Queue.Capacity' queue.Queue.Capacity + growBy' batchAr.Length' queue.Queue.Size);  	queue.Queue.Capacity += growBy;  	_queueSizeEvents.OnNext (new QueueResizeInfo {  		PartitionId = queue.Partition'  		Size = queue.Queue.Size'  		Capacity = queue.Queue.Capacity  	});  } else {  	// we're full and not allowed to grow. Throw the batch back to the caller' and continue on.  	var msg = string.Format ("Send Buffer Full for partition {0}. "' Cluster.PartitionStateChanges.Where (ps => ps.Topic == Configuration.Topic && ps.PartitionId == queue.Partition).Take (1).Wait ());  	_log.Error (msg);  	if (OnPermError != null)  		Task.Factory.StartNew (() => OnPermError (new Exception (msg)' batchAr));  	continue;  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,ConnectAsync,The following statement contains a magic number: if (Configuration.AutoGrowSendBuffers) {  	var growBy = Math.Max (queue.Queue.Capacity / 2 + 1' 2 * batchAr.Length);  	_log.Warn ("Capacity of send buffer with size {3} not large enough to accept {2} new messages. Increasing capacity from {0} to {1}"' queue.Queue.Capacity' queue.Queue.Capacity + growBy' batchAr.Length' queue.Queue.Size);  	queue.Queue.Capacity += growBy;  	_queueSizeEvents.OnNext (new QueueResizeInfo {  		PartitionId = queue.Partition'  		Size = queue.Queue.Size'  		Capacity = queue.Queue.Capacity  	});  } else {  	// we're full and not allowed to grow. Throw the batch back to the caller' and continue on.  	var msg = string.Format ("Send Buffer Full for partition {0}. "' Cluster.PartitionStateChanges.Where (ps => ps.Topic == Configuration.Topic && ps.PartitionId == queue.Partition).Take (1).Wait ());  	_log.Error (msg);  	if (OnPermError != null)  		Task.Factory.StartNew (() => OnPermError (new Exception (msg)' batchAr));  	continue;  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The following statement contains a magic number: while (true) {  	var partsMeta = await await _cluster.Scheduler.Ask (() => _cluster.GetOrFetchMetaForTopicAsync (Configuration.Topic));  	if (_allPartitionQueues.Values.All (q => !q.IsReadyForServing)) {  		// TODO: bug: if messages are accumulating in buffer' this will quit. Wait for buffer drainage  		if (_drain.IsCancellationRequested && _allPartitionQueues.Values.All (q => q.Queue.Size == 0)) {  			_log.Info ("Cancel detected and send buffers are empty. Quitting SendLoop");  			break;  		}  		_log.Debug ("Waiting for queue event {0}"' Configuration.Topic);  		await Task.Run (() => {  			_log.Debug ("Start waiting in a thread");  			// TODO: make this wait with timeout  			_queueEventWaitHandler.Wait ();  			_queueEventWaitHandler.Reset ();  		});  		_log.Debug ("Got queue event {0}"' Configuration.Topic);  	} else {  		if (_log.IsDebugEnabled)  			_log.Debug ("There are batches in queues' continue working");  	}  	// if we are being told to shut down (already waited for drain)' then send all messages back over OnPermError' and quit.  	if (_shutdown.IsCancellationRequested) {  		var messages = _allPartitionQueues.SelectMany (pq => pq.Value.Queue.Get (pq.Value.Queue.Size)).ToArray ();  		if (messages.Length > 0) {  			var msg = string.Format ("Not all messages could be sent before shutdown. {0} messages remain."' messages.Length);  			if (OnPermError != null) {  				_log.Error (msg);  				await Task.Factory.StartNew (() => OnPermError (new Exception (msg)' messages));  			} else {  				_log.Fatal ("{0} There is no OnPermError handler so these messages are LOST!"' msg);  			}  		}  		break;  	}  	var queuesToBeSent = _allPartitionQueues.Values.Where (q => q.IsReadyForServing).ToArray ();  	if (_log.IsDebugEnabled)  		_log.Debug ("There are {0} partition queues with {1} total messages to send."' queuesToBeSent.Length' queuesToBeSent.Sum (qi => qi.Queue.Size));  	if (queuesToBeSent.Length == 0)  		continue;  	// while sill in lock' mark queues as in-progress to skip sending in next iteration  	_log.Debug ("Locking queues: '{0}'/[{1}]"' Configuration.Topic' string.Join ("'"' queuesToBeSent.Select (q => q.Partition)));  	queuesToBeSent.ForEach (q => q.InProgress = true);  	//  	// Send ProduceRequest  	//  	try {  		var sendTasks = queuesToBeSent.Select (q => new {  			partsMeta.First (m => m.Id == q.Partition).Leader'  			Queue = q  		}).// leader'queue -> leader'queue[]  		GroupBy (q => q.Leader' (i' queues) => new {  			Leader = i'  			Queues = queues.Select (q1 => q1.Queue).ToArray ()  		}).Select (queues => new {  			queues.Leader'  			queues.Queues  		}).Select (async brokerBatch => {  			try {  				//  				// Walk queuese in zig-zag manner from the tail towards the head until we are withing size limit  				// The point of zig-zag is fairness: if producing is slow' we do not want to keep sending only first   				// partition in the queue while starving the last.  				//  				var maxSize = Configuration.MaxMessageSetSizeInBytes;  				var runningSize = 4;  				// array of messages len  				const int messageFixedSize = 8 + // offset  				4 + // message size   				4 + // crc  				1 + // magic  				1 + // attributes  				4 + // size of key array  				4;  				// size of value array  				brokerBatch.Queues.ForEach (q => q.CountInProgress = 0);  				var queueCount = brokerBatch.Queues.Length;  				int finishedQueues = 0;  				for (int i = 0; ; i = (i + 1) % queueCount) {  					// TODO: prevent enqueing of message of too lage size' which exceeds the limit.   					// Do it in syncronous part' before msg rich the queue  					if (i == 0)  						finishedQueues = 0;  					var queue = brokerBatch.Queues [i];  					// whole queue is already taken  					if (queue.Queue.Size == queue.CountInProgress) {  						finishedQueues++;  						if (finishedQueues == queueCount)  							break;  						continue;  					}  					var msg = queue.Queue.PeekSingle (queue.CountInProgress);  					var keyLen = msg.Key != null ? msg.Key.Length : 0;  					var msgLen = msg.Value != null ? msg.Value.Length : 0;  					var msgSize = messageFixedSize + keyLen + msgLen;  					if (runningSize + msgSize > maxSize)  						break;  					runningSize += msgSize;  					queue.CountInProgress++;  				}  				var messages = brokerBatch.Queues.SelectMany (q => q.Queue.PeekEnum (q.CountInProgress));  				// TODO: freeze permanent errors and throw consumer exceptions upon sending to perm error partition  				var response = await _cluster.SendBatchAsync (brokerBatch.Leader' messages' this);  				var failedResponsePartitions = response.Topics.Where (t => t.TopicName == Configuration.Topic).// should contain response only for our topic' but just in case...  				SelectMany (t => t.Partitions).Where (p => !p.ErrorCode.IsSuccess ()).ToArray ();  				// some errors' figure out which batches to dismiss from queues  				var failedPartitionIds = new HashSet<int> (failedResponsePartitions.Select (p => p.Partition));  				var successPartitionQueues = brokerBatch.Queues.Where (q => !failedPartitionIds.Contains (q.Partition)).ToArray ();  				var permanentErrorPartitions = failedResponsePartitions.Where (p => p.ErrorCode.IsPermanentFailure ()).ToArray ();  				var recoverableErrorPartitions = failedResponsePartitions.Except (permanentErrorPartitions).ToArray ();  				var permanentErrorPartitionIds = new HashSet<int> (permanentErrorPartitions.Select (p => p.Partition));  				// notify of errors from send response and trigger recovery monitor tracking them  				recoverableErrorPartitions.ForEach (failedPart => {  					_log.Warn ("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode);  					_cluster.NotifyPartitionStateChange (new PartitionStateChangeEvent (Topic' failedPart.Partition' failedPart.ErrorCode));  				});  				if (permanentErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  					EtwTrace.Log.ProducerPermanentFailure (_id' permanentErrorPartitions.Length);  					string error = string.Join ("' "' permanentErrorPartitions.Select (p => string.Format ("{0}:{1}"' p.Partition' p.ErrorCode)));  					error = string.Format ("[{0}]"' error);  					EtwTrace.Log.ProducerPermanentFailureDetails (_id' error);  				}  				if (recoverableErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  					EtwTrace.Log.ProducerRecoverableErrors (_id' recoverableErrorPartitions.Length);  				}  				// Pop permanently failed messages from the queue  				var permanentFailedMessages = brokerBatch.Queues.Where (q => permanentErrorPartitionIds.Contains (q.Partition)).SelectMany (q => q.Queue.GetEnum (q.CountInProgress)).ToArray ();  				// fire permanent error  				if (OnPermError != null && permanentFailedMessages.Length > 0) {  					var msg = string.Join ("'"' permanentErrorPartitions.Select (p => p.ErrorCode).Distinct ());  					msg = string.Format ("Produce request failed with errors: [{0}]"' msg);  					await Task.Factory.StartNew (() => OnPermError (new BrokerException (msg)' permanentFailedMessages));  				}  				// Do nothing with recoverable errors' they will be sent again next time  				var successMessages = successPartitionQueues.SelectMany (q => q.Queue.Get (q.CountInProgress)).ToArray ();  				if (OnSuccess != null && successMessages.Length != 0)  					await Task.Factory.StartNew (() => OnSuccess (successMessages));  			} catch (ThreadAbortException e) {  				// It happen when Watchdog kills stuck thread. We want stack trace to be logged with high priority  				_log.Fatal (e' "Send has been aborted. Probably hung thread detected");  			} catch (Exception e) {  				_log.Debug (e' "Exception while sending batch to topic '{0}' BrokerId {1}"' Configuration.Topic' brokerBatch.Leader);  				//brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError)));  			}  		});  		await Task.WhenAll (sendTasks);  	} finally {  		queuesToBeSent.ForEach (q => q.InProgress = false);  		if (_log.IsDebugEnabled)  			_log.Debug ("Unlocked queue '{0}'/{1}"' Configuration.Topic' string.Join ("'"' queuesToBeSent.Select (q => q.Partition)));  	}  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The following statement contains a magic number: while (true) {  	var partsMeta = await await _cluster.Scheduler.Ask (() => _cluster.GetOrFetchMetaForTopicAsync (Configuration.Topic));  	if (_allPartitionQueues.Values.All (q => !q.IsReadyForServing)) {  		// TODO: bug: if messages are accumulating in buffer' this will quit. Wait for buffer drainage  		if (_drain.IsCancellationRequested && _allPartitionQueues.Values.All (q => q.Queue.Size == 0)) {  			_log.Info ("Cancel detected and send buffers are empty. Quitting SendLoop");  			break;  		}  		_log.Debug ("Waiting for queue event {0}"' Configuration.Topic);  		await Task.Run (() => {  			_log.Debug ("Start waiting in a thread");  			// TODO: make this wait with timeout  			_queueEventWaitHandler.Wait ();  			_queueEventWaitHandler.Reset ();  		});  		_log.Debug ("Got queue event {0}"' Configuration.Topic);  	} else {  		if (_log.IsDebugEnabled)  			_log.Debug ("There are batches in queues' continue working");  	}  	// if we are being told to shut down (already waited for drain)' then send all messages back over OnPermError' and quit.  	if (_shutdown.IsCancellationRequested) {  		var messages = _allPartitionQueues.SelectMany (pq => pq.Value.Queue.Get (pq.Value.Queue.Size)).ToArray ();  		if (messages.Length > 0) {  			var msg = string.Format ("Not all messages could be sent before shutdown. {0} messages remain."' messages.Length);  			if (OnPermError != null) {  				_log.Error (msg);  				await Task.Factory.StartNew (() => OnPermError (new Exception (msg)' messages));  			} else {  				_log.Fatal ("{0} There is no OnPermError handler so these messages are LOST!"' msg);  			}  		}  		break;  	}  	var queuesToBeSent = _allPartitionQueues.Values.Where (q => q.IsReadyForServing).ToArray ();  	if (_log.IsDebugEnabled)  		_log.Debug ("There are {0} partition queues with {1} total messages to send."' queuesToBeSent.Length' queuesToBeSent.Sum (qi => qi.Queue.Size));  	if (queuesToBeSent.Length == 0)  		continue;  	// while sill in lock' mark queues as in-progress to skip sending in next iteration  	_log.Debug ("Locking queues: '{0}'/[{1}]"' Configuration.Topic' string.Join ("'"' queuesToBeSent.Select (q => q.Partition)));  	queuesToBeSent.ForEach (q => q.InProgress = true);  	//  	// Send ProduceRequest  	//  	try {  		var sendTasks = queuesToBeSent.Select (q => new {  			partsMeta.First (m => m.Id == q.Partition).Leader'  			Queue = q  		}).// leader'queue -> leader'queue[]  		GroupBy (q => q.Leader' (i' queues) => new {  			Leader = i'  			Queues = queues.Select (q1 => q1.Queue).ToArray ()  		}).Select (queues => new {  			queues.Leader'  			queues.Queues  		}).Select (async brokerBatch => {  			try {  				//  				// Walk queuese in zig-zag manner from the tail towards the head until we are withing size limit  				// The point of zig-zag is fairness: if producing is slow' we do not want to keep sending only first   				// partition in the queue while starving the last.  				//  				var maxSize = Configuration.MaxMessageSetSizeInBytes;  				var runningSize = 4;  				// array of messages len  				const int messageFixedSize = 8 + // offset  				4 + // message size   				4 + // crc  				1 + // magic  				1 + // attributes  				4 + // size of key array  				4;  				// size of value array  				brokerBatch.Queues.ForEach (q => q.CountInProgress = 0);  				var queueCount = brokerBatch.Queues.Length;  				int finishedQueues = 0;  				for (int i = 0; ; i = (i + 1) % queueCount) {  					// TODO: prevent enqueing of message of too lage size' which exceeds the limit.   					// Do it in syncronous part' before msg rich the queue  					if (i == 0)  						finishedQueues = 0;  					var queue = brokerBatch.Queues [i];  					// whole queue is already taken  					if (queue.Queue.Size == queue.CountInProgress) {  						finishedQueues++;  						if (finishedQueues == queueCount)  							break;  						continue;  					}  					var msg = queue.Queue.PeekSingle (queue.CountInProgress);  					var keyLen = msg.Key != null ? msg.Key.Length : 0;  					var msgLen = msg.Value != null ? msg.Value.Length : 0;  					var msgSize = messageFixedSize + keyLen + msgLen;  					if (runningSize + msgSize > maxSize)  						break;  					runningSize += msgSize;  					queue.CountInProgress++;  				}  				var messages = brokerBatch.Queues.SelectMany (q => q.Queue.PeekEnum (q.CountInProgress));  				// TODO: freeze permanent errors and throw consumer exceptions upon sending to perm error partition  				var response = await _cluster.SendBatchAsync (brokerBatch.Leader' messages' this);  				var failedResponsePartitions = response.Topics.Where (t => t.TopicName == Configuration.Topic).// should contain response only for our topic' but just in case...  				SelectMany (t => t.Partitions).Where (p => !p.ErrorCode.IsSuccess ()).ToArray ();  				// some errors' figure out which batches to dismiss from queues  				var failedPartitionIds = new HashSet<int> (failedResponsePartitions.Select (p => p.Partition));  				var successPartitionQueues = brokerBatch.Queues.Where (q => !failedPartitionIds.Contains (q.Partition)).ToArray ();  				var permanentErrorPartitions = failedResponsePartitions.Where (p => p.ErrorCode.IsPermanentFailure ()).ToArray ();  				var recoverableErrorPartitions = failedResponsePartitions.Except (permanentErrorPartitions).ToArray ();  				var permanentErrorPartitionIds = new HashSet<int> (permanentErrorPartitions.Select (p => p.Partition));  				// notify of errors from send response and trigger recovery monitor tracking them  				recoverableErrorPartitions.ForEach (failedPart => {  					_log.Warn ("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode);  					_cluster.NotifyPartitionStateChange (new PartitionStateChangeEvent (Topic' failedPart.Partition' failedPart.ErrorCode));  				});  				if (permanentErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  					EtwTrace.Log.ProducerPermanentFailure (_id' permanentErrorPartitions.Length);  					string error = string.Join ("' "' permanentErrorPartitions.Select (p => string.Format ("{0}:{1}"' p.Partition' p.ErrorCode)));  					error = string.Format ("[{0}]"' error);  					EtwTrace.Log.ProducerPermanentFailureDetails (_id' error);  				}  				if (recoverableErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  					EtwTrace.Log.ProducerRecoverableErrors (_id' recoverableErrorPartitions.Length);  				}  				// Pop permanently failed messages from the queue  				var permanentFailedMessages = brokerBatch.Queues.Where (q => permanentErrorPartitionIds.Contains (q.Partition)).SelectMany (q => q.Queue.GetEnum (q.CountInProgress)).ToArray ();  				// fire permanent error  				if (OnPermError != null && permanentFailedMessages.Length > 0) {  					var msg = string.Join ("'"' permanentErrorPartitions.Select (p => p.ErrorCode).Distinct ());  					msg = string.Format ("Produce request failed with errors: [{0}]"' msg);  					await Task.Factory.StartNew (() => OnPermError (new BrokerException (msg)' permanentFailedMessages));  				}  				// Do nothing with recoverable errors' they will be sent again next time  				var successMessages = successPartitionQueues.SelectMany (q => q.Queue.Get (q.CountInProgress)).ToArray ();  				if (OnSuccess != null && successMessages.Length != 0)  					await Task.Factory.StartNew (() => OnSuccess (successMessages));  			} catch (ThreadAbortException e) {  				// It happen when Watchdog kills stuck thread. We want stack trace to be logged with high priority  				_log.Fatal (e' "Send has been aborted. Probably hung thread detected");  			} catch (Exception e) {  				_log.Debug (e' "Exception while sending batch to topic '{0}' BrokerId {1}"' Configuration.Topic' brokerBatch.Leader);  				//brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError)));  			}  		});  		await Task.WhenAll (sendTasks);  	} finally {  		queuesToBeSent.ForEach (q => q.InProgress = false);  		if (_log.IsDebugEnabled)  			_log.Debug ("Unlocked queue '{0}'/{1}"' Configuration.Topic' string.Join ("'"' queuesToBeSent.Select (q => q.Partition)));  	}  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The following statement contains a magic number: while (true) {  	var partsMeta = await await _cluster.Scheduler.Ask (() => _cluster.GetOrFetchMetaForTopicAsync (Configuration.Topic));  	if (_allPartitionQueues.Values.All (q => !q.IsReadyForServing)) {  		// TODO: bug: if messages are accumulating in buffer' this will quit. Wait for buffer drainage  		if (_drain.IsCancellationRequested && _allPartitionQueues.Values.All (q => q.Queue.Size == 0)) {  			_log.Info ("Cancel detected and send buffers are empty. Quitting SendLoop");  			break;  		}  		_log.Debug ("Waiting for queue event {0}"' Configuration.Topic);  		await Task.Run (() => {  			_log.Debug ("Start waiting in a thread");  			// TODO: make this wait with timeout  			_queueEventWaitHandler.Wait ();  			_queueEventWaitHandler.Reset ();  		});  		_log.Debug ("Got queue event {0}"' Configuration.Topic);  	} else {  		if (_log.IsDebugEnabled)  			_log.Debug ("There are batches in queues' continue working");  	}  	// if we are being told to shut down (already waited for drain)' then send all messages back over OnPermError' and quit.  	if (_shutdown.IsCancellationRequested) {  		var messages = _allPartitionQueues.SelectMany (pq => pq.Value.Queue.Get (pq.Value.Queue.Size)).ToArray ();  		if (messages.Length > 0) {  			var msg = string.Format ("Not all messages could be sent before shutdown. {0} messages remain."' messages.Length);  			if (OnPermError != null) {  				_log.Error (msg);  				await Task.Factory.StartNew (() => OnPermError (new Exception (msg)' messages));  			} else {  				_log.Fatal ("{0} There is no OnPermError handler so these messages are LOST!"' msg);  			}  		}  		break;  	}  	var queuesToBeSent = _allPartitionQueues.Values.Where (q => q.IsReadyForServing).ToArray ();  	if (_log.IsDebugEnabled)  		_log.Debug ("There are {0} partition queues with {1} total messages to send."' queuesToBeSent.Length' queuesToBeSent.Sum (qi => qi.Queue.Size));  	if (queuesToBeSent.Length == 0)  		continue;  	// while sill in lock' mark queues as in-progress to skip sending in next iteration  	_log.Debug ("Locking queues: '{0}'/[{1}]"' Configuration.Topic' string.Join ("'"' queuesToBeSent.Select (q => q.Partition)));  	queuesToBeSent.ForEach (q => q.InProgress = true);  	//  	// Send ProduceRequest  	//  	try {  		var sendTasks = queuesToBeSent.Select (q => new {  			partsMeta.First (m => m.Id == q.Partition).Leader'  			Queue = q  		}).// leader'queue -> leader'queue[]  		GroupBy (q => q.Leader' (i' queues) => new {  			Leader = i'  			Queues = queues.Select (q1 => q1.Queue).ToArray ()  		}).Select (queues => new {  			queues.Leader'  			queues.Queues  		}).Select (async brokerBatch => {  			try {  				//  				// Walk queuese in zig-zag manner from the tail towards the head until we are withing size limit  				// The point of zig-zag is fairness: if producing is slow' we do not want to keep sending only first   				// partition in the queue while starving the last.  				//  				var maxSize = Configuration.MaxMessageSetSizeInBytes;  				var runningSize = 4;  				// array of messages len  				const int messageFixedSize = 8 + // offset  				4 + // message size   				4 + // crc  				1 + // magic  				1 + // attributes  				4 + // size of key array  				4;  				// size of value array  				brokerBatch.Queues.ForEach (q => q.CountInProgress = 0);  				var queueCount = brokerBatch.Queues.Length;  				int finishedQueues = 0;  				for (int i = 0; ; i = (i + 1) % queueCount) {  					// TODO: prevent enqueing of message of too lage size' which exceeds the limit.   					// Do it in syncronous part' before msg rich the queue  					if (i == 0)  						finishedQueues = 0;  					var queue = brokerBatch.Queues [i];  					// whole queue is already taken  					if (queue.Queue.Size == queue.CountInProgress) {  						finishedQueues++;  						if (finishedQueues == queueCount)  							break;  						continue;  					}  					var msg = queue.Queue.PeekSingle (queue.CountInProgress);  					var keyLen = msg.Key != null ? msg.Key.Length : 0;  					var msgLen = msg.Value != null ? msg.Value.Length : 0;  					var msgSize = messageFixedSize + keyLen + msgLen;  					if (runningSize + msgSize > maxSize)  						break;  					runningSize += msgSize;  					queue.CountInProgress++;  				}  				var messages = brokerBatch.Queues.SelectMany (q => q.Queue.PeekEnum (q.CountInProgress));  				// TODO: freeze permanent errors and throw consumer exceptions upon sending to perm error partition  				var response = await _cluster.SendBatchAsync (brokerBatch.Leader' messages' this);  				var failedResponsePartitions = response.Topics.Where (t => t.TopicName == Configuration.Topic).// should contain response only for our topic' but just in case...  				SelectMany (t => t.Partitions).Where (p => !p.ErrorCode.IsSuccess ()).ToArray ();  				// some errors' figure out which batches to dismiss from queues  				var failedPartitionIds = new HashSet<int> (failedResponsePartitions.Select (p => p.Partition));  				var successPartitionQueues = brokerBatch.Queues.Where (q => !failedPartitionIds.Contains (q.Partition)).ToArray ();  				var permanentErrorPartitions = failedResponsePartitions.Where (p => p.ErrorCode.IsPermanentFailure ()).ToArray ();  				var recoverableErrorPartitions = failedResponsePartitions.Except (permanentErrorPartitions).ToArray ();  				var permanentErrorPartitionIds = new HashSet<int> (permanentErrorPartitions.Select (p => p.Partition));  				// notify of errors from send response and trigger recovery monitor tracking them  				recoverableErrorPartitions.ForEach (failedPart => {  					_log.Warn ("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode);  					_cluster.NotifyPartitionStateChange (new PartitionStateChangeEvent (Topic' failedPart.Partition' failedPart.ErrorCode));  				});  				if (permanentErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  					EtwTrace.Log.ProducerPermanentFailure (_id' permanentErrorPartitions.Length);  					string error = string.Join ("' "' permanentErrorPartitions.Select (p => string.Format ("{0}:{1}"' p.Partition' p.ErrorCode)));  					error = string.Format ("[{0}]"' error);  					EtwTrace.Log.ProducerPermanentFailureDetails (_id' error);  				}  				if (recoverableErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  					EtwTrace.Log.ProducerRecoverableErrors (_id' recoverableErrorPartitions.Length);  				}  				// Pop permanently failed messages from the queue  				var permanentFailedMessages = brokerBatch.Queues.Where (q => permanentErrorPartitionIds.Contains (q.Partition)).SelectMany (q => q.Queue.GetEnum (q.CountInProgress)).ToArray ();  				// fire permanent error  				if (OnPermError != null && permanentFailedMessages.Length > 0) {  					var msg = string.Join ("'"' permanentErrorPartitions.Select (p => p.ErrorCode).Distinct ());  					msg = string.Format ("Produce request failed with errors: [{0}]"' msg);  					await Task.Factory.StartNew (() => OnPermError (new BrokerException (msg)' permanentFailedMessages));  				}  				// Do nothing with recoverable errors' they will be sent again next time  				var successMessages = successPartitionQueues.SelectMany (q => q.Queue.Get (q.CountInProgress)).ToArray ();  				if (OnSuccess != null && successMessages.Length != 0)  					await Task.Factory.StartNew (() => OnSuccess (successMessages));  			} catch (ThreadAbortException e) {  				// It happen when Watchdog kills stuck thread. We want stack trace to be logged with high priority  				_log.Fatal (e' "Send has been aborted. Probably hung thread detected");  			} catch (Exception e) {  				_log.Debug (e' "Exception while sending batch to topic '{0}' BrokerId {1}"' Configuration.Topic' brokerBatch.Leader);  				//brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError)));  			}  		});  		await Task.WhenAll (sendTasks);  	} finally {  		queuesToBeSent.ForEach (q => q.InProgress = false);  		if (_log.IsDebugEnabled)  			_log.Debug ("Unlocked queue '{0}'/{1}"' Configuration.Topic' string.Join ("'"' queuesToBeSent.Select (q => q.Partition)));  	}  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The following statement contains a magic number: while (true) {  	var partsMeta = await await _cluster.Scheduler.Ask (() => _cluster.GetOrFetchMetaForTopicAsync (Configuration.Topic));  	if (_allPartitionQueues.Values.All (q => !q.IsReadyForServing)) {  		// TODO: bug: if messages are accumulating in buffer' this will quit. Wait for buffer drainage  		if (_drain.IsCancellationRequested && _allPartitionQueues.Values.All (q => q.Queue.Size == 0)) {  			_log.Info ("Cancel detected and send buffers are empty. Quitting SendLoop");  			break;  		}  		_log.Debug ("Waiting for queue event {0}"' Configuration.Topic);  		await Task.Run (() => {  			_log.Debug ("Start waiting in a thread");  			// TODO: make this wait with timeout  			_queueEventWaitHandler.Wait ();  			_queueEventWaitHandler.Reset ();  		});  		_log.Debug ("Got queue event {0}"' Configuration.Topic);  	} else {  		if (_log.IsDebugEnabled)  			_log.Debug ("There are batches in queues' continue working");  	}  	// if we are being told to shut down (already waited for drain)' then send all messages back over OnPermError' and quit.  	if (_shutdown.IsCancellationRequested) {  		var messages = _allPartitionQueues.SelectMany (pq => pq.Value.Queue.Get (pq.Value.Queue.Size)).ToArray ();  		if (messages.Length > 0) {  			var msg = string.Format ("Not all messages could be sent before shutdown. {0} messages remain."' messages.Length);  			if (OnPermError != null) {  				_log.Error (msg);  				await Task.Factory.StartNew (() => OnPermError (new Exception (msg)' messages));  			} else {  				_log.Fatal ("{0} There is no OnPermError handler so these messages are LOST!"' msg);  			}  		}  		break;  	}  	var queuesToBeSent = _allPartitionQueues.Values.Where (q => q.IsReadyForServing).ToArray ();  	if (_log.IsDebugEnabled)  		_log.Debug ("There are {0} partition queues with {1} total messages to send."' queuesToBeSent.Length' queuesToBeSent.Sum (qi => qi.Queue.Size));  	if (queuesToBeSent.Length == 0)  		continue;  	// while sill in lock' mark queues as in-progress to skip sending in next iteration  	_log.Debug ("Locking queues: '{0}'/[{1}]"' Configuration.Topic' string.Join ("'"' queuesToBeSent.Select (q => q.Partition)));  	queuesToBeSent.ForEach (q => q.InProgress = true);  	//  	// Send ProduceRequest  	//  	try {  		var sendTasks = queuesToBeSent.Select (q => new {  			partsMeta.First (m => m.Id == q.Partition).Leader'  			Queue = q  		}).// leader'queue -> leader'queue[]  		GroupBy (q => q.Leader' (i' queues) => new {  			Leader = i'  			Queues = queues.Select (q1 => q1.Queue).ToArray ()  		}).Select (queues => new {  			queues.Leader'  			queues.Queues  		}).Select (async brokerBatch => {  			try {  				//  				// Walk queuese in zig-zag manner from the tail towards the head until we are withing size limit  				// The point of zig-zag is fairness: if producing is slow' we do not want to keep sending only first   				// partition in the queue while starving the last.  				//  				var maxSize = Configuration.MaxMessageSetSizeInBytes;  				var runningSize = 4;  				// array of messages len  				const int messageFixedSize = 8 + // offset  				4 + // message size   				4 + // crc  				1 + // magic  				1 + // attributes  				4 + // size of key array  				4;  				// size of value array  				brokerBatch.Queues.ForEach (q => q.CountInProgress = 0);  				var queueCount = brokerBatch.Queues.Length;  				int finishedQueues = 0;  				for (int i = 0; ; i = (i + 1) % queueCount) {  					// TODO: prevent enqueing of message of too lage size' which exceeds the limit.   					// Do it in syncronous part' before msg rich the queue  					if (i == 0)  						finishedQueues = 0;  					var queue = brokerBatch.Queues [i];  					// whole queue is already taken  					if (queue.Queue.Size == queue.CountInProgress) {  						finishedQueues++;  						if (finishedQueues == queueCount)  							break;  						continue;  					}  					var msg = queue.Queue.PeekSingle (queue.CountInProgress);  					var keyLen = msg.Key != null ? msg.Key.Length : 0;  					var msgLen = msg.Value != null ? msg.Value.Length : 0;  					var msgSize = messageFixedSize + keyLen + msgLen;  					if (runningSize + msgSize > maxSize)  						break;  					runningSize += msgSize;  					queue.CountInProgress++;  				}  				var messages = brokerBatch.Queues.SelectMany (q => q.Queue.PeekEnum (q.CountInProgress));  				// TODO: freeze permanent errors and throw consumer exceptions upon sending to perm error partition  				var response = await _cluster.SendBatchAsync (brokerBatch.Leader' messages' this);  				var failedResponsePartitions = response.Topics.Where (t => t.TopicName == Configuration.Topic).// should contain response only for our topic' but just in case...  				SelectMany (t => t.Partitions).Where (p => !p.ErrorCode.IsSuccess ()).ToArray ();  				// some errors' figure out which batches to dismiss from queues  				var failedPartitionIds = new HashSet<int> (failedResponsePartitions.Select (p => p.Partition));  				var successPartitionQueues = brokerBatch.Queues.Where (q => !failedPartitionIds.Contains (q.Partition)).ToArray ();  				var permanentErrorPartitions = failedResponsePartitions.Where (p => p.ErrorCode.IsPermanentFailure ()).ToArray ();  				var recoverableErrorPartitions = failedResponsePartitions.Except (permanentErrorPartitions).ToArray ();  				var permanentErrorPartitionIds = new HashSet<int> (permanentErrorPartitions.Select (p => p.Partition));  				// notify of errors from send response and trigger recovery monitor tracking them  				recoverableErrorPartitions.ForEach (failedPart => {  					_log.Warn ("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode);  					_cluster.NotifyPartitionStateChange (new PartitionStateChangeEvent (Topic' failedPart.Partition' failedPart.ErrorCode));  				});  				if (permanentErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  					EtwTrace.Log.ProducerPermanentFailure (_id' permanentErrorPartitions.Length);  					string error = string.Join ("' "' permanentErrorPartitions.Select (p => string.Format ("{0}:{1}"' p.Partition' p.ErrorCode)));  					error = string.Format ("[{0}]"' error);  					EtwTrace.Log.ProducerPermanentFailureDetails (_id' error);  				}  				if (recoverableErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  					EtwTrace.Log.ProducerRecoverableErrors (_id' recoverableErrorPartitions.Length);  				}  				// Pop permanently failed messages from the queue  				var permanentFailedMessages = brokerBatch.Queues.Where (q => permanentErrorPartitionIds.Contains (q.Partition)).SelectMany (q => q.Queue.GetEnum (q.CountInProgress)).ToArray ();  				// fire permanent error  				if (OnPermError != null && permanentFailedMessages.Length > 0) {  					var msg = string.Join ("'"' permanentErrorPartitions.Select (p => p.ErrorCode).Distinct ());  					msg = string.Format ("Produce request failed with errors: [{0}]"' msg);  					await Task.Factory.StartNew (() => OnPermError (new BrokerException (msg)' permanentFailedMessages));  				}  				// Do nothing with recoverable errors' they will be sent again next time  				var successMessages = successPartitionQueues.SelectMany (q => q.Queue.Get (q.CountInProgress)).ToArray ();  				if (OnSuccess != null && successMessages.Length != 0)  					await Task.Factory.StartNew (() => OnSuccess (successMessages));  			} catch (ThreadAbortException e) {  				// It happen when Watchdog kills stuck thread. We want stack trace to be logged with high priority  				_log.Fatal (e' "Send has been aborted. Probably hung thread detected");  			} catch (Exception e) {  				_log.Debug (e' "Exception while sending batch to topic '{0}' BrokerId {1}"' Configuration.Topic' brokerBatch.Leader);  				//brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError)));  			}  		});  		await Task.WhenAll (sendTasks);  	} finally {  		queuesToBeSent.ForEach (q => q.InProgress = false);  		if (_log.IsDebugEnabled)  			_log.Debug ("Unlocked queue '{0}'/{1}"' Configuration.Topic' string.Join ("'"' queuesToBeSent.Select (q => q.Partition)));  	}  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The following statement contains a magic number: while (true) {  	var partsMeta = await await _cluster.Scheduler.Ask (() => _cluster.GetOrFetchMetaForTopicAsync (Configuration.Topic));  	if (_allPartitionQueues.Values.All (q => !q.IsReadyForServing)) {  		// TODO: bug: if messages are accumulating in buffer' this will quit. Wait for buffer drainage  		if (_drain.IsCancellationRequested && _allPartitionQueues.Values.All (q => q.Queue.Size == 0)) {  			_log.Info ("Cancel detected and send buffers are empty. Quitting SendLoop");  			break;  		}  		_log.Debug ("Waiting for queue event {0}"' Configuration.Topic);  		await Task.Run (() => {  			_log.Debug ("Start waiting in a thread");  			// TODO: make this wait with timeout  			_queueEventWaitHandler.Wait ();  			_queueEventWaitHandler.Reset ();  		});  		_log.Debug ("Got queue event {0}"' Configuration.Topic);  	} else {  		if (_log.IsDebugEnabled)  			_log.Debug ("There are batches in queues' continue working");  	}  	// if we are being told to shut down (already waited for drain)' then send all messages back over OnPermError' and quit.  	if (_shutdown.IsCancellationRequested) {  		var messages = _allPartitionQueues.SelectMany (pq => pq.Value.Queue.Get (pq.Value.Queue.Size)).ToArray ();  		if (messages.Length > 0) {  			var msg = string.Format ("Not all messages could be sent before shutdown. {0} messages remain."' messages.Length);  			if (OnPermError != null) {  				_log.Error (msg);  				await Task.Factory.StartNew (() => OnPermError (new Exception (msg)' messages));  			} else {  				_log.Fatal ("{0} There is no OnPermError handler so these messages are LOST!"' msg);  			}  		}  		break;  	}  	var queuesToBeSent = _allPartitionQueues.Values.Where (q => q.IsReadyForServing).ToArray ();  	if (_log.IsDebugEnabled)  		_log.Debug ("There are {0} partition queues with {1} total messages to send."' queuesToBeSent.Length' queuesToBeSent.Sum (qi => qi.Queue.Size));  	if (queuesToBeSent.Length == 0)  		continue;  	// while sill in lock' mark queues as in-progress to skip sending in next iteration  	_log.Debug ("Locking queues: '{0}'/[{1}]"' Configuration.Topic' string.Join ("'"' queuesToBeSent.Select (q => q.Partition)));  	queuesToBeSent.ForEach (q => q.InProgress = true);  	//  	// Send ProduceRequest  	//  	try {  		var sendTasks = queuesToBeSent.Select (q => new {  			partsMeta.First (m => m.Id == q.Partition).Leader'  			Queue = q  		}).// leader'queue -> leader'queue[]  		GroupBy (q => q.Leader' (i' queues) => new {  			Leader = i'  			Queues = queues.Select (q1 => q1.Queue).ToArray ()  		}).Select (queues => new {  			queues.Leader'  			queues.Queues  		}).Select (async brokerBatch => {  			try {  				//  				// Walk queuese in zig-zag manner from the tail towards the head until we are withing size limit  				// The point of zig-zag is fairness: if producing is slow' we do not want to keep sending only first   				// partition in the queue while starving the last.  				//  				var maxSize = Configuration.MaxMessageSetSizeInBytes;  				var runningSize = 4;  				// array of messages len  				const int messageFixedSize = 8 + // offset  				4 + // message size   				4 + // crc  				1 + // magic  				1 + // attributes  				4 + // size of key array  				4;  				// size of value array  				brokerBatch.Queues.ForEach (q => q.CountInProgress = 0);  				var queueCount = brokerBatch.Queues.Length;  				int finishedQueues = 0;  				for (int i = 0; ; i = (i + 1) % queueCount) {  					// TODO: prevent enqueing of message of too lage size' which exceeds the limit.   					// Do it in syncronous part' before msg rich the queue  					if (i == 0)  						finishedQueues = 0;  					var queue = brokerBatch.Queues [i];  					// whole queue is already taken  					if (queue.Queue.Size == queue.CountInProgress) {  						finishedQueues++;  						if (finishedQueues == queueCount)  							break;  						continue;  					}  					var msg = queue.Queue.PeekSingle (queue.CountInProgress);  					var keyLen = msg.Key != null ? msg.Key.Length : 0;  					var msgLen = msg.Value != null ? msg.Value.Length : 0;  					var msgSize = messageFixedSize + keyLen + msgLen;  					if (runningSize + msgSize > maxSize)  						break;  					runningSize += msgSize;  					queue.CountInProgress++;  				}  				var messages = brokerBatch.Queues.SelectMany (q => q.Queue.PeekEnum (q.CountInProgress));  				// TODO: freeze permanent errors and throw consumer exceptions upon sending to perm error partition  				var response = await _cluster.SendBatchAsync (brokerBatch.Leader' messages' this);  				var failedResponsePartitions = response.Topics.Where (t => t.TopicName == Configuration.Topic).// should contain response only for our topic' but just in case...  				SelectMany (t => t.Partitions).Where (p => !p.ErrorCode.IsSuccess ()).ToArray ();  				// some errors' figure out which batches to dismiss from queues  				var failedPartitionIds = new HashSet<int> (failedResponsePartitions.Select (p => p.Partition));  				var successPartitionQueues = brokerBatch.Queues.Where (q => !failedPartitionIds.Contains (q.Partition)).ToArray ();  				var permanentErrorPartitions = failedResponsePartitions.Where (p => p.ErrorCode.IsPermanentFailure ()).ToArray ();  				var recoverableErrorPartitions = failedResponsePartitions.Except (permanentErrorPartitions).ToArray ();  				var permanentErrorPartitionIds = new HashSet<int> (permanentErrorPartitions.Select (p => p.Partition));  				// notify of errors from send response and trigger recovery monitor tracking them  				recoverableErrorPartitions.ForEach (failedPart => {  					_log.Warn ("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode);  					_cluster.NotifyPartitionStateChange (new PartitionStateChangeEvent (Topic' failedPart.Partition' failedPart.ErrorCode));  				});  				if (permanentErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  					EtwTrace.Log.ProducerPermanentFailure (_id' permanentErrorPartitions.Length);  					string error = string.Join ("' "' permanentErrorPartitions.Select (p => string.Format ("{0}:{1}"' p.Partition' p.ErrorCode)));  					error = string.Format ("[{0}]"' error);  					EtwTrace.Log.ProducerPermanentFailureDetails (_id' error);  				}  				if (recoverableErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  					EtwTrace.Log.ProducerRecoverableErrors (_id' recoverableErrorPartitions.Length);  				}  				// Pop permanently failed messages from the queue  				var permanentFailedMessages = brokerBatch.Queues.Where (q => permanentErrorPartitionIds.Contains (q.Partition)).SelectMany (q => q.Queue.GetEnum (q.CountInProgress)).ToArray ();  				// fire permanent error  				if (OnPermError != null && permanentFailedMessages.Length > 0) {  					var msg = string.Join ("'"' permanentErrorPartitions.Select (p => p.ErrorCode).Distinct ());  					msg = string.Format ("Produce request failed with errors: [{0}]"' msg);  					await Task.Factory.StartNew (() => OnPermError (new BrokerException (msg)' permanentFailedMessages));  				}  				// Do nothing with recoverable errors' they will be sent again next time  				var successMessages = successPartitionQueues.SelectMany (q => q.Queue.Get (q.CountInProgress)).ToArray ();  				if (OnSuccess != null && successMessages.Length != 0)  					await Task.Factory.StartNew (() => OnSuccess (successMessages));  			} catch (ThreadAbortException e) {  				// It happen when Watchdog kills stuck thread. We want stack trace to be logged with high priority  				_log.Fatal (e' "Send has been aborted. Probably hung thread detected");  			} catch (Exception e) {  				_log.Debug (e' "Exception while sending batch to topic '{0}' BrokerId {1}"' Configuration.Topic' brokerBatch.Leader);  				//brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError)));  			}  		});  		await Task.WhenAll (sendTasks);  	} finally {  		queuesToBeSent.ForEach (q => q.InProgress = false);  		if (_log.IsDebugEnabled)  			_log.Debug ("Unlocked queue '{0}'/{1}"' Configuration.Topic' string.Join ("'"' queuesToBeSent.Select (q => q.Partition)));  	}  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The following statement contains a magic number: while (true) {  	var partsMeta = await await _cluster.Scheduler.Ask (() => _cluster.GetOrFetchMetaForTopicAsync (Configuration.Topic));  	if (_allPartitionQueues.Values.All (q => !q.IsReadyForServing)) {  		// TODO: bug: if messages are accumulating in buffer' this will quit. Wait for buffer drainage  		if (_drain.IsCancellationRequested && _allPartitionQueues.Values.All (q => q.Queue.Size == 0)) {  			_log.Info ("Cancel detected and send buffers are empty. Quitting SendLoop");  			break;  		}  		_log.Debug ("Waiting for queue event {0}"' Configuration.Topic);  		await Task.Run (() => {  			_log.Debug ("Start waiting in a thread");  			// TODO: make this wait with timeout  			_queueEventWaitHandler.Wait ();  			_queueEventWaitHandler.Reset ();  		});  		_log.Debug ("Got queue event {0}"' Configuration.Topic);  	} else {  		if (_log.IsDebugEnabled)  			_log.Debug ("There are batches in queues' continue working");  	}  	// if we are being told to shut down (already waited for drain)' then send all messages back over OnPermError' and quit.  	if (_shutdown.IsCancellationRequested) {  		var messages = _allPartitionQueues.SelectMany (pq => pq.Value.Queue.Get (pq.Value.Queue.Size)).ToArray ();  		if (messages.Length > 0) {  			var msg = string.Format ("Not all messages could be sent before shutdown. {0} messages remain."' messages.Length);  			if (OnPermError != null) {  				_log.Error (msg);  				await Task.Factory.StartNew (() => OnPermError (new Exception (msg)' messages));  			} else {  				_log.Fatal ("{0} There is no OnPermError handler so these messages are LOST!"' msg);  			}  		}  		break;  	}  	var queuesToBeSent = _allPartitionQueues.Values.Where (q => q.IsReadyForServing).ToArray ();  	if (_log.IsDebugEnabled)  		_log.Debug ("There are {0} partition queues with {1} total messages to send."' queuesToBeSent.Length' queuesToBeSent.Sum (qi => qi.Queue.Size));  	if (queuesToBeSent.Length == 0)  		continue;  	// while sill in lock' mark queues as in-progress to skip sending in next iteration  	_log.Debug ("Locking queues: '{0}'/[{1}]"' Configuration.Topic' string.Join ("'"' queuesToBeSent.Select (q => q.Partition)));  	queuesToBeSent.ForEach (q => q.InProgress = true);  	//  	// Send ProduceRequest  	//  	try {  		var sendTasks = queuesToBeSent.Select (q => new {  			partsMeta.First (m => m.Id == q.Partition).Leader'  			Queue = q  		}).// leader'queue -> leader'queue[]  		GroupBy (q => q.Leader' (i' queues) => new {  			Leader = i'  			Queues = queues.Select (q1 => q1.Queue).ToArray ()  		}).Select (queues => new {  			queues.Leader'  			queues.Queues  		}).Select (async brokerBatch => {  			try {  				//  				// Walk queuese in zig-zag manner from the tail towards the head until we are withing size limit  				// The point of zig-zag is fairness: if producing is slow' we do not want to keep sending only first   				// partition in the queue while starving the last.  				//  				var maxSize = Configuration.MaxMessageSetSizeInBytes;  				var runningSize = 4;  				// array of messages len  				const int messageFixedSize = 8 + // offset  				4 + // message size   				4 + // crc  				1 + // magic  				1 + // attributes  				4 + // size of key array  				4;  				// size of value array  				brokerBatch.Queues.ForEach (q => q.CountInProgress = 0);  				var queueCount = brokerBatch.Queues.Length;  				int finishedQueues = 0;  				for (int i = 0; ; i = (i + 1) % queueCount) {  					// TODO: prevent enqueing of message of too lage size' which exceeds the limit.   					// Do it in syncronous part' before msg rich the queue  					if (i == 0)  						finishedQueues = 0;  					var queue = brokerBatch.Queues [i];  					// whole queue is already taken  					if (queue.Queue.Size == queue.CountInProgress) {  						finishedQueues++;  						if (finishedQueues == queueCount)  							break;  						continue;  					}  					var msg = queue.Queue.PeekSingle (queue.CountInProgress);  					var keyLen = msg.Key != null ? msg.Key.Length : 0;  					var msgLen = msg.Value != null ? msg.Value.Length : 0;  					var msgSize = messageFixedSize + keyLen + msgLen;  					if (runningSize + msgSize > maxSize)  						break;  					runningSize += msgSize;  					queue.CountInProgress++;  				}  				var messages = brokerBatch.Queues.SelectMany (q => q.Queue.PeekEnum (q.CountInProgress));  				// TODO: freeze permanent errors and throw consumer exceptions upon sending to perm error partition  				var response = await _cluster.SendBatchAsync (brokerBatch.Leader' messages' this);  				var failedResponsePartitions = response.Topics.Where (t => t.TopicName == Configuration.Topic).// should contain response only for our topic' but just in case...  				SelectMany (t => t.Partitions).Where (p => !p.ErrorCode.IsSuccess ()).ToArray ();  				// some errors' figure out which batches to dismiss from queues  				var failedPartitionIds = new HashSet<int> (failedResponsePartitions.Select (p => p.Partition));  				var successPartitionQueues = brokerBatch.Queues.Where (q => !failedPartitionIds.Contains (q.Partition)).ToArray ();  				var permanentErrorPartitions = failedResponsePartitions.Where (p => p.ErrorCode.IsPermanentFailure ()).ToArray ();  				var recoverableErrorPartitions = failedResponsePartitions.Except (permanentErrorPartitions).ToArray ();  				var permanentErrorPartitionIds = new HashSet<int> (permanentErrorPartitions.Select (p => p.Partition));  				// notify of errors from send response and trigger recovery monitor tracking them  				recoverableErrorPartitions.ForEach (failedPart => {  					_log.Warn ("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode);  					_cluster.NotifyPartitionStateChange (new PartitionStateChangeEvent (Topic' failedPart.Partition' failedPart.ErrorCode));  				});  				if (permanentErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  					EtwTrace.Log.ProducerPermanentFailure (_id' permanentErrorPartitions.Length);  					string error = string.Join ("' "' permanentErrorPartitions.Select (p => string.Format ("{0}:{1}"' p.Partition' p.ErrorCode)));  					error = string.Format ("[{0}]"' error);  					EtwTrace.Log.ProducerPermanentFailureDetails (_id' error);  				}  				if (recoverableErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  					EtwTrace.Log.ProducerRecoverableErrors (_id' recoverableErrorPartitions.Length);  				}  				// Pop permanently failed messages from the queue  				var permanentFailedMessages = brokerBatch.Queues.Where (q => permanentErrorPartitionIds.Contains (q.Partition)).SelectMany (q => q.Queue.GetEnum (q.CountInProgress)).ToArray ();  				// fire permanent error  				if (OnPermError != null && permanentFailedMessages.Length > 0) {  					var msg = string.Join ("'"' permanentErrorPartitions.Select (p => p.ErrorCode).Distinct ());  					msg = string.Format ("Produce request failed with errors: [{0}]"' msg);  					await Task.Factory.StartNew (() => OnPermError (new BrokerException (msg)' permanentFailedMessages));  				}  				// Do nothing with recoverable errors' they will be sent again next time  				var successMessages = successPartitionQueues.SelectMany (q => q.Queue.Get (q.CountInProgress)).ToArray ();  				if (OnSuccess != null && successMessages.Length != 0)  					await Task.Factory.StartNew (() => OnSuccess (successMessages));  			} catch (ThreadAbortException e) {  				// It happen when Watchdog kills stuck thread. We want stack trace to be logged with high priority  				_log.Fatal (e' "Send has been aborted. Probably hung thread detected");  			} catch (Exception e) {  				_log.Debug (e' "Exception while sending batch to topic '{0}' BrokerId {1}"' Configuration.Topic' brokerBatch.Leader);  				//brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError)));  			}  		});  		await Task.WhenAll (sendTasks);  	} finally {  		queuesToBeSent.ForEach (q => q.InProgress = false);  		if (_log.IsDebugEnabled)  			_log.Debug ("Unlocked queue '{0}'/{1}"' Configuration.Topic' string.Join ("'"' queuesToBeSent.Select (q => q.Partition)));  	}  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The following statement contains a magic number: try {  	var sendTasks = queuesToBeSent.Select (q => new {  		partsMeta.First (m => m.Id == q.Partition).Leader'  		Queue = q  	}).// leader'queue -> leader'queue[]  	GroupBy (q => q.Leader' (i' queues) => new {  		Leader = i'  		Queues = queues.Select (q1 => q1.Queue).ToArray ()  	}).Select (queues => new {  		queues.Leader'  		queues.Queues  	}).Select (async brokerBatch => {  		try {  			//  			// Walk queuese in zig-zag manner from the tail towards the head until we are withing size limit  			// The point of zig-zag is fairness: if producing is slow' we do not want to keep sending only first   			// partition in the queue while starving the last.  			//  			var maxSize = Configuration.MaxMessageSetSizeInBytes;  			var runningSize = 4;  			// array of messages len  			const int messageFixedSize = 8 + // offset  			4 + // message size   			4 + // crc  			1 + // magic  			1 + // attributes  			4 + // size of key array  			4;  			// size of value array  			brokerBatch.Queues.ForEach (q => q.CountInProgress = 0);  			var queueCount = brokerBatch.Queues.Length;  			int finishedQueues = 0;  			for (int i = 0; ; i = (i + 1) % queueCount) {  				// TODO: prevent enqueing of message of too lage size' which exceeds the limit.   				// Do it in syncronous part' before msg rich the queue  				if (i == 0)  					finishedQueues = 0;  				var queue = brokerBatch.Queues [i];  				// whole queue is already taken  				if (queue.Queue.Size == queue.CountInProgress) {  					finishedQueues++;  					if (finishedQueues == queueCount)  						break;  					continue;  				}  				var msg = queue.Queue.PeekSingle (queue.CountInProgress);  				var keyLen = msg.Key != null ? msg.Key.Length : 0;  				var msgLen = msg.Value != null ? msg.Value.Length : 0;  				var msgSize = messageFixedSize + keyLen + msgLen;  				if (runningSize + msgSize > maxSize)  					break;  				runningSize += msgSize;  				queue.CountInProgress++;  			}  			var messages = brokerBatch.Queues.SelectMany (q => q.Queue.PeekEnum (q.CountInProgress));  			// TODO: freeze permanent errors and throw consumer exceptions upon sending to perm error partition  			var response = await _cluster.SendBatchAsync (brokerBatch.Leader' messages' this);  			var failedResponsePartitions = response.Topics.Where (t => t.TopicName == Configuration.Topic).// should contain response only for our topic' but just in case...  			SelectMany (t => t.Partitions).Where (p => !p.ErrorCode.IsSuccess ()).ToArray ();  			// some errors' figure out which batches to dismiss from queues  			var failedPartitionIds = new HashSet<int> (failedResponsePartitions.Select (p => p.Partition));  			var successPartitionQueues = brokerBatch.Queues.Where (q => !failedPartitionIds.Contains (q.Partition)).ToArray ();  			var permanentErrorPartitions = failedResponsePartitions.Where (p => p.ErrorCode.IsPermanentFailure ()).ToArray ();  			var recoverableErrorPartitions = failedResponsePartitions.Except (permanentErrorPartitions).ToArray ();  			var permanentErrorPartitionIds = new HashSet<int> (permanentErrorPartitions.Select (p => p.Partition));  			// notify of errors from send response and trigger recovery monitor tracking them  			recoverableErrorPartitions.ForEach (failedPart => {  				_log.Warn ("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode);  				_cluster.NotifyPartitionStateChange (new PartitionStateChangeEvent (Topic' failedPart.Partition' failedPart.ErrorCode));  			});  			if (permanentErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  				EtwTrace.Log.ProducerPermanentFailure (_id' permanentErrorPartitions.Length);  				string error = string.Join ("' "' permanentErrorPartitions.Select (p => string.Format ("{0}:{1}"' p.Partition' p.ErrorCode)));  				error = string.Format ("[{0}]"' error);  				EtwTrace.Log.ProducerPermanentFailureDetails (_id' error);  			}  			if (recoverableErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  				EtwTrace.Log.ProducerRecoverableErrors (_id' recoverableErrorPartitions.Length);  			}  			// Pop permanently failed messages from the queue  			var permanentFailedMessages = brokerBatch.Queues.Where (q => permanentErrorPartitionIds.Contains (q.Partition)).SelectMany (q => q.Queue.GetEnum (q.CountInProgress)).ToArray ();  			// fire permanent error  			if (OnPermError != null && permanentFailedMessages.Length > 0) {  				var msg = string.Join ("'"' permanentErrorPartitions.Select (p => p.ErrorCode).Distinct ());  				msg = string.Format ("Produce request failed with errors: [{0}]"' msg);  				await Task.Factory.StartNew (() => OnPermError (new BrokerException (msg)' permanentFailedMessages));  			}  			// Do nothing with recoverable errors' they will be sent again next time  			var successMessages = successPartitionQueues.SelectMany (q => q.Queue.Get (q.CountInProgress)).ToArray ();  			if (OnSuccess != null && successMessages.Length != 0)  				await Task.Factory.StartNew (() => OnSuccess (successMessages));  		} catch (ThreadAbortException e) {  			// It happen when Watchdog kills stuck thread. We want stack trace to be logged with high priority  			_log.Fatal (e' "Send has been aborted. Probably hung thread detected");  		} catch (Exception e) {  			_log.Debug (e' "Exception while sending batch to topic '{0}' BrokerId {1}"' Configuration.Topic' brokerBatch.Leader);  			//brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError)));  		}  	});  	await Task.WhenAll (sendTasks);  } finally {  	queuesToBeSent.ForEach (q => q.InProgress = false);  	if (_log.IsDebugEnabled)  		_log.Debug ("Unlocked queue '{0}'/{1}"' Configuration.Topic' string.Join ("'"' queuesToBeSent.Select (q => q.Partition)));  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The following statement contains a magic number: try {  	var sendTasks = queuesToBeSent.Select (q => new {  		partsMeta.First (m => m.Id == q.Partition).Leader'  		Queue = q  	}).// leader'queue -> leader'queue[]  	GroupBy (q => q.Leader' (i' queues) => new {  		Leader = i'  		Queues = queues.Select (q1 => q1.Queue).ToArray ()  	}).Select (queues => new {  		queues.Leader'  		queues.Queues  	}).Select (async brokerBatch => {  		try {  			//  			// Walk queuese in zig-zag manner from the tail towards the head until we are withing size limit  			// The point of zig-zag is fairness: if producing is slow' we do not want to keep sending only first   			// partition in the queue while starving the last.  			//  			var maxSize = Configuration.MaxMessageSetSizeInBytes;  			var runningSize = 4;  			// array of messages len  			const int messageFixedSize = 8 + // offset  			4 + // message size   			4 + // crc  			1 + // magic  			1 + // attributes  			4 + // size of key array  			4;  			// size of value array  			brokerBatch.Queues.ForEach (q => q.CountInProgress = 0);  			var queueCount = brokerBatch.Queues.Length;  			int finishedQueues = 0;  			for (int i = 0; ; i = (i + 1) % queueCount) {  				// TODO: prevent enqueing of message of too lage size' which exceeds the limit.   				// Do it in syncronous part' before msg rich the queue  				if (i == 0)  					finishedQueues = 0;  				var queue = brokerBatch.Queues [i];  				// whole queue is already taken  				if (queue.Queue.Size == queue.CountInProgress) {  					finishedQueues++;  					if (finishedQueues == queueCount)  						break;  					continue;  				}  				var msg = queue.Queue.PeekSingle (queue.CountInProgress);  				var keyLen = msg.Key != null ? msg.Key.Length : 0;  				var msgLen = msg.Value != null ? msg.Value.Length : 0;  				var msgSize = messageFixedSize + keyLen + msgLen;  				if (runningSize + msgSize > maxSize)  					break;  				runningSize += msgSize;  				queue.CountInProgress++;  			}  			var messages = brokerBatch.Queues.SelectMany (q => q.Queue.PeekEnum (q.CountInProgress));  			// TODO: freeze permanent errors and throw consumer exceptions upon sending to perm error partition  			var response = await _cluster.SendBatchAsync (brokerBatch.Leader' messages' this);  			var failedResponsePartitions = response.Topics.Where (t => t.TopicName == Configuration.Topic).// should contain response only for our topic' but just in case...  			SelectMany (t => t.Partitions).Where (p => !p.ErrorCode.IsSuccess ()).ToArray ();  			// some errors' figure out which batches to dismiss from queues  			var failedPartitionIds = new HashSet<int> (failedResponsePartitions.Select (p => p.Partition));  			var successPartitionQueues = brokerBatch.Queues.Where (q => !failedPartitionIds.Contains (q.Partition)).ToArray ();  			var permanentErrorPartitions = failedResponsePartitions.Where (p => p.ErrorCode.IsPermanentFailure ()).ToArray ();  			var recoverableErrorPartitions = failedResponsePartitions.Except (permanentErrorPartitions).ToArray ();  			var permanentErrorPartitionIds = new HashSet<int> (permanentErrorPartitions.Select (p => p.Partition));  			// notify of errors from send response and trigger recovery monitor tracking them  			recoverableErrorPartitions.ForEach (failedPart => {  				_log.Warn ("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode);  				_cluster.NotifyPartitionStateChange (new PartitionStateChangeEvent (Topic' failedPart.Partition' failedPart.ErrorCode));  			});  			if (permanentErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  				EtwTrace.Log.ProducerPermanentFailure (_id' permanentErrorPartitions.Length);  				string error = string.Join ("' "' permanentErrorPartitions.Select (p => string.Format ("{0}:{1}"' p.Partition' p.ErrorCode)));  				error = string.Format ("[{0}]"' error);  				EtwTrace.Log.ProducerPermanentFailureDetails (_id' error);  			}  			if (recoverableErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  				EtwTrace.Log.ProducerRecoverableErrors (_id' recoverableErrorPartitions.Length);  			}  			// Pop permanently failed messages from the queue  			var permanentFailedMessages = brokerBatch.Queues.Where (q => permanentErrorPartitionIds.Contains (q.Partition)).SelectMany (q => q.Queue.GetEnum (q.CountInProgress)).ToArray ();  			// fire permanent error  			if (OnPermError != null && permanentFailedMessages.Length > 0) {  				var msg = string.Join ("'"' permanentErrorPartitions.Select (p => p.ErrorCode).Distinct ());  				msg = string.Format ("Produce request failed with errors: [{0}]"' msg);  				await Task.Factory.StartNew (() => OnPermError (new BrokerException (msg)' permanentFailedMessages));  			}  			// Do nothing with recoverable errors' they will be sent again next time  			var successMessages = successPartitionQueues.SelectMany (q => q.Queue.Get (q.CountInProgress)).ToArray ();  			if (OnSuccess != null && successMessages.Length != 0)  				await Task.Factory.StartNew (() => OnSuccess (successMessages));  		} catch (ThreadAbortException e) {  			// It happen when Watchdog kills stuck thread. We want stack trace to be logged with high priority  			_log.Fatal (e' "Send has been aborted. Probably hung thread detected");  		} catch (Exception e) {  			_log.Debug (e' "Exception while sending batch to topic '{0}' BrokerId {1}"' Configuration.Topic' brokerBatch.Leader);  			//brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError)));  		}  	});  	await Task.WhenAll (sendTasks);  } finally {  	queuesToBeSent.ForEach (q => q.InProgress = false);  	if (_log.IsDebugEnabled)  		_log.Debug ("Unlocked queue '{0}'/{1}"' Configuration.Topic' string.Join ("'"' queuesToBeSent.Select (q => q.Partition)));  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The following statement contains a magic number: try {  	var sendTasks = queuesToBeSent.Select (q => new {  		partsMeta.First (m => m.Id == q.Partition).Leader'  		Queue = q  	}).// leader'queue -> leader'queue[]  	GroupBy (q => q.Leader' (i' queues) => new {  		Leader = i'  		Queues = queues.Select (q1 => q1.Queue).ToArray ()  	}).Select (queues => new {  		queues.Leader'  		queues.Queues  	}).Select (async brokerBatch => {  		try {  			//  			// Walk queuese in zig-zag manner from the tail towards the head until we are withing size limit  			// The point of zig-zag is fairness: if producing is slow' we do not want to keep sending only first   			// partition in the queue while starving the last.  			//  			var maxSize = Configuration.MaxMessageSetSizeInBytes;  			var runningSize = 4;  			// array of messages len  			const int messageFixedSize = 8 + // offset  			4 + // message size   			4 + // crc  			1 + // magic  			1 + // attributes  			4 + // size of key array  			4;  			// size of value array  			brokerBatch.Queues.ForEach (q => q.CountInProgress = 0);  			var queueCount = brokerBatch.Queues.Length;  			int finishedQueues = 0;  			for (int i = 0; ; i = (i + 1) % queueCount) {  				// TODO: prevent enqueing of message of too lage size' which exceeds the limit.   				// Do it in syncronous part' before msg rich the queue  				if (i == 0)  					finishedQueues = 0;  				var queue = brokerBatch.Queues [i];  				// whole queue is already taken  				if (queue.Queue.Size == queue.CountInProgress) {  					finishedQueues++;  					if (finishedQueues == queueCount)  						break;  					continue;  				}  				var msg = queue.Queue.PeekSingle (queue.CountInProgress);  				var keyLen = msg.Key != null ? msg.Key.Length : 0;  				var msgLen = msg.Value != null ? msg.Value.Length : 0;  				var msgSize = messageFixedSize + keyLen + msgLen;  				if (runningSize + msgSize > maxSize)  					break;  				runningSize += msgSize;  				queue.CountInProgress++;  			}  			var messages = brokerBatch.Queues.SelectMany (q => q.Queue.PeekEnum (q.CountInProgress));  			// TODO: freeze permanent errors and throw consumer exceptions upon sending to perm error partition  			var response = await _cluster.SendBatchAsync (brokerBatch.Leader' messages' this);  			var failedResponsePartitions = response.Topics.Where (t => t.TopicName == Configuration.Topic).// should contain response only for our topic' but just in case...  			SelectMany (t => t.Partitions).Where (p => !p.ErrorCode.IsSuccess ()).ToArray ();  			// some errors' figure out which batches to dismiss from queues  			var failedPartitionIds = new HashSet<int> (failedResponsePartitions.Select (p => p.Partition));  			var successPartitionQueues = brokerBatch.Queues.Where (q => !failedPartitionIds.Contains (q.Partition)).ToArray ();  			var permanentErrorPartitions = failedResponsePartitions.Where (p => p.ErrorCode.IsPermanentFailure ()).ToArray ();  			var recoverableErrorPartitions = failedResponsePartitions.Except (permanentErrorPartitions).ToArray ();  			var permanentErrorPartitionIds = new HashSet<int> (permanentErrorPartitions.Select (p => p.Partition));  			// notify of errors from send response and trigger recovery monitor tracking them  			recoverableErrorPartitions.ForEach (failedPart => {  				_log.Warn ("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode);  				_cluster.NotifyPartitionStateChange (new PartitionStateChangeEvent (Topic' failedPart.Partition' failedPart.ErrorCode));  			});  			if (permanentErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  				EtwTrace.Log.ProducerPermanentFailure (_id' permanentErrorPartitions.Length);  				string error = string.Join ("' "' permanentErrorPartitions.Select (p => string.Format ("{0}:{1}"' p.Partition' p.ErrorCode)));  				error = string.Format ("[{0}]"' error);  				EtwTrace.Log.ProducerPermanentFailureDetails (_id' error);  			}  			if (recoverableErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  				EtwTrace.Log.ProducerRecoverableErrors (_id' recoverableErrorPartitions.Length);  			}  			// Pop permanently failed messages from the queue  			var permanentFailedMessages = brokerBatch.Queues.Where (q => permanentErrorPartitionIds.Contains (q.Partition)).SelectMany (q => q.Queue.GetEnum (q.CountInProgress)).ToArray ();  			// fire permanent error  			if (OnPermError != null && permanentFailedMessages.Length > 0) {  				var msg = string.Join ("'"' permanentErrorPartitions.Select (p => p.ErrorCode).Distinct ());  				msg = string.Format ("Produce request failed with errors: [{0}]"' msg);  				await Task.Factory.StartNew (() => OnPermError (new BrokerException (msg)' permanentFailedMessages));  			}  			// Do nothing with recoverable errors' they will be sent again next time  			var successMessages = successPartitionQueues.SelectMany (q => q.Queue.Get (q.CountInProgress)).ToArray ();  			if (OnSuccess != null && successMessages.Length != 0)  				await Task.Factory.StartNew (() => OnSuccess (successMessages));  		} catch (ThreadAbortException e) {  			// It happen when Watchdog kills stuck thread. We want stack trace to be logged with high priority  			_log.Fatal (e' "Send has been aborted. Probably hung thread detected");  		} catch (Exception e) {  			_log.Debug (e' "Exception while sending batch to topic '{0}' BrokerId {1}"' Configuration.Topic' brokerBatch.Leader);  			//brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError)));  		}  	});  	await Task.WhenAll (sendTasks);  } finally {  	queuesToBeSent.ForEach (q => q.InProgress = false);  	if (_log.IsDebugEnabled)  		_log.Debug ("Unlocked queue '{0}'/{1}"' Configuration.Topic' string.Join ("'"' queuesToBeSent.Select (q => q.Partition)));  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The following statement contains a magic number: try {  	var sendTasks = queuesToBeSent.Select (q => new {  		partsMeta.First (m => m.Id == q.Partition).Leader'  		Queue = q  	}).// leader'queue -> leader'queue[]  	GroupBy (q => q.Leader' (i' queues) => new {  		Leader = i'  		Queues = queues.Select (q1 => q1.Queue).ToArray ()  	}).Select (queues => new {  		queues.Leader'  		queues.Queues  	}).Select (async brokerBatch => {  		try {  			//  			// Walk queuese in zig-zag manner from the tail towards the head until we are withing size limit  			// The point of zig-zag is fairness: if producing is slow' we do not want to keep sending only first   			// partition in the queue while starving the last.  			//  			var maxSize = Configuration.MaxMessageSetSizeInBytes;  			var runningSize = 4;  			// array of messages len  			const int messageFixedSize = 8 + // offset  			4 + // message size   			4 + // crc  			1 + // magic  			1 + // attributes  			4 + // size of key array  			4;  			// size of value array  			brokerBatch.Queues.ForEach (q => q.CountInProgress = 0);  			var queueCount = brokerBatch.Queues.Length;  			int finishedQueues = 0;  			for (int i = 0; ; i = (i + 1) % queueCount) {  				// TODO: prevent enqueing of message of too lage size' which exceeds the limit.   				// Do it in syncronous part' before msg rich the queue  				if (i == 0)  					finishedQueues = 0;  				var queue = brokerBatch.Queues [i];  				// whole queue is already taken  				if (queue.Queue.Size == queue.CountInProgress) {  					finishedQueues++;  					if (finishedQueues == queueCount)  						break;  					continue;  				}  				var msg = queue.Queue.PeekSingle (queue.CountInProgress);  				var keyLen = msg.Key != null ? msg.Key.Length : 0;  				var msgLen = msg.Value != null ? msg.Value.Length : 0;  				var msgSize = messageFixedSize + keyLen + msgLen;  				if (runningSize + msgSize > maxSize)  					break;  				runningSize += msgSize;  				queue.CountInProgress++;  			}  			var messages = brokerBatch.Queues.SelectMany (q => q.Queue.PeekEnum (q.CountInProgress));  			// TODO: freeze permanent errors and throw consumer exceptions upon sending to perm error partition  			var response = await _cluster.SendBatchAsync (brokerBatch.Leader' messages' this);  			var failedResponsePartitions = response.Topics.Where (t => t.TopicName == Configuration.Topic).// should contain response only for our topic' but just in case...  			SelectMany (t => t.Partitions).Where (p => !p.ErrorCode.IsSuccess ()).ToArray ();  			// some errors' figure out which batches to dismiss from queues  			var failedPartitionIds = new HashSet<int> (failedResponsePartitions.Select (p => p.Partition));  			var successPartitionQueues = brokerBatch.Queues.Where (q => !failedPartitionIds.Contains (q.Partition)).ToArray ();  			var permanentErrorPartitions = failedResponsePartitions.Where (p => p.ErrorCode.IsPermanentFailure ()).ToArray ();  			var recoverableErrorPartitions = failedResponsePartitions.Except (permanentErrorPartitions).ToArray ();  			var permanentErrorPartitionIds = new HashSet<int> (permanentErrorPartitions.Select (p => p.Partition));  			// notify of errors from send response and trigger recovery monitor tracking them  			recoverableErrorPartitions.ForEach (failedPart => {  				_log.Warn ("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode);  				_cluster.NotifyPartitionStateChange (new PartitionStateChangeEvent (Topic' failedPart.Partition' failedPart.ErrorCode));  			});  			if (permanentErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  				EtwTrace.Log.ProducerPermanentFailure (_id' permanentErrorPartitions.Length);  				string error = string.Join ("' "' permanentErrorPartitions.Select (p => string.Format ("{0}:{1}"' p.Partition' p.ErrorCode)));  				error = string.Format ("[{0}]"' error);  				EtwTrace.Log.ProducerPermanentFailureDetails (_id' error);  			}  			if (recoverableErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  				EtwTrace.Log.ProducerRecoverableErrors (_id' recoverableErrorPartitions.Length);  			}  			// Pop permanently failed messages from the queue  			var permanentFailedMessages = brokerBatch.Queues.Where (q => permanentErrorPartitionIds.Contains (q.Partition)).SelectMany (q => q.Queue.GetEnum (q.CountInProgress)).ToArray ();  			// fire permanent error  			if (OnPermError != null && permanentFailedMessages.Length > 0) {  				var msg = string.Join ("'"' permanentErrorPartitions.Select (p => p.ErrorCode).Distinct ());  				msg = string.Format ("Produce request failed with errors: [{0}]"' msg);  				await Task.Factory.StartNew (() => OnPermError (new BrokerException (msg)' permanentFailedMessages));  			}  			// Do nothing with recoverable errors' they will be sent again next time  			var successMessages = successPartitionQueues.SelectMany (q => q.Queue.Get (q.CountInProgress)).ToArray ();  			if (OnSuccess != null && successMessages.Length != 0)  				await Task.Factory.StartNew (() => OnSuccess (successMessages));  		} catch (ThreadAbortException e) {  			// It happen when Watchdog kills stuck thread. We want stack trace to be logged with high priority  			_log.Fatal (e' "Send has been aborted. Probably hung thread detected");  		} catch (Exception e) {  			_log.Debug (e' "Exception while sending batch to topic '{0}' BrokerId {1}"' Configuration.Topic' brokerBatch.Leader);  			//brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError)));  		}  	});  	await Task.WhenAll (sendTasks);  } finally {  	queuesToBeSent.ForEach (q => q.InProgress = false);  	if (_log.IsDebugEnabled)  		_log.Debug ("Unlocked queue '{0}'/{1}"' Configuration.Topic' string.Join ("'"' queuesToBeSent.Select (q => q.Partition)));  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The following statement contains a magic number: try {  	var sendTasks = queuesToBeSent.Select (q => new {  		partsMeta.First (m => m.Id == q.Partition).Leader'  		Queue = q  	}).// leader'queue -> leader'queue[]  	GroupBy (q => q.Leader' (i' queues) => new {  		Leader = i'  		Queues = queues.Select (q1 => q1.Queue).ToArray ()  	}).Select (queues => new {  		queues.Leader'  		queues.Queues  	}).Select (async brokerBatch => {  		try {  			//  			// Walk queuese in zig-zag manner from the tail towards the head until we are withing size limit  			// The point of zig-zag is fairness: if producing is slow' we do not want to keep sending only first   			// partition in the queue while starving the last.  			//  			var maxSize = Configuration.MaxMessageSetSizeInBytes;  			var runningSize = 4;  			// array of messages len  			const int messageFixedSize = 8 + // offset  			4 + // message size   			4 + // crc  			1 + // magic  			1 + // attributes  			4 + // size of key array  			4;  			// size of value array  			brokerBatch.Queues.ForEach (q => q.CountInProgress = 0);  			var queueCount = brokerBatch.Queues.Length;  			int finishedQueues = 0;  			for (int i = 0; ; i = (i + 1) % queueCount) {  				// TODO: prevent enqueing of message of too lage size' which exceeds the limit.   				// Do it in syncronous part' before msg rich the queue  				if (i == 0)  					finishedQueues = 0;  				var queue = brokerBatch.Queues [i];  				// whole queue is already taken  				if (queue.Queue.Size == queue.CountInProgress) {  					finishedQueues++;  					if (finishedQueues == queueCount)  						break;  					continue;  				}  				var msg = queue.Queue.PeekSingle (queue.CountInProgress);  				var keyLen = msg.Key != null ? msg.Key.Length : 0;  				var msgLen = msg.Value != null ? msg.Value.Length : 0;  				var msgSize = messageFixedSize + keyLen + msgLen;  				if (runningSize + msgSize > maxSize)  					break;  				runningSize += msgSize;  				queue.CountInProgress++;  			}  			var messages = brokerBatch.Queues.SelectMany (q => q.Queue.PeekEnum (q.CountInProgress));  			// TODO: freeze permanent errors and throw consumer exceptions upon sending to perm error partition  			var response = await _cluster.SendBatchAsync (brokerBatch.Leader' messages' this);  			var failedResponsePartitions = response.Topics.Where (t => t.TopicName == Configuration.Topic).// should contain response only for our topic' but just in case...  			SelectMany (t => t.Partitions).Where (p => !p.ErrorCode.IsSuccess ()).ToArray ();  			// some errors' figure out which batches to dismiss from queues  			var failedPartitionIds = new HashSet<int> (failedResponsePartitions.Select (p => p.Partition));  			var successPartitionQueues = brokerBatch.Queues.Where (q => !failedPartitionIds.Contains (q.Partition)).ToArray ();  			var permanentErrorPartitions = failedResponsePartitions.Where (p => p.ErrorCode.IsPermanentFailure ()).ToArray ();  			var recoverableErrorPartitions = failedResponsePartitions.Except (permanentErrorPartitions).ToArray ();  			var permanentErrorPartitionIds = new HashSet<int> (permanentErrorPartitions.Select (p => p.Partition));  			// notify of errors from send response and trigger recovery monitor tracking them  			recoverableErrorPartitions.ForEach (failedPart => {  				_log.Warn ("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode);  				_cluster.NotifyPartitionStateChange (new PartitionStateChangeEvent (Topic' failedPart.Partition' failedPart.ErrorCode));  			});  			if (permanentErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  				EtwTrace.Log.ProducerPermanentFailure (_id' permanentErrorPartitions.Length);  				string error = string.Join ("' "' permanentErrorPartitions.Select (p => string.Format ("{0}:{1}"' p.Partition' p.ErrorCode)));  				error = string.Format ("[{0}]"' error);  				EtwTrace.Log.ProducerPermanentFailureDetails (_id' error);  			}  			if (recoverableErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  				EtwTrace.Log.ProducerRecoverableErrors (_id' recoverableErrorPartitions.Length);  			}  			// Pop permanently failed messages from the queue  			var permanentFailedMessages = brokerBatch.Queues.Where (q => permanentErrorPartitionIds.Contains (q.Partition)).SelectMany (q => q.Queue.GetEnum (q.CountInProgress)).ToArray ();  			// fire permanent error  			if (OnPermError != null && permanentFailedMessages.Length > 0) {  				var msg = string.Join ("'"' permanentErrorPartitions.Select (p => p.ErrorCode).Distinct ());  				msg = string.Format ("Produce request failed with errors: [{0}]"' msg);  				await Task.Factory.StartNew (() => OnPermError (new BrokerException (msg)' permanentFailedMessages));  			}  			// Do nothing with recoverable errors' they will be sent again next time  			var successMessages = successPartitionQueues.SelectMany (q => q.Queue.Get (q.CountInProgress)).ToArray ();  			if (OnSuccess != null && successMessages.Length != 0)  				await Task.Factory.StartNew (() => OnSuccess (successMessages));  		} catch (ThreadAbortException e) {  			// It happen when Watchdog kills stuck thread. We want stack trace to be logged with high priority  			_log.Fatal (e' "Send has been aborted. Probably hung thread detected");  		} catch (Exception e) {  			_log.Debug (e' "Exception while sending batch to topic '{0}' BrokerId {1}"' Configuration.Topic' brokerBatch.Leader);  			//brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError)));  		}  	});  	await Task.WhenAll (sendTasks);  } finally {  	queuesToBeSent.ForEach (q => q.InProgress = false);  	if (_log.IsDebugEnabled)  		_log.Debug ("Unlocked queue '{0}'/{1}"' Configuration.Topic' string.Join ("'"' queuesToBeSent.Select (q => q.Partition)));  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The following statement contains a magic number: try {  	var sendTasks = queuesToBeSent.Select (q => new {  		partsMeta.First (m => m.Id == q.Partition).Leader'  		Queue = q  	}).// leader'queue -> leader'queue[]  	GroupBy (q => q.Leader' (i' queues) => new {  		Leader = i'  		Queues = queues.Select (q1 => q1.Queue).ToArray ()  	}).Select (queues => new {  		queues.Leader'  		queues.Queues  	}).Select (async brokerBatch => {  		try {  			//  			// Walk queuese in zig-zag manner from the tail towards the head until we are withing size limit  			// The point of zig-zag is fairness: if producing is slow' we do not want to keep sending only first   			// partition in the queue while starving the last.  			//  			var maxSize = Configuration.MaxMessageSetSizeInBytes;  			var runningSize = 4;  			// array of messages len  			const int messageFixedSize = 8 + // offset  			4 + // message size   			4 + // crc  			1 + // magic  			1 + // attributes  			4 + // size of key array  			4;  			// size of value array  			brokerBatch.Queues.ForEach (q => q.CountInProgress = 0);  			var queueCount = brokerBatch.Queues.Length;  			int finishedQueues = 0;  			for (int i = 0; ; i = (i + 1) % queueCount) {  				// TODO: prevent enqueing of message of too lage size' which exceeds the limit.   				// Do it in syncronous part' before msg rich the queue  				if (i == 0)  					finishedQueues = 0;  				var queue = brokerBatch.Queues [i];  				// whole queue is already taken  				if (queue.Queue.Size == queue.CountInProgress) {  					finishedQueues++;  					if (finishedQueues == queueCount)  						break;  					continue;  				}  				var msg = queue.Queue.PeekSingle (queue.CountInProgress);  				var keyLen = msg.Key != null ? msg.Key.Length : 0;  				var msgLen = msg.Value != null ? msg.Value.Length : 0;  				var msgSize = messageFixedSize + keyLen + msgLen;  				if (runningSize + msgSize > maxSize)  					break;  				runningSize += msgSize;  				queue.CountInProgress++;  			}  			var messages = brokerBatch.Queues.SelectMany (q => q.Queue.PeekEnum (q.CountInProgress));  			// TODO: freeze permanent errors and throw consumer exceptions upon sending to perm error partition  			var response = await _cluster.SendBatchAsync (brokerBatch.Leader' messages' this);  			var failedResponsePartitions = response.Topics.Where (t => t.TopicName == Configuration.Topic).// should contain response only for our topic' but just in case...  			SelectMany (t => t.Partitions).Where (p => !p.ErrorCode.IsSuccess ()).ToArray ();  			// some errors' figure out which batches to dismiss from queues  			var failedPartitionIds = new HashSet<int> (failedResponsePartitions.Select (p => p.Partition));  			var successPartitionQueues = brokerBatch.Queues.Where (q => !failedPartitionIds.Contains (q.Partition)).ToArray ();  			var permanentErrorPartitions = failedResponsePartitions.Where (p => p.ErrorCode.IsPermanentFailure ()).ToArray ();  			var recoverableErrorPartitions = failedResponsePartitions.Except (permanentErrorPartitions).ToArray ();  			var permanentErrorPartitionIds = new HashSet<int> (permanentErrorPartitions.Select (p => p.Partition));  			// notify of errors from send response and trigger recovery monitor tracking them  			recoverableErrorPartitions.ForEach (failedPart => {  				_log.Warn ("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode);  				_cluster.NotifyPartitionStateChange (new PartitionStateChangeEvent (Topic' failedPart.Partition' failedPart.ErrorCode));  			});  			if (permanentErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  				EtwTrace.Log.ProducerPermanentFailure (_id' permanentErrorPartitions.Length);  				string error = string.Join ("' "' permanentErrorPartitions.Select (p => string.Format ("{0}:{1}"' p.Partition' p.ErrorCode)));  				error = string.Format ("[{0}]"' error);  				EtwTrace.Log.ProducerPermanentFailureDetails (_id' error);  			}  			if (recoverableErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  				EtwTrace.Log.ProducerRecoverableErrors (_id' recoverableErrorPartitions.Length);  			}  			// Pop permanently failed messages from the queue  			var permanentFailedMessages = brokerBatch.Queues.Where (q => permanentErrorPartitionIds.Contains (q.Partition)).SelectMany (q => q.Queue.GetEnum (q.CountInProgress)).ToArray ();  			// fire permanent error  			if (OnPermError != null && permanentFailedMessages.Length > 0) {  				var msg = string.Join ("'"' permanentErrorPartitions.Select (p => p.ErrorCode).Distinct ());  				msg = string.Format ("Produce request failed with errors: [{0}]"' msg);  				await Task.Factory.StartNew (() => OnPermError (new BrokerException (msg)' permanentFailedMessages));  			}  			// Do nothing with recoverable errors' they will be sent again next time  			var successMessages = successPartitionQueues.SelectMany (q => q.Queue.Get (q.CountInProgress)).ToArray ();  			if (OnSuccess != null && successMessages.Length != 0)  				await Task.Factory.StartNew (() => OnSuccess (successMessages));  		} catch (ThreadAbortException e) {  			// It happen when Watchdog kills stuck thread. We want stack trace to be logged with high priority  			_log.Fatal (e' "Send has been aborted. Probably hung thread detected");  		} catch (Exception e) {  			_log.Debug (e' "Exception while sending batch to topic '{0}' BrokerId {1}"' Configuration.Topic' brokerBatch.Leader);  			//brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError)));  		}  	});  	await Task.WhenAll (sendTasks);  } finally {  	queuesToBeSent.ForEach (q => q.InProgress = false);  	if (_log.IsDebugEnabled)  		_log.Debug ("Unlocked queue '{0}'/{1}"' Configuration.Topic' string.Join ("'"' queuesToBeSent.Select (q => q.Partition)));  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The following statement contains a magic number: try {  	//  	// Walk queuese in zig-zag manner from the tail towards the head until we are withing size limit  	// The point of zig-zag is fairness: if producing is slow' we do not want to keep sending only first   	// partition in the queue while starving the last.  	//  	var maxSize = Configuration.MaxMessageSetSizeInBytes;  	var runningSize = 4;  	// array of messages len  	const int messageFixedSize = 8 + // offset  	4 + // message size   	4 + // crc  	1 + // magic  	1 + // attributes  	4 + // size of key array  	4;  	// size of value array  	brokerBatch.Queues.ForEach (q => q.CountInProgress = 0);  	var queueCount = brokerBatch.Queues.Length;  	int finishedQueues = 0;  	for (int i = 0; ; i = (i + 1) % queueCount) {  		// TODO: prevent enqueing of message of too lage size' which exceeds the limit.   		// Do it in syncronous part' before msg rich the queue  		if (i == 0)  			finishedQueues = 0;  		var queue = brokerBatch.Queues [i];  		// whole queue is already taken  		if (queue.Queue.Size == queue.CountInProgress) {  			finishedQueues++;  			if (finishedQueues == queueCount)  				break;  			continue;  		}  		var msg = queue.Queue.PeekSingle (queue.CountInProgress);  		var keyLen = msg.Key != null ? msg.Key.Length : 0;  		var msgLen = msg.Value != null ? msg.Value.Length : 0;  		var msgSize = messageFixedSize + keyLen + msgLen;  		if (runningSize + msgSize > maxSize)  			break;  		runningSize += msgSize;  		queue.CountInProgress++;  	}  	var messages = brokerBatch.Queues.SelectMany (q => q.Queue.PeekEnum (q.CountInProgress));  	// TODO: freeze permanent errors and throw consumer exceptions upon sending to perm error partition  	var response = await _cluster.SendBatchAsync (brokerBatch.Leader' messages' this);  	var failedResponsePartitions = response.Topics.Where (t => t.TopicName == Configuration.Topic).// should contain response only for our topic' but just in case...  	SelectMany (t => t.Partitions).Where (p => !p.ErrorCode.IsSuccess ()).ToArray ();  	// some errors' figure out which batches to dismiss from queues  	var failedPartitionIds = new HashSet<int> (failedResponsePartitions.Select (p => p.Partition));  	var successPartitionQueues = brokerBatch.Queues.Where (q => !failedPartitionIds.Contains (q.Partition)).ToArray ();  	var permanentErrorPartitions = failedResponsePartitions.Where (p => p.ErrorCode.IsPermanentFailure ()).ToArray ();  	var recoverableErrorPartitions = failedResponsePartitions.Except (permanentErrorPartitions).ToArray ();  	var permanentErrorPartitionIds = new HashSet<int> (permanentErrorPartitions.Select (p => p.Partition));  	// notify of errors from send response and trigger recovery monitor tracking them  	recoverableErrorPartitions.ForEach (failedPart => {  		_log.Warn ("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode);  		_cluster.NotifyPartitionStateChange (new PartitionStateChangeEvent (Topic' failedPart.Partition' failedPart.ErrorCode));  	});  	if (permanentErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  		EtwTrace.Log.ProducerPermanentFailure (_id' permanentErrorPartitions.Length);  		string error = string.Join ("' "' permanentErrorPartitions.Select (p => string.Format ("{0}:{1}"' p.Partition' p.ErrorCode)));  		error = string.Format ("[{0}]"' error);  		EtwTrace.Log.ProducerPermanentFailureDetails (_id' error);  	}  	if (recoverableErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  		EtwTrace.Log.ProducerRecoverableErrors (_id' recoverableErrorPartitions.Length);  	}  	// Pop permanently failed messages from the queue  	var permanentFailedMessages = brokerBatch.Queues.Where (q => permanentErrorPartitionIds.Contains (q.Partition)).SelectMany (q => q.Queue.GetEnum (q.CountInProgress)).ToArray ();  	// fire permanent error  	if (OnPermError != null && permanentFailedMessages.Length > 0) {  		var msg = string.Join ("'"' permanentErrorPartitions.Select (p => p.ErrorCode).Distinct ());  		msg = string.Format ("Produce request failed with errors: [{0}]"' msg);  		await Task.Factory.StartNew (() => OnPermError (new BrokerException (msg)' permanentFailedMessages));  	}  	// Do nothing with recoverable errors' they will be sent again next time  	var successMessages = successPartitionQueues.SelectMany (q => q.Queue.Get (q.CountInProgress)).ToArray ();  	if (OnSuccess != null && successMessages.Length != 0)  		await Task.Factory.StartNew (() => OnSuccess (successMessages));  } catch (ThreadAbortException e) {  	// It happen when Watchdog kills stuck thread. We want stack trace to be logged with high priority  	_log.Fatal (e' "Send has been aborted. Probably hung thread detected");  } catch (Exception e) {  	_log.Debug (e' "Exception while sending batch to topic '{0}' BrokerId {1}"' Configuration.Topic' brokerBatch.Leader);  	//brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError)));  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The following statement contains a magic number: try {  	//  	// Walk queuese in zig-zag manner from the tail towards the head until we are withing size limit  	// The point of zig-zag is fairness: if producing is slow' we do not want to keep sending only first   	// partition in the queue while starving the last.  	//  	var maxSize = Configuration.MaxMessageSetSizeInBytes;  	var runningSize = 4;  	// array of messages len  	const int messageFixedSize = 8 + // offset  	4 + // message size   	4 + // crc  	1 + // magic  	1 + // attributes  	4 + // size of key array  	4;  	// size of value array  	brokerBatch.Queues.ForEach (q => q.CountInProgress = 0);  	var queueCount = brokerBatch.Queues.Length;  	int finishedQueues = 0;  	for (int i = 0; ; i = (i + 1) % queueCount) {  		// TODO: prevent enqueing of message of too lage size' which exceeds the limit.   		// Do it in syncronous part' before msg rich the queue  		if (i == 0)  			finishedQueues = 0;  		var queue = brokerBatch.Queues [i];  		// whole queue is already taken  		if (queue.Queue.Size == queue.CountInProgress) {  			finishedQueues++;  			if (finishedQueues == queueCount)  				break;  			continue;  		}  		var msg = queue.Queue.PeekSingle (queue.CountInProgress);  		var keyLen = msg.Key != null ? msg.Key.Length : 0;  		var msgLen = msg.Value != null ? msg.Value.Length : 0;  		var msgSize = messageFixedSize + keyLen + msgLen;  		if (runningSize + msgSize > maxSize)  			break;  		runningSize += msgSize;  		queue.CountInProgress++;  	}  	var messages = brokerBatch.Queues.SelectMany (q => q.Queue.PeekEnum (q.CountInProgress));  	// TODO: freeze permanent errors and throw consumer exceptions upon sending to perm error partition  	var response = await _cluster.SendBatchAsync (brokerBatch.Leader' messages' this);  	var failedResponsePartitions = response.Topics.Where (t => t.TopicName == Configuration.Topic).// should contain response only for our topic' but just in case...  	SelectMany (t => t.Partitions).Where (p => !p.ErrorCode.IsSuccess ()).ToArray ();  	// some errors' figure out which batches to dismiss from queues  	var failedPartitionIds = new HashSet<int> (failedResponsePartitions.Select (p => p.Partition));  	var successPartitionQueues = brokerBatch.Queues.Where (q => !failedPartitionIds.Contains (q.Partition)).ToArray ();  	var permanentErrorPartitions = failedResponsePartitions.Where (p => p.ErrorCode.IsPermanentFailure ()).ToArray ();  	var recoverableErrorPartitions = failedResponsePartitions.Except (permanentErrorPartitions).ToArray ();  	var permanentErrorPartitionIds = new HashSet<int> (permanentErrorPartitions.Select (p => p.Partition));  	// notify of errors from send response and trigger recovery monitor tracking them  	recoverableErrorPartitions.ForEach (failedPart => {  		_log.Warn ("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode);  		_cluster.NotifyPartitionStateChange (new PartitionStateChangeEvent (Topic' failedPart.Partition' failedPart.ErrorCode));  	});  	if (permanentErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  		EtwTrace.Log.ProducerPermanentFailure (_id' permanentErrorPartitions.Length);  		string error = string.Join ("' "' permanentErrorPartitions.Select (p => string.Format ("{0}:{1}"' p.Partition' p.ErrorCode)));  		error = string.Format ("[{0}]"' error);  		EtwTrace.Log.ProducerPermanentFailureDetails (_id' error);  	}  	if (recoverableErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  		EtwTrace.Log.ProducerRecoverableErrors (_id' recoverableErrorPartitions.Length);  	}  	// Pop permanently failed messages from the queue  	var permanentFailedMessages = brokerBatch.Queues.Where (q => permanentErrorPartitionIds.Contains (q.Partition)).SelectMany (q => q.Queue.GetEnum (q.CountInProgress)).ToArray ();  	// fire permanent error  	if (OnPermError != null && permanentFailedMessages.Length > 0) {  		var msg = string.Join ("'"' permanentErrorPartitions.Select (p => p.ErrorCode).Distinct ());  		msg = string.Format ("Produce request failed with errors: [{0}]"' msg);  		await Task.Factory.StartNew (() => OnPermError (new BrokerException (msg)' permanentFailedMessages));  	}  	// Do nothing with recoverable errors' they will be sent again next time  	var successMessages = successPartitionQueues.SelectMany (q => q.Queue.Get (q.CountInProgress)).ToArray ();  	if (OnSuccess != null && successMessages.Length != 0)  		await Task.Factory.StartNew (() => OnSuccess (successMessages));  } catch (ThreadAbortException e) {  	// It happen when Watchdog kills stuck thread. We want stack trace to be logged with high priority  	_log.Fatal (e' "Send has been aborted. Probably hung thread detected");  } catch (Exception e) {  	_log.Debug (e' "Exception while sending batch to topic '{0}' BrokerId {1}"' Configuration.Topic' brokerBatch.Leader);  	//brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError)));  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The following statement contains a magic number: try {  	//  	// Walk queuese in zig-zag manner from the tail towards the head until we are withing size limit  	// The point of zig-zag is fairness: if producing is slow' we do not want to keep sending only first   	// partition in the queue while starving the last.  	//  	var maxSize = Configuration.MaxMessageSetSizeInBytes;  	var runningSize = 4;  	// array of messages len  	const int messageFixedSize = 8 + // offset  	4 + // message size   	4 + // crc  	1 + // magic  	1 + // attributes  	4 + // size of key array  	4;  	// size of value array  	brokerBatch.Queues.ForEach (q => q.CountInProgress = 0);  	var queueCount = brokerBatch.Queues.Length;  	int finishedQueues = 0;  	for (int i = 0; ; i = (i + 1) % queueCount) {  		// TODO: prevent enqueing of message of too lage size' which exceeds the limit.   		// Do it in syncronous part' before msg rich the queue  		if (i == 0)  			finishedQueues = 0;  		var queue = brokerBatch.Queues [i];  		// whole queue is already taken  		if (queue.Queue.Size == queue.CountInProgress) {  			finishedQueues++;  			if (finishedQueues == queueCount)  				break;  			continue;  		}  		var msg = queue.Queue.PeekSingle (queue.CountInProgress);  		var keyLen = msg.Key != null ? msg.Key.Length : 0;  		var msgLen = msg.Value != null ? msg.Value.Length : 0;  		var msgSize = messageFixedSize + keyLen + msgLen;  		if (runningSize + msgSize > maxSize)  			break;  		runningSize += msgSize;  		queue.CountInProgress++;  	}  	var messages = brokerBatch.Queues.SelectMany (q => q.Queue.PeekEnum (q.CountInProgress));  	// TODO: freeze permanent errors and throw consumer exceptions upon sending to perm error partition  	var response = await _cluster.SendBatchAsync (brokerBatch.Leader' messages' this);  	var failedResponsePartitions = response.Topics.Where (t => t.TopicName == Configuration.Topic).// should contain response only for our topic' but just in case...  	SelectMany (t => t.Partitions).Where (p => !p.ErrorCode.IsSuccess ()).ToArray ();  	// some errors' figure out which batches to dismiss from queues  	var failedPartitionIds = new HashSet<int> (failedResponsePartitions.Select (p => p.Partition));  	var successPartitionQueues = brokerBatch.Queues.Where (q => !failedPartitionIds.Contains (q.Partition)).ToArray ();  	var permanentErrorPartitions = failedResponsePartitions.Where (p => p.ErrorCode.IsPermanentFailure ()).ToArray ();  	var recoverableErrorPartitions = failedResponsePartitions.Except (permanentErrorPartitions).ToArray ();  	var permanentErrorPartitionIds = new HashSet<int> (permanentErrorPartitions.Select (p => p.Partition));  	// notify of errors from send response and trigger recovery monitor tracking them  	recoverableErrorPartitions.ForEach (failedPart => {  		_log.Warn ("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode);  		_cluster.NotifyPartitionStateChange (new PartitionStateChangeEvent (Topic' failedPart.Partition' failedPart.ErrorCode));  	});  	if (permanentErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  		EtwTrace.Log.ProducerPermanentFailure (_id' permanentErrorPartitions.Length);  		string error = string.Join ("' "' permanentErrorPartitions.Select (p => string.Format ("{0}:{1}"' p.Partition' p.ErrorCode)));  		error = string.Format ("[{0}]"' error);  		EtwTrace.Log.ProducerPermanentFailureDetails (_id' error);  	}  	if (recoverableErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  		EtwTrace.Log.ProducerRecoverableErrors (_id' recoverableErrorPartitions.Length);  	}  	// Pop permanently failed messages from the queue  	var permanentFailedMessages = brokerBatch.Queues.Where (q => permanentErrorPartitionIds.Contains (q.Partition)).SelectMany (q => q.Queue.GetEnum (q.CountInProgress)).ToArray ();  	// fire permanent error  	if (OnPermError != null && permanentFailedMessages.Length > 0) {  		var msg = string.Join ("'"' permanentErrorPartitions.Select (p => p.ErrorCode).Distinct ());  		msg = string.Format ("Produce request failed with errors: [{0}]"' msg);  		await Task.Factory.StartNew (() => OnPermError (new BrokerException (msg)' permanentFailedMessages));  	}  	// Do nothing with recoverable errors' they will be sent again next time  	var successMessages = successPartitionQueues.SelectMany (q => q.Queue.Get (q.CountInProgress)).ToArray ();  	if (OnSuccess != null && successMessages.Length != 0)  		await Task.Factory.StartNew (() => OnSuccess (successMessages));  } catch (ThreadAbortException e) {  	// It happen when Watchdog kills stuck thread. We want stack trace to be logged with high priority  	_log.Fatal (e' "Send has been aborted. Probably hung thread detected");  } catch (Exception e) {  	_log.Debug (e' "Exception while sending batch to topic '{0}' BrokerId {1}"' Configuration.Topic' brokerBatch.Leader);  	//brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError)));  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The following statement contains a magic number: try {  	//  	// Walk queuese in zig-zag manner from the tail towards the head until we are withing size limit  	// The point of zig-zag is fairness: if producing is slow' we do not want to keep sending only first   	// partition in the queue while starving the last.  	//  	var maxSize = Configuration.MaxMessageSetSizeInBytes;  	var runningSize = 4;  	// array of messages len  	const int messageFixedSize = 8 + // offset  	4 + // message size   	4 + // crc  	1 + // magic  	1 + // attributes  	4 + // size of key array  	4;  	// size of value array  	brokerBatch.Queues.ForEach (q => q.CountInProgress = 0);  	var queueCount = brokerBatch.Queues.Length;  	int finishedQueues = 0;  	for (int i = 0; ; i = (i + 1) % queueCount) {  		// TODO: prevent enqueing of message of too lage size' which exceeds the limit.   		// Do it in syncronous part' before msg rich the queue  		if (i == 0)  			finishedQueues = 0;  		var queue = brokerBatch.Queues [i];  		// whole queue is already taken  		if (queue.Queue.Size == queue.CountInProgress) {  			finishedQueues++;  			if (finishedQueues == queueCount)  				break;  			continue;  		}  		var msg = queue.Queue.PeekSingle (queue.CountInProgress);  		var keyLen = msg.Key != null ? msg.Key.Length : 0;  		var msgLen = msg.Value != null ? msg.Value.Length : 0;  		var msgSize = messageFixedSize + keyLen + msgLen;  		if (runningSize + msgSize > maxSize)  			break;  		runningSize += msgSize;  		queue.CountInProgress++;  	}  	var messages = brokerBatch.Queues.SelectMany (q => q.Queue.PeekEnum (q.CountInProgress));  	// TODO: freeze permanent errors and throw consumer exceptions upon sending to perm error partition  	var response = await _cluster.SendBatchAsync (brokerBatch.Leader' messages' this);  	var failedResponsePartitions = response.Topics.Where (t => t.TopicName == Configuration.Topic).// should contain response only for our topic' but just in case...  	SelectMany (t => t.Partitions).Where (p => !p.ErrorCode.IsSuccess ()).ToArray ();  	// some errors' figure out which batches to dismiss from queues  	var failedPartitionIds = new HashSet<int> (failedResponsePartitions.Select (p => p.Partition));  	var successPartitionQueues = brokerBatch.Queues.Where (q => !failedPartitionIds.Contains (q.Partition)).ToArray ();  	var permanentErrorPartitions = failedResponsePartitions.Where (p => p.ErrorCode.IsPermanentFailure ()).ToArray ();  	var recoverableErrorPartitions = failedResponsePartitions.Except (permanentErrorPartitions).ToArray ();  	var permanentErrorPartitionIds = new HashSet<int> (permanentErrorPartitions.Select (p => p.Partition));  	// notify of errors from send response and trigger recovery monitor tracking them  	recoverableErrorPartitions.ForEach (failedPart => {  		_log.Warn ("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode);  		_cluster.NotifyPartitionStateChange (new PartitionStateChangeEvent (Topic' failedPart.Partition' failedPart.ErrorCode));  	});  	if (permanentErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  		EtwTrace.Log.ProducerPermanentFailure (_id' permanentErrorPartitions.Length);  		string error = string.Join ("' "' permanentErrorPartitions.Select (p => string.Format ("{0}:{1}"' p.Partition' p.ErrorCode)));  		error = string.Format ("[{0}]"' error);  		EtwTrace.Log.ProducerPermanentFailureDetails (_id' error);  	}  	if (recoverableErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  		EtwTrace.Log.ProducerRecoverableErrors (_id' recoverableErrorPartitions.Length);  	}  	// Pop permanently failed messages from the queue  	var permanentFailedMessages = brokerBatch.Queues.Where (q => permanentErrorPartitionIds.Contains (q.Partition)).SelectMany (q => q.Queue.GetEnum (q.CountInProgress)).ToArray ();  	// fire permanent error  	if (OnPermError != null && permanentFailedMessages.Length > 0) {  		var msg = string.Join ("'"' permanentErrorPartitions.Select (p => p.ErrorCode).Distinct ());  		msg = string.Format ("Produce request failed with errors: [{0}]"' msg);  		await Task.Factory.StartNew (() => OnPermError (new BrokerException (msg)' permanentFailedMessages));  	}  	// Do nothing with recoverable errors' they will be sent again next time  	var successMessages = successPartitionQueues.SelectMany (q => q.Queue.Get (q.CountInProgress)).ToArray ();  	if (OnSuccess != null && successMessages.Length != 0)  		await Task.Factory.StartNew (() => OnSuccess (successMessages));  } catch (ThreadAbortException e) {  	// It happen when Watchdog kills stuck thread. We want stack trace to be logged with high priority  	_log.Fatal (e' "Send has been aborted. Probably hung thread detected");  } catch (Exception e) {  	_log.Debug (e' "Exception while sending batch to topic '{0}' BrokerId {1}"' Configuration.Topic' brokerBatch.Leader);  	//brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError)));  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The following statement contains a magic number: try {  	//  	// Walk queuese in zig-zag manner from the tail towards the head until we are withing size limit  	// The point of zig-zag is fairness: if producing is slow' we do not want to keep sending only first   	// partition in the queue while starving the last.  	//  	var maxSize = Configuration.MaxMessageSetSizeInBytes;  	var runningSize = 4;  	// array of messages len  	const int messageFixedSize = 8 + // offset  	4 + // message size   	4 + // crc  	1 + // magic  	1 + // attributes  	4 + // size of key array  	4;  	// size of value array  	brokerBatch.Queues.ForEach (q => q.CountInProgress = 0);  	var queueCount = brokerBatch.Queues.Length;  	int finishedQueues = 0;  	for (int i = 0; ; i = (i + 1) % queueCount) {  		// TODO: prevent enqueing of message of too lage size' which exceeds the limit.   		// Do it in syncronous part' before msg rich the queue  		if (i == 0)  			finishedQueues = 0;  		var queue = brokerBatch.Queues [i];  		// whole queue is already taken  		if (queue.Queue.Size == queue.CountInProgress) {  			finishedQueues++;  			if (finishedQueues == queueCount)  				break;  			continue;  		}  		var msg = queue.Queue.PeekSingle (queue.CountInProgress);  		var keyLen = msg.Key != null ? msg.Key.Length : 0;  		var msgLen = msg.Value != null ? msg.Value.Length : 0;  		var msgSize = messageFixedSize + keyLen + msgLen;  		if (runningSize + msgSize > maxSize)  			break;  		runningSize += msgSize;  		queue.CountInProgress++;  	}  	var messages = brokerBatch.Queues.SelectMany (q => q.Queue.PeekEnum (q.CountInProgress));  	// TODO: freeze permanent errors and throw consumer exceptions upon sending to perm error partition  	var response = await _cluster.SendBatchAsync (brokerBatch.Leader' messages' this);  	var failedResponsePartitions = response.Topics.Where (t => t.TopicName == Configuration.Topic).// should contain response only for our topic' but just in case...  	SelectMany (t => t.Partitions).Where (p => !p.ErrorCode.IsSuccess ()).ToArray ();  	// some errors' figure out which batches to dismiss from queues  	var failedPartitionIds = new HashSet<int> (failedResponsePartitions.Select (p => p.Partition));  	var successPartitionQueues = brokerBatch.Queues.Where (q => !failedPartitionIds.Contains (q.Partition)).ToArray ();  	var permanentErrorPartitions = failedResponsePartitions.Where (p => p.ErrorCode.IsPermanentFailure ()).ToArray ();  	var recoverableErrorPartitions = failedResponsePartitions.Except (permanentErrorPartitions).ToArray ();  	var permanentErrorPartitionIds = new HashSet<int> (permanentErrorPartitions.Select (p => p.Partition));  	// notify of errors from send response and trigger recovery monitor tracking them  	recoverableErrorPartitions.ForEach (failedPart => {  		_log.Warn ("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode);  		_cluster.NotifyPartitionStateChange (new PartitionStateChangeEvent (Topic' failedPart.Partition' failedPart.ErrorCode));  	});  	if (permanentErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  		EtwTrace.Log.ProducerPermanentFailure (_id' permanentErrorPartitions.Length);  		string error = string.Join ("' "' permanentErrorPartitions.Select (p => string.Format ("{0}:{1}"' p.Partition' p.ErrorCode)));  		error = string.Format ("[{0}]"' error);  		EtwTrace.Log.ProducerPermanentFailureDetails (_id' error);  	}  	if (recoverableErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  		EtwTrace.Log.ProducerRecoverableErrors (_id' recoverableErrorPartitions.Length);  	}  	// Pop permanently failed messages from the queue  	var permanentFailedMessages = brokerBatch.Queues.Where (q => permanentErrorPartitionIds.Contains (q.Partition)).SelectMany (q => q.Queue.GetEnum (q.CountInProgress)).ToArray ();  	// fire permanent error  	if (OnPermError != null && permanentFailedMessages.Length > 0) {  		var msg = string.Join ("'"' permanentErrorPartitions.Select (p => p.ErrorCode).Distinct ());  		msg = string.Format ("Produce request failed with errors: [{0}]"' msg);  		await Task.Factory.StartNew (() => OnPermError (new BrokerException (msg)' permanentFailedMessages));  	}  	// Do nothing with recoverable errors' they will be sent again next time  	var successMessages = successPartitionQueues.SelectMany (q => q.Queue.Get (q.CountInProgress)).ToArray ();  	if (OnSuccess != null && successMessages.Length != 0)  		await Task.Factory.StartNew (() => OnSuccess (successMessages));  } catch (ThreadAbortException e) {  	// It happen when Watchdog kills stuck thread. We want stack trace to be logged with high priority  	_log.Fatal (e' "Send has been aborted. Probably hung thread detected");  } catch (Exception e) {  	_log.Debug (e' "Exception while sending batch to topic '{0}' BrokerId {1}"' Configuration.Topic' brokerBatch.Leader);  	//brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError)));  }  
Magic Number,kafka4net,Producer,D:\newReposJune17\ntent-ad_kafka4net\src\Producer.cs,SendLoop,The following statement contains a magic number: try {  	//  	// Walk queuese in zig-zag manner from the tail towards the head until we are withing size limit  	// The point of zig-zag is fairness: if producing is slow' we do not want to keep sending only first   	// partition in the queue while starving the last.  	//  	var maxSize = Configuration.MaxMessageSetSizeInBytes;  	var runningSize = 4;  	// array of messages len  	const int messageFixedSize = 8 + // offset  	4 + // message size   	4 + // crc  	1 + // magic  	1 + // attributes  	4 + // size of key array  	4;  	// size of value array  	brokerBatch.Queues.ForEach (q => q.CountInProgress = 0);  	var queueCount = brokerBatch.Queues.Length;  	int finishedQueues = 0;  	for (int i = 0; ; i = (i + 1) % queueCount) {  		// TODO: prevent enqueing of message of too lage size' which exceeds the limit.   		// Do it in syncronous part' before msg rich the queue  		if (i == 0)  			finishedQueues = 0;  		var queue = brokerBatch.Queues [i];  		// whole queue is already taken  		if (queue.Queue.Size == queue.CountInProgress) {  			finishedQueues++;  			if (finishedQueues == queueCount)  				break;  			continue;  		}  		var msg = queue.Queue.PeekSingle (queue.CountInProgress);  		var keyLen = msg.Key != null ? msg.Key.Length : 0;  		var msgLen = msg.Value != null ? msg.Value.Length : 0;  		var msgSize = messageFixedSize + keyLen + msgLen;  		if (runningSize + msgSize > maxSize)  			break;  		runningSize += msgSize;  		queue.CountInProgress++;  	}  	var messages = brokerBatch.Queues.SelectMany (q => q.Queue.PeekEnum (q.CountInProgress));  	// TODO: freeze permanent errors and throw consumer exceptions upon sending to perm error partition  	var response = await _cluster.SendBatchAsync (brokerBatch.Leader' messages' this);  	var failedResponsePartitions = response.Topics.Where (t => t.TopicName == Configuration.Topic).// should contain response only for our topic' but just in case...  	SelectMany (t => t.Partitions).Where (p => !p.ErrorCode.IsSuccess ()).ToArray ();  	// some errors' figure out which batches to dismiss from queues  	var failedPartitionIds = new HashSet<int> (failedResponsePartitions.Select (p => p.Partition));  	var successPartitionQueues = brokerBatch.Queues.Where (q => !failedPartitionIds.Contains (q.Partition)).ToArray ();  	var permanentErrorPartitions = failedResponsePartitions.Where (p => p.ErrorCode.IsPermanentFailure ()).ToArray ();  	var recoverableErrorPartitions = failedResponsePartitions.Except (permanentErrorPartitions).ToArray ();  	var permanentErrorPartitionIds = new HashSet<int> (permanentErrorPartitions.Select (p => p.Partition));  	// notify of errors from send response and trigger recovery monitor tracking them  	recoverableErrorPartitions.ForEach (failedPart => {  		_log.Warn ("Produce Request Failed for topic {0} partition {1} with error {2}"' Topic' failedPart.Partition' failedPart.ErrorCode);  		_cluster.NotifyPartitionStateChange (new PartitionStateChangeEvent (Topic' failedPart.Partition' failedPart.ErrorCode));  	});  	if (permanentErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  		EtwTrace.Log.ProducerPermanentFailure (_id' permanentErrorPartitions.Length);  		string error = string.Join ("' "' permanentErrorPartitions.Select (p => string.Format ("{0}:{1}"' p.Partition' p.ErrorCode)));  		error = string.Format ("[{0}]"' error);  		EtwTrace.Log.ProducerPermanentFailureDetails (_id' error);  	}  	if (recoverableErrorPartitions.Length > 0 && EtwTrace.Log.IsEnabled ()) {  		EtwTrace.Log.ProducerRecoverableErrors (_id' recoverableErrorPartitions.Length);  	}  	// Pop permanently failed messages from the queue  	var permanentFailedMessages = brokerBatch.Queues.Where (q => permanentErrorPartitionIds.Contains (q.Partition)).SelectMany (q => q.Queue.GetEnum (q.CountInProgress)).ToArray ();  	// fire permanent error  	if (OnPermError != null && permanentFailedMessages.Length > 0) {  		var msg = string.Join ("'"' permanentErrorPartitions.Select (p => p.ErrorCode).Distinct ());  		msg = string.Format ("Produce request failed with errors: [{0}]"' msg);  		await Task.Factory.StartNew (() => OnPermError (new BrokerException (msg)' permanentFailedMessages));  	}  	// Do nothing with recoverable errors' they will be sent again next time  	var successMessages = successPartitionQueues.SelectMany (q => q.Queue.Get (q.CountInProgress)).ToArray ();  	if (OnSuccess != null && successMessages.Length != 0)  		await Task.Factory.StartNew (() => OnSuccess (successMessages));  } catch (ThreadAbortException e) {  	// It happen when Watchdog kills stuck thread. We want stack trace to be logged with high priority  	_log.Fatal (e' "Send has been aborted. Probably hung thread detected");  } catch (Exception e) {  	_log.Debug (e' "Exception while sending batch to topic '{0}' BrokerId {1}"' Configuration.Topic' brokerBatch.Leader);  	//brokerBatch.Queues.ForEach(partQueue => _cluster.NotifyPartitionStateChange(new Tuple<string' int' ErrorCode>(Topic' partQueue.Partition' ErrorCode.TransportError)));  }  
Magic Number,kafka4net,Cluster,D:\newReposJune17\ntent-ad_kafka4net\src\Cluster.cs,ConnectAsync,The following statement contains a magic number: await Scheduler.Ask (() => {  	// we cannot reconnect if we have closed already.  	if (_state == ClusterState.Closed)  		throw new BrokerException ("Cluster is already closed. Cannot reconnect. Please create a new Cluster.");  	if (_state != ClusterState.Disconnected)  		return false;  	_log.Debug ("Connecting");  	var initBrokers = Connection.ParseAddress (_seedBrokers).Select (seed => new BrokerMeta {  		Host = seed.Item1'  		Port = seed.Item2'  		NodeId = -99  	}).ToArray ();  	EtwTrace.Log.ClusterStarting (_id);  	var initMeta = new MetadataResponse {  		Topics = new TopicMeta[0]'  		Brokers = initBrokers  	};  	MergeTopicMeta (initMeta);  	_state = ClusterState.Connected;  	// start up a recovery monitor to watch for recovered partitions  	_partitionRecoveryMonitor = new PartitionRecoveryMonitor (this' _protocol' _cancel.Token);  	// Merge metadata that recovery monitor discovers  	_partitionRecoveryMonitor.NewMetadataEvents.Subscribe (MergeTopicMeta' ex => _log.Error (ex' "Error thrown by RecoveryMonitor.NewMetadataEvents!"));  	_log.Debug ("Connected");  	EtwTrace.Log.ClusterStarted (_id);  	return true;  }).ConfigureAwait (false);  
Magic Number,kafka4net,Cluster,D:\newReposJune17\ntent-ad_kafka4net\src\Cluster.cs,FetchPartitionOffsetsImplAsync,The following statement contains a magic number: while (!_cancel.IsCancellationRequested) {  	numAttempts++;  	bool retry;  	Exception exception;  	try {  		// get partition list  		var parts = (await GetOrFetchMetaForTopicAsync (topic)).Select (p => new OffsetRequest.PartitionData {  			Id = p.Id'  			Time = time'  			MaxNumOffsets = 1  		}).ToArray ();  		// group parts by broker  		var requests = (from part in parts  		let broker = FindBrokerMetaForPartitionId (topic' part.Id)  		group part by broker into brokerGrp  		let req = new OffsetRequest {  			TopicName = topic'  			Partitions = brokerGrp.ToArray ()  		}  		select _protocol.GetOffsets (req' brokerGrp.Key.Conn)).ToArray ();  		await Task.WhenAll (requests);  		// handle recoverable errors' such as tcp transport exceptions (with limited retry)  		// or partition relocation error codes  		if (requests.Any (r => r.IsFaulted))  			throw new AggregateException ("Failure when getting offsets info"' requests.Where (r => r.IsFaulted).Select (r => r.Exception));  		var partitions = (from r in requests  		from part in r.Result.Partitions  		select part).ToArray ();  		if (partitions.Any (p => !p.ErrorCode.IsSuccess ()))  			throw new Exception (string.Format ("Partition Errors: [{0}]"' string.Join ("'"' partitions.Select (p => p.Partition + ":" + p.ErrorCode))));  		//if (partitions.Any(p => (p.Offsets.Length == 1 ? -1 : p.Offsets[1])==-1))  		//    throw new Exception(string.Format("Partition Head Offset is -1 for partition(s): [{0}]"' string.Join("'"' partitions.Select(p => p.Partition + ":" + (p.Offsets.Length == 1 ? -1 : p.Offsets[1])))));  		return new TopicPartitionOffsets (topic' partitions.ToDictionary (tp => tp.Partition' tp => tp.Offsets.First ()));  	} catch (Exception ex) {  		retry = numAttempts < 4;  		exception = ex;  	}  	if (retry) {  		_log.Warn ("Could not fetch offsets for topic {0}. Will Retry. Message: {1}"' topic' exception.Message);  		MergeTopicMeta (await FetchMetaWithRetryAsync (topic));  		await Task.Delay (TimeSpan.FromSeconds (1));  	} else {  		var error = "Could not fetch offsets for topic {0} after 4 attempts! Failing call";  		_log.Fatal (exception' error' topic);  		throw new BrokerException (error' exception);  	}  }  
Magic Number,kafka4net,Cluster,D:\newReposJune17\ntent-ad_kafka4net\src\Cluster.cs,FetchPartitionOffsetsImplAsync,The following statement contains a magic number: try {  	// get partition list  	var parts = (await GetOrFetchMetaForTopicAsync (topic)).Select (p => new OffsetRequest.PartitionData {  		Id = p.Id'  		Time = time'  		MaxNumOffsets = 1  	}).ToArray ();  	// group parts by broker  	var requests = (from part in parts  	let broker = FindBrokerMetaForPartitionId (topic' part.Id)  	group part by broker into brokerGrp  	let req = new OffsetRequest {  		TopicName = topic'  		Partitions = brokerGrp.ToArray ()  	}  	select _protocol.GetOffsets (req' brokerGrp.Key.Conn)).ToArray ();  	await Task.WhenAll (requests);  	// handle recoverable errors' such as tcp transport exceptions (with limited retry)  	// or partition relocation error codes  	if (requests.Any (r => r.IsFaulted))  		throw new AggregateException ("Failure when getting offsets info"' requests.Where (r => r.IsFaulted).Select (r => r.Exception));  	var partitions = (from r in requests  	from part in r.Result.Partitions  	select part).ToArray ();  	if (partitions.Any (p => !p.ErrorCode.IsSuccess ()))  		throw new Exception (string.Format ("Partition Errors: [{0}]"' string.Join ("'"' partitions.Select (p => p.Partition + ":" + p.ErrorCode))));  	//if (partitions.Any(p => (p.Offsets.Length == 1 ? -1 : p.Offsets[1])==-1))  	//    throw new Exception(string.Format("Partition Head Offset is -1 for partition(s): [{0}]"' string.Join("'"' partitions.Select(p => p.Partition + ":" + (p.Offsets.Length == 1 ? -1 : p.Offsets[1])))));  	return new TopicPartitionOffsets (topic' partitions.ToDictionary (tp => tp.Partition' tp => tp.Offsets.First ()));  } catch (Exception ex) {  	retry = numAttempts < 4;  	exception = ex;  }  
Magic Number,kafka4net,Cluster,D:\newReposJune17\ntent-ad_kafka4net\src\Cluster.cs,FetchPartitionOffsetsImplAsync,The following statement contains a magic number: retry = numAttempts < 4;  
Magic Number,kafka4net,Cluster,D:\newReposJune17\ntent-ad_kafka4net\src\Cluster.cs,FetchMetaWithRetryAsync,The following statement contains a magic number: while (!_cancel.IsCancellationRequested) {  	try {  		_log.Debug ("FetchMetaWithRetryAsync: sending MetadataRequest...");  		var meta = await _protocol.MetadataRequest (new TopicRequest {  			Topics = new[] {  				topic  			}  		});  		_log.Debug ("FetchMetaWithRetryAsync: got MetadataResponse {0}"' meta);  		var errorCode = meta.Topics.Single ().ErrorCode;  		if (errorCode.IsSuccess ()) {  			_log.Debug ("Discovered topic: '{0}'"' topic);  			return meta;  		}  		if (!errorCode.IsPermanentFailure ()) {  			_log.Debug ("Topic: '{0}': LeaderNotAvailable"' topic);  			continue;  		}  		_log.Error ("Topic: '{0}': {1}"' topic' errorCode);  		throw new BrokerException (string.Format ("Can not fetch metadata for topic '{0}'. {1}"' topic' errorCode));  	} catch (Exception e) {  		if (_cancel.IsCancellationRequested)  			// cluster is shutting down' no big deal.  			_log.Info (e' "Exception during shutdown while trying to fetch topic '{0}' metadata"' topic);  		else {  			// if we got a CorrelationLoopException or ObjectDisposedException' it means the connection is no longer OK.   			// Check if it was asked to shut down (happens a lot when "seed" connections are replaced with actual ones)  			var cle = e as CorrelationLoopException;  			var ode = e as ObjectDisposedException;  			if (cle != null && cle.IsRequestedClose)  				_log.Info (e' "Connection requested close while trying to fetch topic '{0}' metadata' will retry."' topic);  			else if (ode != null)  				_log.Warn (e' "Connection Disposed while trying to fetch topic '{0}' metadata' will retry."' topic);  			else  				_log.Error (e' "Error while trying to fetch topic '{0}' metadata' will retry."' topic);  		}  	}  	await Task.Delay (500' _cancel.Token);  }  
Magic Number,kafka4net,Cluster,D:\newReposJune17\ntent-ad_kafka4net\src\Cluster.cs,FetchMetaWithRetryAsync,The following statement contains a magic number: await Task.Delay (500' _cancel.Token);  
Magic Number,kafka4net,Cluster,D:\newReposJune17\ntent-ad_kafka4net\src\Cluster.cs,MergeTopicMeta,The following statement contains a magic number: newBrokers.Where (b => b.NodeId != -99).ForEach (b => _newBrokerSubject.OnNext (b));  
Magic Number,kafka4net.Compression,KafkaSnappyStream,D:\newReposJune17\ntent-ad_kafka4net\src\Compression\KafkaSnappyStream.cs,AllocateBuffers,The following statement contains a magic number: uncompressedBuffer = new byte[32 * 1024];  
Magic Number,kafka4net.Compression,KafkaSnappyStream,D:\newReposJune17\ntent-ad_kafka4net\src\Compression\KafkaSnappyStream.cs,AllocateBuffers,The following statement contains a magic number: uncompressedBuffer = new byte[32 * 1024];  
Magic Number,kafka4net.Compression,KafkaSnappyStream,D:\newReposJune17\ntent-ad_kafka4net\src\Compression\KafkaSnappyStream.cs,WriteHeader,The following statement contains a magic number: _base.Write (_versionsHeader' 0' 8);  
Magic Number,kafka4net.Compression,KafkaSnappyStream,D:\newReposJune17\ntent-ad_kafka4net\src\Compression\KafkaSnappyStream.cs,ReadBlock,The following statement contains a magic number: if (!StreamUtils.ReadAll (_base' _compressedBuffer' 4))  	return false;  
Magic Number,kafka4net.Compression,Lz4KafkaStream,D:\newReposJune17\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,Flush,The following statement contains a magic number: if (compressedSize >= _bufferLen) {  	// Lz4 allows to write non-compressed block.   	// Reuse _ncompressedBuffer which is not needed anymore to serialize block size  	LittleEndianConverter.Write ((uint)(_bufferLen | 1 << 31)' _compressedBuffer' 0);  	// highest bit set indicates no compression  	_base.Write (_compressedBuffer' 0' 4);  	_base.Write (_uncompressedBuffer' 0' _bufferLen);  } else {  	LittleEndianConverter.Write ((uint)compressedSize' _uncompressedBuffer' 0);  	_base.Write (_uncompressedBuffer' 0' 4);  	_base.Write (_compressedBuffer' 0' compressedSize);  }  
Magic Number,kafka4net.Compression,Lz4KafkaStream,D:\newReposJune17\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,Flush,The following statement contains a magic number: if (compressedSize >= _bufferLen) {  	// Lz4 allows to write non-compressed block.   	// Reuse _ncompressedBuffer which is not needed anymore to serialize block size  	LittleEndianConverter.Write ((uint)(_bufferLen | 1 << 31)' _compressedBuffer' 0);  	// highest bit set indicates no compression  	_base.Write (_compressedBuffer' 0' 4);  	_base.Write (_uncompressedBuffer' 0' _bufferLen);  } else {  	LittleEndianConverter.Write ((uint)compressedSize' _uncompressedBuffer' 0);  	_base.Write (_uncompressedBuffer' 0' 4);  	_base.Write (_compressedBuffer' 0' compressedSize);  }  
Magic Number,kafka4net.Compression,Lz4KafkaStream,D:\newReposJune17\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,Flush,The following statement contains a magic number: if (compressedSize >= _bufferLen) {  	// Lz4 allows to write non-compressed block.   	// Reuse _ncompressedBuffer which is not needed anymore to serialize block size  	LittleEndianConverter.Write ((uint)(_bufferLen | 1 << 31)' _compressedBuffer' 0);  	// highest bit set indicates no compression  	_base.Write (_compressedBuffer' 0' 4);  	_base.Write (_uncompressedBuffer' 0' _bufferLen);  } else {  	LittleEndianConverter.Write ((uint)compressedSize' _uncompressedBuffer' 0);  	_base.Write (_uncompressedBuffer' 0' 4);  	_base.Write (_compressedBuffer' 0' compressedSize);  }  
Magic Number,kafka4net.Compression,Lz4KafkaStream,D:\newReposJune17\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,Flush,The following statement contains a magic number: LittleEndianConverter.Write ((uint)(_bufferLen | 1 << 31)' _compressedBuffer' 0);  
Magic Number,kafka4net.Compression,Lz4KafkaStream,D:\newReposJune17\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,Flush,The following statement contains a magic number: _base.Write (_compressedBuffer' 0' 4);  
Magic Number,kafka4net.Compression,Lz4KafkaStream,D:\newReposJune17\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,Flush,The following statement contains a magic number: _base.Write (_uncompressedBuffer' 0' 4);  
Magic Number,kafka4net.Compression,Lz4KafkaStream,D:\newReposJune17\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,ReadHeader,The following statement contains a magic number: if (!StreamUtils.ReadAll (_base' _headerBuffer' 7))  	return false;  
Magic Number,kafka4net.Compression,Lz4KafkaStream,D:\newReposJune17\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,ReadHeader,The following statement contains a magic number: _hasher.Update (_headerBuffer' 6);  
Magic Number,kafka4net.Compression,Lz4KafkaStream,D:\newReposJune17\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,ReadHeader,The following statement contains a magic number: if (calculatedChecksum != _headerBuffer [6])  	throw new InvalidDataException ("Lz4 Frame Descriptor checksum mismatch");  
Magic Number,kafka4net.Compression,Lz4KafkaStream,D:\newReposJune17\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,CreateHeader,The following statement contains a magic number: buff [4] = (byte)flags;  
Magic Number,kafka4net.Compression,Lz4KafkaStream,D:\newReposJune17\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,CreateHeader,The following statement contains a magic number: buff [5] = (byte)bd;  
Magic Number,kafka4net.Compression,Lz4KafkaStream,D:\newReposJune17\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,CreateHeader,The following statement contains a magic number: hasher.Update (buff' 6);  
Magic Number,kafka4net.Compression,Lz4KafkaStream,D:\newReposJune17\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,CreateHeader,The following statement contains a magic number: buff [6] = (byte)checksum;  
Magic Number,kafka4net.Compression,Lz4KafkaStream,D:\newReposJune17\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,ReadBlock,The following statement contains a magic number: if (!StreamUtils.ReadAll (_base' _uncompressedBuffer' 4))  	throw new InvalidDataException ("Unexpected end of LZ4 data block");  
Magic Number,kafka4net.Compression,Lz4KafkaStream,D:\newReposJune17\ntent-ad_kafka4net\src\Compression\Lz4KafkaStream.cs,WriteEof,The following statement contains a magic number: _base.Write (_zero32' 0' 4);  
Magic Number,kafka4net.ConsumerImpl,TopicPartition,D:\newReposJune17\ntent-ad_kafka4net\src\ConsumerImpl\TopicPartition.cs,GetHashCode,The following statement contains a magic number: unchecked// disable overflow' for the unlikely possibility that you   {  	// are compiling with overflow-checking enabled  	int hash = 27;  	hash = (13 * hash) + _topic.GetHashCode ();  	hash = (13 * hash) + _partitionId.GetHashCode ();  	return hash;  }  
Magic Number,kafka4net.ConsumerImpl,TopicPartition,D:\newReposJune17\ntent-ad_kafka4net\src\ConsumerImpl\TopicPartition.cs,GetHashCode,The following statement contains a magic number: unchecked// disable overflow' for the unlikely possibility that you   {  	// are compiling with overflow-checking enabled  	int hash = 27;  	hash = (13 * hash) + _topic.GetHashCode ();  	hash = (13 * hash) + _partitionId.GetHashCode ();  	return hash;  }  
Magic Number,kafka4net.ConsumerImpl,TopicPartition,D:\newReposJune17\ntent-ad_kafka4net\src\ConsumerImpl\TopicPartition.cs,GetHashCode,The following statement contains a magic number: unchecked// disable overflow' for the unlikely possibility that you   {  	// are compiling with overflow-checking enabled  	int hash = 27;  	hash = (13 * hash) + _topic.GetHashCode ();  	hash = (13 * hash) + _partitionId.GetHashCode ();  	return hash;  }  
Magic Number,kafka4net.ConsumerImpl,TopicPartition,D:\newReposJune17\ntent-ad_kafka4net\src\ConsumerImpl\TopicPartition.cs,GetHashCode,The following statement contains a magic number: hash = (13 * hash) + _topic.GetHashCode ();  
Magic Number,kafka4net.ConsumerImpl,TopicPartition,D:\newReposJune17\ntent-ad_kafka4net\src\ConsumerImpl\TopicPartition.cs,GetHashCode,The following statement contains a magic number: hash = (13 * hash) + _partitionId.GetHashCode ();  
Magic Number,kafka4net.Internal,PartitionStateChangeEvent,D:\newReposJune17\ntent-ad_kafka4net\src\Internal\PartitionStateChangeEvent.cs,GetHashCode,The following statement contains a magic number: unchecked// disable overflow' for the unlikely possibility that you   {  	// are compiling with overflow-checking enabled  	int hash = 27;  	hash = (13 * hash) + Topic.GetHashCode ();  	hash = (13 * hash) + PartitionId.GetHashCode ();  	hash = (13 * hash) + ErrorCode.GetHashCode ();  	return hash;  }  
Magic Number,kafka4net.Internal,PartitionStateChangeEvent,D:\newReposJune17\ntent-ad_kafka4net\src\Internal\PartitionStateChangeEvent.cs,GetHashCode,The following statement contains a magic number: unchecked// disable overflow' for the unlikely possibility that you   {  	// are compiling with overflow-checking enabled  	int hash = 27;  	hash = (13 * hash) + Topic.GetHashCode ();  	hash = (13 * hash) + PartitionId.GetHashCode ();  	hash = (13 * hash) + ErrorCode.GetHashCode ();  	return hash;  }  
Magic Number,kafka4net.Internal,PartitionStateChangeEvent,D:\newReposJune17\ntent-ad_kafka4net\src\Internal\PartitionStateChangeEvent.cs,GetHashCode,The following statement contains a magic number: unchecked// disable overflow' for the unlikely possibility that you   {  	// are compiling with overflow-checking enabled  	int hash = 27;  	hash = (13 * hash) + Topic.GetHashCode ();  	hash = (13 * hash) + PartitionId.GetHashCode ();  	hash = (13 * hash) + ErrorCode.GetHashCode ();  	return hash;  }  
Magic Number,kafka4net.Internal,PartitionStateChangeEvent,D:\newReposJune17\ntent-ad_kafka4net\src\Internal\PartitionStateChangeEvent.cs,GetHashCode,The following statement contains a magic number: unchecked// disable overflow' for the unlikely possibility that you   {  	// are compiling with overflow-checking enabled  	int hash = 27;  	hash = (13 * hash) + Topic.GetHashCode ();  	hash = (13 * hash) + PartitionId.GetHashCode ();  	hash = (13 * hash) + ErrorCode.GetHashCode ();  	return hash;  }  
Magic Number,kafka4net.Internal,PartitionStateChangeEvent,D:\newReposJune17\ntent-ad_kafka4net\src\Internal\PartitionStateChangeEvent.cs,GetHashCode,The following statement contains a magic number: hash = (13 * hash) + Topic.GetHashCode ();  
Magic Number,kafka4net.Internal,PartitionStateChangeEvent,D:\newReposJune17\ntent-ad_kafka4net\src\Internal\PartitionStateChangeEvent.cs,GetHashCode,The following statement contains a magic number: hash = (13 * hash) + PartitionId.GetHashCode ();  
Magic Number,kafka4net.Internal,PartitionStateChangeEvent,D:\newReposJune17\ntent-ad_kafka4net\src\Internal\PartitionStateChangeEvent.cs,GetHashCode,The following statement contains a magic number: hash = (13 * hash) + ErrorCode.GetHashCode ();  
Magic Number,kafka4net.Internal,PartitionRecoveryMonitor,D:\newReposJune17\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The following statement contains a magic number: while (!_cancel.IsCancellationRequested) {  	//_log.Debug("RecoveryLoop iterating {0}"' this);  	//  	// Check either there is any job for given broker  	//  	if (_failedList.Count == 0) {  		// TODO: await for the list to receive 1st item instead of looping  		await Task.Delay (1000' _cancel);  		continue;  	}  	//  	// Query metadata from given broker for any failed topics.  	//  	MetadataResponse response;  	try {  		EtwTrace.Log.RecoveryMonitor_SendingPing (_id' broker.Host' broker.Port);  		response = await _protocol.MetadataRequest (new TopicRequest {  			Topics = _failedList.Keys.Select (t => t.Item1).Distinct ().ToArray ()  		}' broker' noTransportErrors: true);  		EtwTrace.Log.RecoveryMonitor_PingResponse (_id' broker.Host' broker.Port);  	} catch (Exception ex) {  		_log.Debug ("PartitionRecoveryMonitor error. Broker: {0}' error: {1}"' broker' ex.Message);  		EtwTrace.Log.RecoveryMonitor_PingFailed (_id' broker.Host' broker.Port' ex.Message);  		response = null;  	}  	if (response == null) {  		await Task.Delay (1000' _cancel);  		continue;  	}  	//  	// Join failed partitions with successful responses to find out recovered ones  	//  	Tuple<string' int' int>[] maybeHealedPartitions = (from responseTopic in response.Topics  	from responsePart in responseTopic.Partitions  	let key = new Tuple<string' int> (responseTopic.TopicName' responsePart.Id)  	where responseTopic.ErrorCode.IsSuccess () && responsePart.ErrorCode.IsSuccess () && _failedList.ContainsKey (key)  	select Tuple.Create (responseTopic.TopicName' responsePart.Id' responsePart.Leader)).ToArray ();  	if (_log.IsDebugEnabled) {  		if (maybeHealedPartitions.Length == 0) {  			_log.Debug ("Out of {0} partitions returned from broker {2}' none of the {3} errored partitions are healed. Current partition states for errored partitions: [{1}]"' response.Topics.SelectMany (t => t.Partitions).Count ()' string.Join ("'"' response.Topics.SelectMany (t => t.Partitions.Select (p => new {  				t.TopicName'  				TopicErrorCode = t.ErrorCode'  				PartitionId = p.Id'  				PartitionErrorCode = p.ErrorCode  			})).Where (p => _failedList.ContainsKey (new Tuple<string' int> (p.TopicName' p.PartitionId))).Select (p => string.Format ("{0}:{1}:{2}:{3}"' p.TopicName' p.TopicErrorCode' p.PartitionId' p.PartitionErrorCode)))' broker' _failedList.Count);  		} else {  			var str = new StringBuilder ();  			foreach (var leader in maybeHealedPartitions.GroupBy (p => p.Item3' (i' tuples) => new {  				Leader = i'  				Topics = tuples.GroupBy (t => t.Item1)  			})) {  				str.AppendFormat (" Leader: {0}\n"' leader.Leader);  				foreach (var topic1 in leader.Topics) {  					str.AppendFormat ("  Topic: {0} "' topic1.Key);  					str.AppendFormat ("[{0}]\n"' string.Join ("'"' topic1.Select (t => t.Item2)));  				}  			}  			_log.Debug ("Healed partitions found by broker {0} (will check broker availability):\n{1}"' broker' str.ToString ());  		}  	}  	if (EtwTrace.Log.IsEnabled ()) {  		if (maybeHealedPartitions.Length != 0) {  			EtwTrace.Log.RecoveryMonitor_PossiblyHealedPartitions (_id' maybeHealedPartitions.Length);  		} else {  			EtwTrace.Log.RecoveryMonitor_NoHealedPartitions (_id);  		}  	}  	//  	// Make sure that brokers for healed partitions are accessible' because it is possible that  	// broker B1 said that partition belongs to B2 and B2 can not be reach.  	// It is checked only that said broker responds to metadata request without exceptions.  	//  	var aliveChecks = maybeHealedPartitions.GroupBy (p => p.Item3).Select (async brokerGrp => {  		BrokerMeta newBroker;  		_brokers.TryGetValue (brokerGrp.Key' out newBroker);  		if (newBroker == null) {  			newBroker = response.Brokers.SingleOrDefault (b => b.NodeId == brokerGrp.Key);  			// If Cluster started when one of the brokers was down' and later it comes alive'  			// it will be missing from our list of brokers. See issue #14.  			_log.Debug ("received MetadataResponse for broker that is not yet in our list: {0}"' newBroker);  			if (newBroker == null) {  				_log.Error ("Got metadata response with partition refering to a broker which is not part of the response: {0}"' response.ToString ());  				return;  			}  			// Broadcast only newly discovered broker and strip everything else' because this is the only  			// confirmed data.  			var filteredMeta = new MetadataResponse {  				Brokers = new[] {  					newBroker  				}'  				Topics = new TopicMeta[] {    				}  			};  			_newMetadataEvent.OnNext (filteredMeta);  		}  		try {  			EtwTrace.Log.RecoveryMonitor_CheckingBrokerAccessibility (_id' newBroker.Host' newBroker.Port' newBroker.NodeId);  			MetadataResponse response2 = await _protocol.MetadataRequest (new TopicRequest {  				Topics = brokerGrp.Select (g => g.Item1).Distinct ().ToArray ()  			}' newBroker' noTransportErrors: true);  			EtwTrace.Log.RecoveryMonitor_BrokerIsAccessible (_id' newBroker.Host' newBroker.Port' newBroker.NodeId);  			// success!  			// raise new metadata event   			_log.Info ("Alive brokers detected: {0} which responded with: {1}"' newBroker' response2);  			// Join maybe healed partitions with partitions which belong to alive broker  			var confirmedHealedTopics = (from maybeHealedPartition in brokerGrp  			from healedTopic in response2.Topics  			where healedTopic.TopicName == maybeHealedPartition.Item1  			from healedPart in healedTopic.Partitions  			where healedPart.Id == maybeHealedPartition.Item2 && healedPart.Leader == brokerGrp.Key  			group healedPart by new {  				healedTopic.TopicName'  				healedTopic.ErrorCode  			} into healedTopicGrp  			select healedTopicGrp);  			// broadcast only trully healed partitions which belong to alive broker  			var filteredResponse = new MetadataResponse {  				Brokers = response2.Brokers'  				// we may broadcast more than 1 broker' but it should be ok because discovery of new broker metadata does not cause any actions  				Topics = confirmedHealedTopics.Where (t => t.Any ()).// broadcast only topics which have healed partitions  				Select (t => new TopicMeta {  					ErrorCode = t.Key.ErrorCode'  					TopicName = t.Key.TopicName'  					Partitions = t.ToArray ()  				}).ToArray ()  			};  			_log.Debug ("Broadcasting filtered response {0}"' filteredResponse);  			if (EtwTrace.Log.IsEnabled ())  				foreach (var topic in filteredResponse.Topics)  					EtwTrace.Log.RecoveryMonitor_HealedPartitions (_id' newBroker.Host' newBroker.Port' newBroker.NodeId' topic.TopicName' string.Join ("'"' topic.Partitions.Select (p => p.Id)));  			_newMetadataEvent.OnNext (filteredResponse);  		} catch (Exception e) {  			_log.Warn ("Metadata points to broker but it is not accessible. Error: {0}"' e.Message);  		}  	});  	// Wait for all checks to complete' otherwise' if a broker does not respond and hold connection open until tcp timeout'  	// we will keep accumulating responses in memory faster than they time out. See https://github.com/ntent-ad/kafka4net/issues/30  	await Task.WhenAll (aliveChecks.ToArray ());  	await Task.Delay (3000' _cancel);  }  
Magic Number,kafka4net.Internal,PartitionRecoveryMonitor,D:\newReposJune17\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The following statement contains a magic number: while (!_cancel.IsCancellationRequested) {  	//_log.Debug("RecoveryLoop iterating {0}"' this);  	//  	// Check either there is any job for given broker  	//  	if (_failedList.Count == 0) {  		// TODO: await for the list to receive 1st item instead of looping  		await Task.Delay (1000' _cancel);  		continue;  	}  	//  	// Query metadata from given broker for any failed topics.  	//  	MetadataResponse response;  	try {  		EtwTrace.Log.RecoveryMonitor_SendingPing (_id' broker.Host' broker.Port);  		response = await _protocol.MetadataRequest (new TopicRequest {  			Topics = _failedList.Keys.Select (t => t.Item1).Distinct ().ToArray ()  		}' broker' noTransportErrors: true);  		EtwTrace.Log.RecoveryMonitor_PingResponse (_id' broker.Host' broker.Port);  	} catch (Exception ex) {  		_log.Debug ("PartitionRecoveryMonitor error. Broker: {0}' error: {1}"' broker' ex.Message);  		EtwTrace.Log.RecoveryMonitor_PingFailed (_id' broker.Host' broker.Port' ex.Message);  		response = null;  	}  	if (response == null) {  		await Task.Delay (1000' _cancel);  		continue;  	}  	//  	// Join failed partitions with successful responses to find out recovered ones  	//  	Tuple<string' int' int>[] maybeHealedPartitions = (from responseTopic in response.Topics  	from responsePart in responseTopic.Partitions  	let key = new Tuple<string' int> (responseTopic.TopicName' responsePart.Id)  	where responseTopic.ErrorCode.IsSuccess () && responsePart.ErrorCode.IsSuccess () && _failedList.ContainsKey (key)  	select Tuple.Create (responseTopic.TopicName' responsePart.Id' responsePart.Leader)).ToArray ();  	if (_log.IsDebugEnabled) {  		if (maybeHealedPartitions.Length == 0) {  			_log.Debug ("Out of {0} partitions returned from broker {2}' none of the {3} errored partitions are healed. Current partition states for errored partitions: [{1}]"' response.Topics.SelectMany (t => t.Partitions).Count ()' string.Join ("'"' response.Topics.SelectMany (t => t.Partitions.Select (p => new {  				t.TopicName'  				TopicErrorCode = t.ErrorCode'  				PartitionId = p.Id'  				PartitionErrorCode = p.ErrorCode  			})).Where (p => _failedList.ContainsKey (new Tuple<string' int> (p.TopicName' p.PartitionId))).Select (p => string.Format ("{0}:{1}:{2}:{3}"' p.TopicName' p.TopicErrorCode' p.PartitionId' p.PartitionErrorCode)))' broker' _failedList.Count);  		} else {  			var str = new StringBuilder ();  			foreach (var leader in maybeHealedPartitions.GroupBy (p => p.Item3' (i' tuples) => new {  				Leader = i'  				Topics = tuples.GroupBy (t => t.Item1)  			})) {  				str.AppendFormat (" Leader: {0}\n"' leader.Leader);  				foreach (var topic1 in leader.Topics) {  					str.AppendFormat ("  Topic: {0} "' topic1.Key);  					str.AppendFormat ("[{0}]\n"' string.Join ("'"' topic1.Select (t => t.Item2)));  				}  			}  			_log.Debug ("Healed partitions found by broker {0} (will check broker availability):\n{1}"' broker' str.ToString ());  		}  	}  	if (EtwTrace.Log.IsEnabled ()) {  		if (maybeHealedPartitions.Length != 0) {  			EtwTrace.Log.RecoveryMonitor_PossiblyHealedPartitions (_id' maybeHealedPartitions.Length);  		} else {  			EtwTrace.Log.RecoveryMonitor_NoHealedPartitions (_id);  		}  	}  	//  	// Make sure that brokers for healed partitions are accessible' because it is possible that  	// broker B1 said that partition belongs to B2 and B2 can not be reach.  	// It is checked only that said broker responds to metadata request without exceptions.  	//  	var aliveChecks = maybeHealedPartitions.GroupBy (p => p.Item3).Select (async brokerGrp => {  		BrokerMeta newBroker;  		_brokers.TryGetValue (brokerGrp.Key' out newBroker);  		if (newBroker == null) {  			newBroker = response.Brokers.SingleOrDefault (b => b.NodeId == brokerGrp.Key);  			// If Cluster started when one of the brokers was down' and later it comes alive'  			// it will be missing from our list of brokers. See issue #14.  			_log.Debug ("received MetadataResponse for broker that is not yet in our list: {0}"' newBroker);  			if (newBroker == null) {  				_log.Error ("Got metadata response with partition refering to a broker which is not part of the response: {0}"' response.ToString ());  				return;  			}  			// Broadcast only newly discovered broker and strip everything else' because this is the only  			// confirmed data.  			var filteredMeta = new MetadataResponse {  				Brokers = new[] {  					newBroker  				}'  				Topics = new TopicMeta[] {    				}  			};  			_newMetadataEvent.OnNext (filteredMeta);  		}  		try {  			EtwTrace.Log.RecoveryMonitor_CheckingBrokerAccessibility (_id' newBroker.Host' newBroker.Port' newBroker.NodeId);  			MetadataResponse response2 = await _protocol.MetadataRequest (new TopicRequest {  				Topics = brokerGrp.Select (g => g.Item1).Distinct ().ToArray ()  			}' newBroker' noTransportErrors: true);  			EtwTrace.Log.RecoveryMonitor_BrokerIsAccessible (_id' newBroker.Host' newBroker.Port' newBroker.NodeId);  			// success!  			// raise new metadata event   			_log.Info ("Alive brokers detected: {0} which responded with: {1}"' newBroker' response2);  			// Join maybe healed partitions with partitions which belong to alive broker  			var confirmedHealedTopics = (from maybeHealedPartition in brokerGrp  			from healedTopic in response2.Topics  			where healedTopic.TopicName == maybeHealedPartition.Item1  			from healedPart in healedTopic.Partitions  			where healedPart.Id == maybeHealedPartition.Item2 && healedPart.Leader == brokerGrp.Key  			group healedPart by new {  				healedTopic.TopicName'  				healedTopic.ErrorCode  			} into healedTopicGrp  			select healedTopicGrp);  			// broadcast only trully healed partitions which belong to alive broker  			var filteredResponse = new MetadataResponse {  				Brokers = response2.Brokers'  				// we may broadcast more than 1 broker' but it should be ok because discovery of new broker metadata does not cause any actions  				Topics = confirmedHealedTopics.Where (t => t.Any ()).// broadcast only topics which have healed partitions  				Select (t => new TopicMeta {  					ErrorCode = t.Key.ErrorCode'  					TopicName = t.Key.TopicName'  					Partitions = t.ToArray ()  				}).ToArray ()  			};  			_log.Debug ("Broadcasting filtered response {0}"' filteredResponse);  			if (EtwTrace.Log.IsEnabled ())  				foreach (var topic in filteredResponse.Topics)  					EtwTrace.Log.RecoveryMonitor_HealedPartitions (_id' newBroker.Host' newBroker.Port' newBroker.NodeId' topic.TopicName' string.Join ("'"' topic.Partitions.Select (p => p.Id)));  			_newMetadataEvent.OnNext (filteredResponse);  		} catch (Exception e) {  			_log.Warn ("Metadata points to broker but it is not accessible. Error: {0}"' e.Message);  		}  	});  	// Wait for all checks to complete' otherwise' if a broker does not respond and hold connection open until tcp timeout'  	// we will keep accumulating responses in memory faster than they time out. See https://github.com/ntent-ad/kafka4net/issues/30  	await Task.WhenAll (aliveChecks.ToArray ());  	await Task.Delay (3000' _cancel);  }  
Magic Number,kafka4net.Internal,PartitionRecoveryMonitor,D:\newReposJune17\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The following statement contains a magic number: while (!_cancel.IsCancellationRequested) {  	//_log.Debug("RecoveryLoop iterating {0}"' this);  	//  	// Check either there is any job for given broker  	//  	if (_failedList.Count == 0) {  		// TODO: await for the list to receive 1st item instead of looping  		await Task.Delay (1000' _cancel);  		continue;  	}  	//  	// Query metadata from given broker for any failed topics.  	//  	MetadataResponse response;  	try {  		EtwTrace.Log.RecoveryMonitor_SendingPing (_id' broker.Host' broker.Port);  		response = await _protocol.MetadataRequest (new TopicRequest {  			Topics = _failedList.Keys.Select (t => t.Item1).Distinct ().ToArray ()  		}' broker' noTransportErrors: true);  		EtwTrace.Log.RecoveryMonitor_PingResponse (_id' broker.Host' broker.Port);  	} catch (Exception ex) {  		_log.Debug ("PartitionRecoveryMonitor error. Broker: {0}' error: {1}"' broker' ex.Message);  		EtwTrace.Log.RecoveryMonitor_PingFailed (_id' broker.Host' broker.Port' ex.Message);  		response = null;  	}  	if (response == null) {  		await Task.Delay (1000' _cancel);  		continue;  	}  	//  	// Join failed partitions with successful responses to find out recovered ones  	//  	Tuple<string' int' int>[] maybeHealedPartitions = (from responseTopic in response.Topics  	from responsePart in responseTopic.Partitions  	let key = new Tuple<string' int> (responseTopic.TopicName' responsePart.Id)  	where responseTopic.ErrorCode.IsSuccess () && responsePart.ErrorCode.IsSuccess () && _failedList.ContainsKey (key)  	select Tuple.Create (responseTopic.TopicName' responsePart.Id' responsePart.Leader)).ToArray ();  	if (_log.IsDebugEnabled) {  		if (maybeHealedPartitions.Length == 0) {  			_log.Debug ("Out of {0} partitions returned from broker {2}' none of the {3} errored partitions are healed. Current partition states for errored partitions: [{1}]"' response.Topics.SelectMany (t => t.Partitions).Count ()' string.Join ("'"' response.Topics.SelectMany (t => t.Partitions.Select (p => new {  				t.TopicName'  				TopicErrorCode = t.ErrorCode'  				PartitionId = p.Id'  				PartitionErrorCode = p.ErrorCode  			})).Where (p => _failedList.ContainsKey (new Tuple<string' int> (p.TopicName' p.PartitionId))).Select (p => string.Format ("{0}:{1}:{2}:{3}"' p.TopicName' p.TopicErrorCode' p.PartitionId' p.PartitionErrorCode)))' broker' _failedList.Count);  		} else {  			var str = new StringBuilder ();  			foreach (var leader in maybeHealedPartitions.GroupBy (p => p.Item3' (i' tuples) => new {  				Leader = i'  				Topics = tuples.GroupBy (t => t.Item1)  			})) {  				str.AppendFormat (" Leader: {0}\n"' leader.Leader);  				foreach (var topic1 in leader.Topics) {  					str.AppendFormat ("  Topic: {0} "' topic1.Key);  					str.AppendFormat ("[{0}]\n"' string.Join ("'"' topic1.Select (t => t.Item2)));  				}  			}  			_log.Debug ("Healed partitions found by broker {0} (will check broker availability):\n{1}"' broker' str.ToString ());  		}  	}  	if (EtwTrace.Log.IsEnabled ()) {  		if (maybeHealedPartitions.Length != 0) {  			EtwTrace.Log.RecoveryMonitor_PossiblyHealedPartitions (_id' maybeHealedPartitions.Length);  		} else {  			EtwTrace.Log.RecoveryMonitor_NoHealedPartitions (_id);  		}  	}  	//  	// Make sure that brokers for healed partitions are accessible' because it is possible that  	// broker B1 said that partition belongs to B2 and B2 can not be reach.  	// It is checked only that said broker responds to metadata request without exceptions.  	//  	var aliveChecks = maybeHealedPartitions.GroupBy (p => p.Item3).Select (async brokerGrp => {  		BrokerMeta newBroker;  		_brokers.TryGetValue (brokerGrp.Key' out newBroker);  		if (newBroker == null) {  			newBroker = response.Brokers.SingleOrDefault (b => b.NodeId == brokerGrp.Key);  			// If Cluster started when one of the brokers was down' and later it comes alive'  			// it will be missing from our list of brokers. See issue #14.  			_log.Debug ("received MetadataResponse for broker that is not yet in our list: {0}"' newBroker);  			if (newBroker == null) {  				_log.Error ("Got metadata response with partition refering to a broker which is not part of the response: {0}"' response.ToString ());  				return;  			}  			// Broadcast only newly discovered broker and strip everything else' because this is the only  			// confirmed data.  			var filteredMeta = new MetadataResponse {  				Brokers = new[] {  					newBroker  				}'  				Topics = new TopicMeta[] {    				}  			};  			_newMetadataEvent.OnNext (filteredMeta);  		}  		try {  			EtwTrace.Log.RecoveryMonitor_CheckingBrokerAccessibility (_id' newBroker.Host' newBroker.Port' newBroker.NodeId);  			MetadataResponse response2 = await _protocol.MetadataRequest (new TopicRequest {  				Topics = brokerGrp.Select (g => g.Item1).Distinct ().ToArray ()  			}' newBroker' noTransportErrors: true);  			EtwTrace.Log.RecoveryMonitor_BrokerIsAccessible (_id' newBroker.Host' newBroker.Port' newBroker.NodeId);  			// success!  			// raise new metadata event   			_log.Info ("Alive brokers detected: {0} which responded with: {1}"' newBroker' response2);  			// Join maybe healed partitions with partitions which belong to alive broker  			var confirmedHealedTopics = (from maybeHealedPartition in brokerGrp  			from healedTopic in response2.Topics  			where healedTopic.TopicName == maybeHealedPartition.Item1  			from healedPart in healedTopic.Partitions  			where healedPart.Id == maybeHealedPartition.Item2 && healedPart.Leader == brokerGrp.Key  			group healedPart by new {  				healedTopic.TopicName'  				healedTopic.ErrorCode  			} into healedTopicGrp  			select healedTopicGrp);  			// broadcast only trully healed partitions which belong to alive broker  			var filteredResponse = new MetadataResponse {  				Brokers = response2.Brokers'  				// we may broadcast more than 1 broker' but it should be ok because discovery of new broker metadata does not cause any actions  				Topics = confirmedHealedTopics.Where (t => t.Any ()).// broadcast only topics which have healed partitions  				Select (t => new TopicMeta {  					ErrorCode = t.Key.ErrorCode'  					TopicName = t.Key.TopicName'  					Partitions = t.ToArray ()  				}).ToArray ()  			};  			_log.Debug ("Broadcasting filtered response {0}"' filteredResponse);  			if (EtwTrace.Log.IsEnabled ())  				foreach (var topic in filteredResponse.Topics)  					EtwTrace.Log.RecoveryMonitor_HealedPartitions (_id' newBroker.Host' newBroker.Port' newBroker.NodeId' topic.TopicName' string.Join ("'"' topic.Partitions.Select (p => p.Id)));  			_newMetadataEvent.OnNext (filteredResponse);  		} catch (Exception e) {  			_log.Warn ("Metadata points to broker but it is not accessible. Error: {0}"' e.Message);  		}  	});  	// Wait for all checks to complete' otherwise' if a broker does not respond and hold connection open until tcp timeout'  	// we will keep accumulating responses in memory faster than they time out. See https://github.com/ntent-ad/kafka4net/issues/30  	await Task.WhenAll (aliveChecks.ToArray ());  	await Task.Delay (3000' _cancel);  }  
Magic Number,kafka4net.Internal,PartitionRecoveryMonitor,D:\newReposJune17\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The following statement contains a magic number: if (_failedList.Count == 0) {  	// TODO: await for the list to receive 1st item instead of looping  	await Task.Delay (1000' _cancel);  	continue;  }  
Magic Number,kafka4net.Internal,PartitionRecoveryMonitor,D:\newReposJune17\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The following statement contains a magic number: await Task.Delay (1000' _cancel);  
Magic Number,kafka4net.Internal,PartitionRecoveryMonitor,D:\newReposJune17\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The following statement contains a magic number: if (response == null) {  	await Task.Delay (1000' _cancel);  	continue;  }  
Magic Number,kafka4net.Internal,PartitionRecoveryMonitor,D:\newReposJune17\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The following statement contains a magic number: await Task.Delay (1000' _cancel);  
Magic Number,kafka4net.Internal,PartitionRecoveryMonitor,D:\newReposJune17\ntent-ad_kafka4net\src\Internal\PartitionRecoveryMonitor.cs,RecoveryLoop,The following statement contains a magic number: await Task.Delay (3000' _cancel);  
Magic Number,kafka4net.Protocols,ResponseCorrelation,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\ResponseCorrelation.cs,CorrelateResponseLoop,The following statement contains a magic number: try {  	_cancellation = cancel;  	_log.Debug ("Starting reading loop from socket. {0}"' _id);  	_etw.CorrelationStart ();  	var buff = new byte[16 * 1024];  	while (client.Connected && !cancel.IsCancellationRequested) {  		try {  			// read message size  			//var buff = new byte[4];  			_etw.CorrelationReadingMessageSize ();  			await ReadBuffer (client' buff' 4' cancel);  			if (cancel.IsCancellationRequested) {  				_log.Debug ("Stopped reading from {0} because cancell requested"' _id);  				return;  			}  			var size = BigEndianConverter.ToInt32 (buff);  			_etw.CorrelationReadMessageSize (size);  			// TODO: size sanity check. What is the reasonable max size?  			//var body = new byte[size];  			if (size > buff.Length)  				buff = new byte[size];  			await ReadBuffer (client' buff' size' cancel);  			_etw.CorrelationReadBody (size);  			try {  				int correlationId = -1;  				// TODO: check read==size && read > 4  				correlationId = BigEndianConverter.ToInt32 (buff);  				_etw.CorrelationReceivedCorrelationId (correlationId);  				// find correlated action  				Action<byte[]' int' Exception> handler;  				// TODO: if correlation id is not found' there is a chance of corrupt   				// connection. Maybe recycle the connection?  				if (!_corelationTable.TryRemove (correlationId' out handler)) {  					_log.Error ("Unknown correlationId: " + correlationId);  					continue;  				}  				_etw.CorrelationExecutingHandler ();  				handler (buff' size' null);  				_etw.CorrelationExecutedHandler ();  			} catch (Exception ex) {  				var error = string.Format ("Error with handling message. Message bytes:\n{0}\n"' FormatBytes (buff' size));  				_etw.CorrelationError (ex.Message + " " + error);  				_log.Error (ex' error);  				throw;  			}  		} catch (SocketException e) {  			// shorter version of socket exception' without stack trace dump  			_log.Info ("CorrelationLoop socket exception. {0}. {1}"' e.Message' _id);  			throw;  		} catch (ObjectDisposedException) {  			_log.Debug ("CorrelationLoop socket exception. Object disposed. {0}"' _id);  			throw;  		} catch (IOException) {  			_log.Info ("CorrelationLoop IO exception. {0}"' _id);  			throw;  		} catch (Exception e) {  			_log.Error (e' "CorrelateResponseLoop error. {0}"' _id);  			throw;  		}  	}  	_log.Debug ("Finished reading loop from socket. {0}"' _id);  	EtwTrace.Log.CorrelationComplete ();  } catch (Exception e) {  	_corelationTable.Values.ForEach (c => c (null' 0' e));  	if (_onError != null && !cancel.IsCancellationRequested)  		// don't call back OnError if we were told to cancel  		_onError (e);  	if (!cancel.IsCancellationRequested)  		throw;  } finally {  	_log.Debug ("Finishing CorrelationLoop. Calling back error to clear waiters.");  	_corelationTable.Values.ForEach (c => c (null' 0' new CorrelationLoopException ("Correlation loop closed. Request will never get a response.") {  		IsRequestedClose = cancel.IsCancellationRequested  	}));  	_log.Debug ("Finished CorrelationLoop.");  }  
Magic Number,kafka4net.Protocols,ResponseCorrelation,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\ResponseCorrelation.cs,CorrelateResponseLoop,The following statement contains a magic number: try {  	_cancellation = cancel;  	_log.Debug ("Starting reading loop from socket. {0}"' _id);  	_etw.CorrelationStart ();  	var buff = new byte[16 * 1024];  	while (client.Connected && !cancel.IsCancellationRequested) {  		try {  			// read message size  			//var buff = new byte[4];  			_etw.CorrelationReadingMessageSize ();  			await ReadBuffer (client' buff' 4' cancel);  			if (cancel.IsCancellationRequested) {  				_log.Debug ("Stopped reading from {0} because cancell requested"' _id);  				return;  			}  			var size = BigEndianConverter.ToInt32 (buff);  			_etw.CorrelationReadMessageSize (size);  			// TODO: size sanity check. What is the reasonable max size?  			//var body = new byte[size];  			if (size > buff.Length)  				buff = new byte[size];  			await ReadBuffer (client' buff' size' cancel);  			_etw.CorrelationReadBody (size);  			try {  				int correlationId = -1;  				// TODO: check read==size && read > 4  				correlationId = BigEndianConverter.ToInt32 (buff);  				_etw.CorrelationReceivedCorrelationId (correlationId);  				// find correlated action  				Action<byte[]' int' Exception> handler;  				// TODO: if correlation id is not found' there is a chance of corrupt   				// connection. Maybe recycle the connection?  				if (!_corelationTable.TryRemove (correlationId' out handler)) {  					_log.Error ("Unknown correlationId: " + correlationId);  					continue;  				}  				_etw.CorrelationExecutingHandler ();  				handler (buff' size' null);  				_etw.CorrelationExecutedHandler ();  			} catch (Exception ex) {  				var error = string.Format ("Error with handling message. Message bytes:\n{0}\n"' FormatBytes (buff' size));  				_etw.CorrelationError (ex.Message + " " + error);  				_log.Error (ex' error);  				throw;  			}  		} catch (SocketException e) {  			// shorter version of socket exception' without stack trace dump  			_log.Info ("CorrelationLoop socket exception. {0}. {1}"' e.Message' _id);  			throw;  		} catch (ObjectDisposedException) {  			_log.Debug ("CorrelationLoop socket exception. Object disposed. {0}"' _id);  			throw;  		} catch (IOException) {  			_log.Info ("CorrelationLoop IO exception. {0}"' _id);  			throw;  		} catch (Exception e) {  			_log.Error (e' "CorrelateResponseLoop error. {0}"' _id);  			throw;  		}  	}  	_log.Debug ("Finished reading loop from socket. {0}"' _id);  	EtwTrace.Log.CorrelationComplete ();  } catch (Exception e) {  	_corelationTable.Values.ForEach (c => c (null' 0' e));  	if (_onError != null && !cancel.IsCancellationRequested)  		// don't call back OnError if we were told to cancel  		_onError (e);  	if (!cancel.IsCancellationRequested)  		throw;  } finally {  	_log.Debug ("Finishing CorrelationLoop. Calling back error to clear waiters.");  	_corelationTable.Values.ForEach (c => c (null' 0' new CorrelationLoopException ("Correlation loop closed. Request will never get a response.") {  		IsRequestedClose = cancel.IsCancellationRequested  	}));  	_log.Debug ("Finished CorrelationLoop.");  }  
Magic Number,kafka4net.Protocols,ResponseCorrelation,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\ResponseCorrelation.cs,CorrelateResponseLoop,The following statement contains a magic number: try {  	_cancellation = cancel;  	_log.Debug ("Starting reading loop from socket. {0}"' _id);  	_etw.CorrelationStart ();  	var buff = new byte[16 * 1024];  	while (client.Connected && !cancel.IsCancellationRequested) {  		try {  			// read message size  			//var buff = new byte[4];  			_etw.CorrelationReadingMessageSize ();  			await ReadBuffer (client' buff' 4' cancel);  			if (cancel.IsCancellationRequested) {  				_log.Debug ("Stopped reading from {0} because cancell requested"' _id);  				return;  			}  			var size = BigEndianConverter.ToInt32 (buff);  			_etw.CorrelationReadMessageSize (size);  			// TODO: size sanity check. What is the reasonable max size?  			//var body = new byte[size];  			if (size > buff.Length)  				buff = new byte[size];  			await ReadBuffer (client' buff' size' cancel);  			_etw.CorrelationReadBody (size);  			try {  				int correlationId = -1;  				// TODO: check read==size && read > 4  				correlationId = BigEndianConverter.ToInt32 (buff);  				_etw.CorrelationReceivedCorrelationId (correlationId);  				// find correlated action  				Action<byte[]' int' Exception> handler;  				// TODO: if correlation id is not found' there is a chance of corrupt   				// connection. Maybe recycle the connection?  				if (!_corelationTable.TryRemove (correlationId' out handler)) {  					_log.Error ("Unknown correlationId: " + correlationId);  					continue;  				}  				_etw.CorrelationExecutingHandler ();  				handler (buff' size' null);  				_etw.CorrelationExecutedHandler ();  			} catch (Exception ex) {  				var error = string.Format ("Error with handling message. Message bytes:\n{0}\n"' FormatBytes (buff' size));  				_etw.CorrelationError (ex.Message + " " + error);  				_log.Error (ex' error);  				throw;  			}  		} catch (SocketException e) {  			// shorter version of socket exception' without stack trace dump  			_log.Info ("CorrelationLoop socket exception. {0}. {1}"' e.Message' _id);  			throw;  		} catch (ObjectDisposedException) {  			_log.Debug ("CorrelationLoop socket exception. Object disposed. {0}"' _id);  			throw;  		} catch (IOException) {  			_log.Info ("CorrelationLoop IO exception. {0}"' _id);  			throw;  		} catch (Exception e) {  			_log.Error (e' "CorrelateResponseLoop error. {0}"' _id);  			throw;  		}  	}  	_log.Debug ("Finished reading loop from socket. {0}"' _id);  	EtwTrace.Log.CorrelationComplete ();  } catch (Exception e) {  	_corelationTable.Values.ForEach (c => c (null' 0' e));  	if (_onError != null && !cancel.IsCancellationRequested)  		// don't call back OnError if we were told to cancel  		_onError (e);  	if (!cancel.IsCancellationRequested)  		throw;  } finally {  	_log.Debug ("Finishing CorrelationLoop. Calling back error to clear waiters.");  	_corelationTable.Values.ForEach (c => c (null' 0' new CorrelationLoopException ("Correlation loop closed. Request will never get a response.") {  		IsRequestedClose = cancel.IsCancellationRequested  	}));  	_log.Debug ("Finished CorrelationLoop.");  }  
Magic Number,kafka4net.Protocols,ResponseCorrelation,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\ResponseCorrelation.cs,CorrelateResponseLoop,The following statement contains a magic number: while (client.Connected && !cancel.IsCancellationRequested) {  	try {  		// read message size  		//var buff = new byte[4];  		_etw.CorrelationReadingMessageSize ();  		await ReadBuffer (client' buff' 4' cancel);  		if (cancel.IsCancellationRequested) {  			_log.Debug ("Stopped reading from {0} because cancell requested"' _id);  			return;  		}  		var size = BigEndianConverter.ToInt32 (buff);  		_etw.CorrelationReadMessageSize (size);  		// TODO: size sanity check. What is the reasonable max size?  		//var body = new byte[size];  		if (size > buff.Length)  			buff = new byte[size];  		await ReadBuffer (client' buff' size' cancel);  		_etw.CorrelationReadBody (size);  		try {  			int correlationId = -1;  			// TODO: check read==size && read > 4  			correlationId = BigEndianConverter.ToInt32 (buff);  			_etw.CorrelationReceivedCorrelationId (correlationId);  			// find correlated action  			Action<byte[]' int' Exception> handler;  			// TODO: if correlation id is not found' there is a chance of corrupt   			// connection. Maybe recycle the connection?  			if (!_corelationTable.TryRemove (correlationId' out handler)) {  				_log.Error ("Unknown correlationId: " + correlationId);  				continue;  			}  			_etw.CorrelationExecutingHandler ();  			handler (buff' size' null);  			_etw.CorrelationExecutedHandler ();  		} catch (Exception ex) {  			var error = string.Format ("Error with handling message. Message bytes:\n{0}\n"' FormatBytes (buff' size));  			_etw.CorrelationError (ex.Message + " " + error);  			_log.Error (ex' error);  			throw;  		}  	} catch (SocketException e) {  		// shorter version of socket exception' without stack trace dump  		_log.Info ("CorrelationLoop socket exception. {0}. {1}"' e.Message' _id);  		throw;  	} catch (ObjectDisposedException) {  		_log.Debug ("CorrelationLoop socket exception. Object disposed. {0}"' _id);  		throw;  	} catch (IOException) {  		_log.Info ("CorrelationLoop IO exception. {0}"' _id);  		throw;  	} catch (Exception e) {  		_log.Error (e' "CorrelateResponseLoop error. {0}"' _id);  		throw;  	}  }  
Magic Number,kafka4net.Protocols,ResponseCorrelation,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\ResponseCorrelation.cs,CorrelateResponseLoop,The following statement contains a magic number: try {  	// read message size  	//var buff = new byte[4];  	_etw.CorrelationReadingMessageSize ();  	await ReadBuffer (client' buff' 4' cancel);  	if (cancel.IsCancellationRequested) {  		_log.Debug ("Stopped reading from {0} because cancell requested"' _id);  		return;  	}  	var size = BigEndianConverter.ToInt32 (buff);  	_etw.CorrelationReadMessageSize (size);  	// TODO: size sanity check. What is the reasonable max size?  	//var body = new byte[size];  	if (size > buff.Length)  		buff = new byte[size];  	await ReadBuffer (client' buff' size' cancel);  	_etw.CorrelationReadBody (size);  	try {  		int correlationId = -1;  		// TODO: check read==size && read > 4  		correlationId = BigEndianConverter.ToInt32 (buff);  		_etw.CorrelationReceivedCorrelationId (correlationId);  		// find correlated action  		Action<byte[]' int' Exception> handler;  		// TODO: if correlation id is not found' there is a chance of corrupt   		// connection. Maybe recycle the connection?  		if (!_corelationTable.TryRemove (correlationId' out handler)) {  			_log.Error ("Unknown correlationId: " + correlationId);  			continue;  		}  		_etw.CorrelationExecutingHandler ();  		handler (buff' size' null);  		_etw.CorrelationExecutedHandler ();  	} catch (Exception ex) {  		var error = string.Format ("Error with handling message. Message bytes:\n{0}\n"' FormatBytes (buff' size));  		_etw.CorrelationError (ex.Message + " " + error);  		_log.Error (ex' error);  		throw;  	}  } catch (SocketException e) {  	// shorter version of socket exception' without stack trace dump  	_log.Info ("CorrelationLoop socket exception. {0}. {1}"' e.Message' _id);  	throw;  } catch (ObjectDisposedException) {  	_log.Debug ("CorrelationLoop socket exception. Object disposed. {0}"' _id);  	throw;  } catch (IOException) {  	_log.Info ("CorrelationLoop IO exception. {0}"' _id);  	throw;  } catch (Exception e) {  	_log.Error (e' "CorrelateResponseLoop error. {0}"' _id);  	throw;  }  
Magic Number,kafka4net.Protocols,ResponseCorrelation,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\ResponseCorrelation.cs,CorrelateResponseLoop,The following statement contains a magic number: await ReadBuffer (client' buff' 4' cancel);  
Magic Number,kafka4net.Protocols,ResponseCorrelation,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\ResponseCorrelation.cs,FormatBytes,The following statement contains a magic number: return buff.Take (Math.Min (256' len)).Aggregate (new StringBuilder ()' (builder' b) => builder.Append (b.ToString ("x2"))' str => str.ToString ());  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,DeserializeMetadataResponse,The following statement contains a magic number: stream.Position += 4;  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,Serialize,The following statement contains a magic number: if (request.Topics == null || request.Topics.Length == 0) {  	stream.Write (_zero32' 0' 4);  } else {  	BigEndianConverter.Write (stream' request.Topics.Length);  	foreach (var t in request.Topics)  		Write (stream' t);  }  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,Serialize,The following statement contains a magic number: stream.Write (_zero32' 0' 4);  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,Write,The following statement contains a magic number: foreach (var message in messages) {  	stream.Write (_zero64' 0' 8);  	// producer does fake offset  	var messageSize = _messageOverheadSize + (message.Key.Length ?? 0) + message.Value.Count;  	BigEndianConverter.Write (stream' messageSize);  	Write (stream' message);  }  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,Write,The following statement contains a magic number: stream.Write (_zero64' 0' 8);  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,WriteCompressedMessageSet,The following statement contains a magic number: stream.Write (_zero64' 0' 8);  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,Write,The following statement contains a magic number: if (message.Key == null) {  	stream.Write (_minusOne32' 0' 4);  } else {  	BigEndianConverter.Write (stream' message.Key.Length);  	stream.Write (message.Key' 0' message.Key.Length);  }  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,Write,The following statement contains a magic number: stream.Write (_minusOne32' 0' 4);  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,WriteArray,The following statement contains a magic number: stream.Write (_minusOne32' 0' 4);  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,Write,The following statement contains a magic number: if (s == null) {  	stream.Write (_minusOne16' 0' 2);  	return;  }  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,Write,The following statement contains a magic number: stream.Write (_minusOne16' 0' 2);  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,WriteRequestHeader,The following statement contains a magic number: stream.Write (_minusOne32' 0' 4);  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,WriteRequestHeader,The following statement contains a magic number: stream.Write (_apiVersion' 0' 2);  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,GetProducerResponse,The following statement contains a magic number: stream.Position += 4;  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,Serialize,The following statement contains a magic number: stream.Write (_minusOne32' 0' 4);  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,Serialize,The following statement contains a magic number: stream.Write (_one32' 0' 4);  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,DeserializeOffsetResponse,The following statement contains a magic number: stream.Position += 4;  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,Serialize,The following statement contains a magic number: stream.Write (_minusOne32' 0' 4);  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,DeserializeFetchResponse,The following statement contains a magic number: stream.Position += 4;  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,The following statement contains a magic number: while (remainingMessageSetBytes > 0) {  	// we need at least be able to read offset and messageSize  	if (remainingMessageSetBytes < 8 + 4) {  		// not enough bytes left. This is a partial message. Skip to the end of the message set.  		stream.Position += remainingMessageSetBytes;  		yield break;  	}  	var offset = BigEndianConverter.ReadInt64 (stream);  	var messageSize = BigEndianConverter.ReadInt32 (stream);  	// we took 12 bytes there' check again that we have a full message.  	remainingMessageSetBytes -= 8 + 4;  	if (remainingMessageSetBytes < messageSize) {  		// not enough bytes left. This is a partial message. Skip to the end of the message set.  		stream.Position += remainingMessageSetBytes;  		yield break;  	}  	// Message  	var crc = (uint)BigEndianConverter.ReadInt32 (stream);  	byte magic = (byte)stream.ReadByte ();  	if (magic != 0)  		throw new BrokerException ("Invalid kafks message magic");  	// TODO: use special exception for data corruption  	var attributes = (byte)stream.ReadByte ();  	var compression = ParseCompression (attributes);  	var key = ReadByteArray (stream);  	var value = ReadByteArray (stream);  	if (compression == CompressionType.None) {  		var msg = new Message ();  		msg.Key = key;  		msg.Value = value;  		msg.Offset = offset;  		var computedCrc = Crc32.Update (magic);  		computedCrc = Crc32.Update (attributes' computedCrc);  		if (key == null) {  			computedCrc = Crc32.Update (_minusOne32' computedCrc);  		} else {  			computedCrc = Crc32.Update (key.Length' computedCrc);  			computedCrc = Crc32.Update (key' computedCrc);  		}  		if (value == null) {  			computedCrc = Crc32.Update (_minusOne32);  		} else {  			computedCrc = Crc32.Update (value.Length' computedCrc);  			computedCrc = Crc32.Update (value' computedCrc);  		}  		computedCrc = Crc32.GetHash (computedCrc);  		if (computedCrc != crc) {  			throw new BrokerException (string.Format ("Corrupt message: Crc does not match. Caclulated {0} but got {1}"' computedCrc' crc));  		}  		yield return msg;  	} else if (compression == CompressionType.Gzip) {  		var decompressedStream = new MemoryStream ();  		new GZipStream (new MemoryStream (value)' CompressionMode.Decompress).CopyTo (decompressedStream);  		decompressedStream.Seek (0' SeekOrigin.Begin);  		// Recursion  		var innerMessages = ReadMessageSet (decompressedStream' (int)decompressedStream.Length);  		foreach (var innerMessage in innerMessages)  			yield return innerMessage;  	} else if (compression == CompressionType.Lz4) {  		using (var lz4Stream = new Lz4KafkaStream (new MemoryStream (value)' CompressionStreamMode.Decompress)) {  			var decompressed = new MemoryStream ();  			lz4Stream.CopyTo (decompressed);  			decompressed.Seek (0' SeekOrigin.Begin);  			var decompressedMessages = ReadMessageSet (decompressed' (int)decompressed.Length);  			foreach (var msg in decompressedMessages)  				yield return msg;  		}  	} else if (compression == CompressionType.Snappy) {  		if (_snappyCompressedBuffer == null)  			KafkaSnappyStream.AllocateBuffers (out _snappyUncompressedBuffer' out _snappyCompressedBuffer);  		using (var snappyStream = new KafkaSnappyStream (new MemoryStream (value)' CompressionStreamMode.Decompress' _snappyUncompressedBuffer' _snappyCompressedBuffer)) {  			var decompressed = new MemoryStream ();  			snappyStream.CopyTo (decompressed);  			decompressed.Seek (0' SeekOrigin.Begin);  			var decompressedMessages = ReadMessageSet (decompressed' (int)decompressed.Length);  			foreach (var msg in decompressedMessages)  				yield return msg;  		}  	} else {  		throw new BrokerException (string.Format ("Unknown compression type: {0}"' attributes & 3));  	}  	// subtract messageSize of that message from remaining bytes  	remainingMessageSetBytes -= messageSize;  }  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,The following statement contains a magic number: while (remainingMessageSetBytes > 0) {  	// we need at least be able to read offset and messageSize  	if (remainingMessageSetBytes < 8 + 4) {  		// not enough bytes left. This is a partial message. Skip to the end of the message set.  		stream.Position += remainingMessageSetBytes;  		yield break;  	}  	var offset = BigEndianConverter.ReadInt64 (stream);  	var messageSize = BigEndianConverter.ReadInt32 (stream);  	// we took 12 bytes there' check again that we have a full message.  	remainingMessageSetBytes -= 8 + 4;  	if (remainingMessageSetBytes < messageSize) {  		// not enough bytes left. This is a partial message. Skip to the end of the message set.  		stream.Position += remainingMessageSetBytes;  		yield break;  	}  	// Message  	var crc = (uint)BigEndianConverter.ReadInt32 (stream);  	byte magic = (byte)stream.ReadByte ();  	if (magic != 0)  		throw new BrokerException ("Invalid kafks message magic");  	// TODO: use special exception for data corruption  	var attributes = (byte)stream.ReadByte ();  	var compression = ParseCompression (attributes);  	var key = ReadByteArray (stream);  	var value = ReadByteArray (stream);  	if (compression == CompressionType.None) {  		var msg = new Message ();  		msg.Key = key;  		msg.Value = value;  		msg.Offset = offset;  		var computedCrc = Crc32.Update (magic);  		computedCrc = Crc32.Update (attributes' computedCrc);  		if (key == null) {  			computedCrc = Crc32.Update (_minusOne32' computedCrc);  		} else {  			computedCrc = Crc32.Update (key.Length' computedCrc);  			computedCrc = Crc32.Update (key' computedCrc);  		}  		if (value == null) {  			computedCrc = Crc32.Update (_minusOne32);  		} else {  			computedCrc = Crc32.Update (value.Length' computedCrc);  			computedCrc = Crc32.Update (value' computedCrc);  		}  		computedCrc = Crc32.GetHash (computedCrc);  		if (computedCrc != crc) {  			throw new BrokerException (string.Format ("Corrupt message: Crc does not match. Caclulated {0} but got {1}"' computedCrc' crc));  		}  		yield return msg;  	} else if (compression == CompressionType.Gzip) {  		var decompressedStream = new MemoryStream ();  		new GZipStream (new MemoryStream (value)' CompressionMode.Decompress).CopyTo (decompressedStream);  		decompressedStream.Seek (0' SeekOrigin.Begin);  		// Recursion  		var innerMessages = ReadMessageSet (decompressedStream' (int)decompressedStream.Length);  		foreach (var innerMessage in innerMessages)  			yield return innerMessage;  	} else if (compression == CompressionType.Lz4) {  		using (var lz4Stream = new Lz4KafkaStream (new MemoryStream (value)' CompressionStreamMode.Decompress)) {  			var decompressed = new MemoryStream ();  			lz4Stream.CopyTo (decompressed);  			decompressed.Seek (0' SeekOrigin.Begin);  			var decompressedMessages = ReadMessageSet (decompressed' (int)decompressed.Length);  			foreach (var msg in decompressedMessages)  				yield return msg;  		}  	} else if (compression == CompressionType.Snappy) {  		if (_snappyCompressedBuffer == null)  			KafkaSnappyStream.AllocateBuffers (out _snappyUncompressedBuffer' out _snappyCompressedBuffer);  		using (var snappyStream = new KafkaSnappyStream (new MemoryStream (value)' CompressionStreamMode.Decompress' _snappyUncompressedBuffer' _snappyCompressedBuffer)) {  			var decompressed = new MemoryStream ();  			snappyStream.CopyTo (decompressed);  			decompressed.Seek (0' SeekOrigin.Begin);  			var decompressedMessages = ReadMessageSet (decompressed' (int)decompressed.Length);  			foreach (var msg in decompressedMessages)  				yield return msg;  		}  	} else {  		throw new BrokerException (string.Format ("Unknown compression type: {0}"' attributes & 3));  	}  	// subtract messageSize of that message from remaining bytes  	remainingMessageSetBytes -= messageSize;  }  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,The following statement contains a magic number: while (remainingMessageSetBytes > 0) {  	// we need at least be able to read offset and messageSize  	if (remainingMessageSetBytes < 8 + 4) {  		// not enough bytes left. This is a partial message. Skip to the end of the message set.  		stream.Position += remainingMessageSetBytes;  		yield break;  	}  	var offset = BigEndianConverter.ReadInt64 (stream);  	var messageSize = BigEndianConverter.ReadInt32 (stream);  	// we took 12 bytes there' check again that we have a full message.  	remainingMessageSetBytes -= 8 + 4;  	if (remainingMessageSetBytes < messageSize) {  		// not enough bytes left. This is a partial message. Skip to the end of the message set.  		stream.Position += remainingMessageSetBytes;  		yield break;  	}  	// Message  	var crc = (uint)BigEndianConverter.ReadInt32 (stream);  	byte magic = (byte)stream.ReadByte ();  	if (magic != 0)  		throw new BrokerException ("Invalid kafks message magic");  	// TODO: use special exception for data corruption  	var attributes = (byte)stream.ReadByte ();  	var compression = ParseCompression (attributes);  	var key = ReadByteArray (stream);  	var value = ReadByteArray (stream);  	if (compression == CompressionType.None) {  		var msg = new Message ();  		msg.Key = key;  		msg.Value = value;  		msg.Offset = offset;  		var computedCrc = Crc32.Update (magic);  		computedCrc = Crc32.Update (attributes' computedCrc);  		if (key == null) {  			computedCrc = Crc32.Update (_minusOne32' computedCrc);  		} else {  			computedCrc = Crc32.Update (key.Length' computedCrc);  			computedCrc = Crc32.Update (key' computedCrc);  		}  		if (value == null) {  			computedCrc = Crc32.Update (_minusOne32);  		} else {  			computedCrc = Crc32.Update (value.Length' computedCrc);  			computedCrc = Crc32.Update (value' computedCrc);  		}  		computedCrc = Crc32.GetHash (computedCrc);  		if (computedCrc != crc) {  			throw new BrokerException (string.Format ("Corrupt message: Crc does not match. Caclulated {0} but got {1}"' computedCrc' crc));  		}  		yield return msg;  	} else if (compression == CompressionType.Gzip) {  		var decompressedStream = new MemoryStream ();  		new GZipStream (new MemoryStream (value)' CompressionMode.Decompress).CopyTo (decompressedStream);  		decompressedStream.Seek (0' SeekOrigin.Begin);  		// Recursion  		var innerMessages = ReadMessageSet (decompressedStream' (int)decompressedStream.Length);  		foreach (var innerMessage in innerMessages)  			yield return innerMessage;  	} else if (compression == CompressionType.Lz4) {  		using (var lz4Stream = new Lz4KafkaStream (new MemoryStream (value)' CompressionStreamMode.Decompress)) {  			var decompressed = new MemoryStream ();  			lz4Stream.CopyTo (decompressed);  			decompressed.Seek (0' SeekOrigin.Begin);  			var decompressedMessages = ReadMessageSet (decompressed' (int)decompressed.Length);  			foreach (var msg in decompressedMessages)  				yield return msg;  		}  	} else if (compression == CompressionType.Snappy) {  		if (_snappyCompressedBuffer == null)  			KafkaSnappyStream.AllocateBuffers (out _snappyUncompressedBuffer' out _snappyCompressedBuffer);  		using (var snappyStream = new KafkaSnappyStream (new MemoryStream (value)' CompressionStreamMode.Decompress' _snappyUncompressedBuffer' _snappyCompressedBuffer)) {  			var decompressed = new MemoryStream ();  			snappyStream.CopyTo (decompressed);  			decompressed.Seek (0' SeekOrigin.Begin);  			var decompressedMessages = ReadMessageSet (decompressed' (int)decompressed.Length);  			foreach (var msg in decompressedMessages)  				yield return msg;  		}  	} else {  		throw new BrokerException (string.Format ("Unknown compression type: {0}"' attributes & 3));  	}  	// subtract messageSize of that message from remaining bytes  	remainingMessageSetBytes -= messageSize;  }  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,The following statement contains a magic number: while (remainingMessageSetBytes > 0) {  	// we need at least be able to read offset and messageSize  	if (remainingMessageSetBytes < 8 + 4) {  		// not enough bytes left. This is a partial message. Skip to the end of the message set.  		stream.Position += remainingMessageSetBytes;  		yield break;  	}  	var offset = BigEndianConverter.ReadInt64 (stream);  	var messageSize = BigEndianConverter.ReadInt32 (stream);  	// we took 12 bytes there' check again that we have a full message.  	remainingMessageSetBytes -= 8 + 4;  	if (remainingMessageSetBytes < messageSize) {  		// not enough bytes left. This is a partial message. Skip to the end of the message set.  		stream.Position += remainingMessageSetBytes;  		yield break;  	}  	// Message  	var crc = (uint)BigEndianConverter.ReadInt32 (stream);  	byte magic = (byte)stream.ReadByte ();  	if (magic != 0)  		throw new BrokerException ("Invalid kafks message magic");  	// TODO: use special exception for data corruption  	var attributes = (byte)stream.ReadByte ();  	var compression = ParseCompression (attributes);  	var key = ReadByteArray (stream);  	var value = ReadByteArray (stream);  	if (compression == CompressionType.None) {  		var msg = new Message ();  		msg.Key = key;  		msg.Value = value;  		msg.Offset = offset;  		var computedCrc = Crc32.Update (magic);  		computedCrc = Crc32.Update (attributes' computedCrc);  		if (key == null) {  			computedCrc = Crc32.Update (_minusOne32' computedCrc);  		} else {  			computedCrc = Crc32.Update (key.Length' computedCrc);  			computedCrc = Crc32.Update (key' computedCrc);  		}  		if (value == null) {  			computedCrc = Crc32.Update (_minusOne32);  		} else {  			computedCrc = Crc32.Update (value.Length' computedCrc);  			computedCrc = Crc32.Update (value' computedCrc);  		}  		computedCrc = Crc32.GetHash (computedCrc);  		if (computedCrc != crc) {  			throw new BrokerException (string.Format ("Corrupt message: Crc does not match. Caclulated {0} but got {1}"' computedCrc' crc));  		}  		yield return msg;  	} else if (compression == CompressionType.Gzip) {  		var decompressedStream = new MemoryStream ();  		new GZipStream (new MemoryStream (value)' CompressionMode.Decompress).CopyTo (decompressedStream);  		decompressedStream.Seek (0' SeekOrigin.Begin);  		// Recursion  		var innerMessages = ReadMessageSet (decompressedStream' (int)decompressedStream.Length);  		foreach (var innerMessage in innerMessages)  			yield return innerMessage;  	} else if (compression == CompressionType.Lz4) {  		using (var lz4Stream = new Lz4KafkaStream (new MemoryStream (value)' CompressionStreamMode.Decompress)) {  			var decompressed = new MemoryStream ();  			lz4Stream.CopyTo (decompressed);  			decompressed.Seek (0' SeekOrigin.Begin);  			var decompressedMessages = ReadMessageSet (decompressed' (int)decompressed.Length);  			foreach (var msg in decompressedMessages)  				yield return msg;  		}  	} else if (compression == CompressionType.Snappy) {  		if (_snappyCompressedBuffer == null)  			KafkaSnappyStream.AllocateBuffers (out _snappyUncompressedBuffer' out _snappyCompressedBuffer);  		using (var snappyStream = new KafkaSnappyStream (new MemoryStream (value)' CompressionStreamMode.Decompress' _snappyUncompressedBuffer' _snappyCompressedBuffer)) {  			var decompressed = new MemoryStream ();  			snappyStream.CopyTo (decompressed);  			decompressed.Seek (0' SeekOrigin.Begin);  			var decompressedMessages = ReadMessageSet (decompressed' (int)decompressed.Length);  			foreach (var msg in decompressedMessages)  				yield return msg;  		}  	} else {  		throw new BrokerException (string.Format ("Unknown compression type: {0}"' attributes & 3));  	}  	// subtract messageSize of that message from remaining bytes  	remainingMessageSetBytes -= messageSize;  }  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,The following statement contains a magic number: while (remainingMessageSetBytes > 0) {  	// we need at least be able to read offset and messageSize  	if (remainingMessageSetBytes < 8 + 4) {  		// not enough bytes left. This is a partial message. Skip to the end of the message set.  		stream.Position += remainingMessageSetBytes;  		yield break;  	}  	var offset = BigEndianConverter.ReadInt64 (stream);  	var messageSize = BigEndianConverter.ReadInt32 (stream);  	// we took 12 bytes there' check again that we have a full message.  	remainingMessageSetBytes -= 8 + 4;  	if (remainingMessageSetBytes < messageSize) {  		// not enough bytes left. This is a partial message. Skip to the end of the message set.  		stream.Position += remainingMessageSetBytes;  		yield break;  	}  	// Message  	var crc = (uint)BigEndianConverter.ReadInt32 (stream);  	byte magic = (byte)stream.ReadByte ();  	if (magic != 0)  		throw new BrokerException ("Invalid kafks message magic");  	// TODO: use special exception for data corruption  	var attributes = (byte)stream.ReadByte ();  	var compression = ParseCompression (attributes);  	var key = ReadByteArray (stream);  	var value = ReadByteArray (stream);  	if (compression == CompressionType.None) {  		var msg = new Message ();  		msg.Key = key;  		msg.Value = value;  		msg.Offset = offset;  		var computedCrc = Crc32.Update (magic);  		computedCrc = Crc32.Update (attributes' computedCrc);  		if (key == null) {  			computedCrc = Crc32.Update (_minusOne32' computedCrc);  		} else {  			computedCrc = Crc32.Update (key.Length' computedCrc);  			computedCrc = Crc32.Update (key' computedCrc);  		}  		if (value == null) {  			computedCrc = Crc32.Update (_minusOne32);  		} else {  			computedCrc = Crc32.Update (value.Length' computedCrc);  			computedCrc = Crc32.Update (value' computedCrc);  		}  		computedCrc = Crc32.GetHash (computedCrc);  		if (computedCrc != crc) {  			throw new BrokerException (string.Format ("Corrupt message: Crc does not match. Caclulated {0} but got {1}"' computedCrc' crc));  		}  		yield return msg;  	} else if (compression == CompressionType.Gzip) {  		var decompressedStream = new MemoryStream ();  		new GZipStream (new MemoryStream (value)' CompressionMode.Decompress).CopyTo (decompressedStream);  		decompressedStream.Seek (0' SeekOrigin.Begin);  		// Recursion  		var innerMessages = ReadMessageSet (decompressedStream' (int)decompressedStream.Length);  		foreach (var innerMessage in innerMessages)  			yield return innerMessage;  	} else if (compression == CompressionType.Lz4) {  		using (var lz4Stream = new Lz4KafkaStream (new MemoryStream (value)' CompressionStreamMode.Decompress)) {  			var decompressed = new MemoryStream ();  			lz4Stream.CopyTo (decompressed);  			decompressed.Seek (0' SeekOrigin.Begin);  			var decompressedMessages = ReadMessageSet (decompressed' (int)decompressed.Length);  			foreach (var msg in decompressedMessages)  				yield return msg;  		}  	} else if (compression == CompressionType.Snappy) {  		if (_snappyCompressedBuffer == null)  			KafkaSnappyStream.AllocateBuffers (out _snappyUncompressedBuffer' out _snappyCompressedBuffer);  		using (var snappyStream = new KafkaSnappyStream (new MemoryStream (value)' CompressionStreamMode.Decompress' _snappyUncompressedBuffer' _snappyCompressedBuffer)) {  			var decompressed = new MemoryStream ();  			snappyStream.CopyTo (decompressed);  			decompressed.Seek (0' SeekOrigin.Begin);  			var decompressedMessages = ReadMessageSet (decompressed' (int)decompressed.Length);  			foreach (var msg in decompressedMessages)  				yield return msg;  		}  	} else {  		throw new BrokerException (string.Format ("Unknown compression type: {0}"' attributes & 3));  	}  	// subtract messageSize of that message from remaining bytes  	remainingMessageSetBytes -= messageSize;  }  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,The following statement contains a magic number: if (remainingMessageSetBytes < 8 + 4) {  	// not enough bytes left. This is a partial message. Skip to the end of the message set.  	stream.Position += remainingMessageSetBytes;  	yield break;  }  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,The following statement contains a magic number: if (remainingMessageSetBytes < 8 + 4) {  	// not enough bytes left. This is a partial message. Skip to the end of the message set.  	stream.Position += remainingMessageSetBytes;  	yield break;  }  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,The following statement contains a magic number: remainingMessageSetBytes -= 8 + 4;  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,The following statement contains a magic number: remainingMessageSetBytes -= 8 + 4;  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,The following statement contains a magic number: if (compression == CompressionType.None) {  	var msg = new Message ();  	msg.Key = key;  	msg.Value = value;  	msg.Offset = offset;  	var computedCrc = Crc32.Update (magic);  	computedCrc = Crc32.Update (attributes' computedCrc);  	if (key == null) {  		computedCrc = Crc32.Update (_minusOne32' computedCrc);  	} else {  		computedCrc = Crc32.Update (key.Length' computedCrc);  		computedCrc = Crc32.Update (key' computedCrc);  	}  	if (value == null) {  		computedCrc = Crc32.Update (_minusOne32);  	} else {  		computedCrc = Crc32.Update (value.Length' computedCrc);  		computedCrc = Crc32.Update (value' computedCrc);  	}  	computedCrc = Crc32.GetHash (computedCrc);  	if (computedCrc != crc) {  		throw new BrokerException (string.Format ("Corrupt message: Crc does not match. Caclulated {0} but got {1}"' computedCrc' crc));  	}  	yield return msg;  } else if (compression == CompressionType.Gzip) {  	var decompressedStream = new MemoryStream ();  	new GZipStream (new MemoryStream (value)' CompressionMode.Decompress).CopyTo (decompressedStream);  	decompressedStream.Seek (0' SeekOrigin.Begin);  	// Recursion  	var innerMessages = ReadMessageSet (decompressedStream' (int)decompressedStream.Length);  	foreach (var innerMessage in innerMessages)  		yield return innerMessage;  } else if (compression == CompressionType.Lz4) {  	using (var lz4Stream = new Lz4KafkaStream (new MemoryStream (value)' CompressionStreamMode.Decompress)) {  		var decompressed = new MemoryStream ();  		lz4Stream.CopyTo (decompressed);  		decompressed.Seek (0' SeekOrigin.Begin);  		var decompressedMessages = ReadMessageSet (decompressed' (int)decompressed.Length);  		foreach (var msg in decompressedMessages)  			yield return msg;  	}  } else if (compression == CompressionType.Snappy) {  	if (_snappyCompressedBuffer == null)  		KafkaSnappyStream.AllocateBuffers (out _snappyUncompressedBuffer' out _snappyCompressedBuffer);  	using (var snappyStream = new KafkaSnappyStream (new MemoryStream (value)' CompressionStreamMode.Decompress' _snappyUncompressedBuffer' _snappyCompressedBuffer)) {  		var decompressed = new MemoryStream ();  		snappyStream.CopyTo (decompressed);  		decompressed.Seek (0' SeekOrigin.Begin);  		var decompressedMessages = ReadMessageSet (decompressed' (int)decompressed.Length);  		foreach (var msg in decompressedMessages)  			yield return msg;  	}  } else {  	throw new BrokerException (string.Format ("Unknown compression type: {0}"' attributes & 3));  }  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,The following statement contains a magic number: if (compression == CompressionType.Gzip) {  	var decompressedStream = new MemoryStream ();  	new GZipStream (new MemoryStream (value)' CompressionMode.Decompress).CopyTo (decompressedStream);  	decompressedStream.Seek (0' SeekOrigin.Begin);  	// Recursion  	var innerMessages = ReadMessageSet (decompressedStream' (int)decompressedStream.Length);  	foreach (var innerMessage in innerMessages)  		yield return innerMessage;  } else if (compression == CompressionType.Lz4) {  	using (var lz4Stream = new Lz4KafkaStream (new MemoryStream (value)' CompressionStreamMode.Decompress)) {  		var decompressed = new MemoryStream ();  		lz4Stream.CopyTo (decompressed);  		decompressed.Seek (0' SeekOrigin.Begin);  		var decompressedMessages = ReadMessageSet (decompressed' (int)decompressed.Length);  		foreach (var msg in decompressedMessages)  			yield return msg;  	}  } else if (compression == CompressionType.Snappy) {  	if (_snappyCompressedBuffer == null)  		KafkaSnappyStream.AllocateBuffers (out _snappyUncompressedBuffer' out _snappyCompressedBuffer);  	using (var snappyStream = new KafkaSnappyStream (new MemoryStream (value)' CompressionStreamMode.Decompress' _snappyUncompressedBuffer' _snappyCompressedBuffer)) {  		var decompressed = new MemoryStream ();  		snappyStream.CopyTo (decompressed);  		decompressed.Seek (0' SeekOrigin.Begin);  		var decompressedMessages = ReadMessageSet (decompressed' (int)decompressed.Length);  		foreach (var msg in decompressedMessages)  			yield return msg;  	}  } else {  	throw new BrokerException (string.Format ("Unknown compression type: {0}"' attributes & 3));  }  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,The following statement contains a magic number: if (compression == CompressionType.Lz4) {  	using (var lz4Stream = new Lz4KafkaStream (new MemoryStream (value)' CompressionStreamMode.Decompress)) {  		var decompressed = new MemoryStream ();  		lz4Stream.CopyTo (decompressed);  		decompressed.Seek (0' SeekOrigin.Begin);  		var decompressedMessages = ReadMessageSet (decompressed' (int)decompressed.Length);  		foreach (var msg in decompressedMessages)  			yield return msg;  	}  } else if (compression == CompressionType.Snappy) {  	if (_snappyCompressedBuffer == null)  		KafkaSnappyStream.AllocateBuffers (out _snappyUncompressedBuffer' out _snappyCompressedBuffer);  	using (var snappyStream = new KafkaSnappyStream (new MemoryStream (value)' CompressionStreamMode.Decompress' _snappyUncompressedBuffer' _snappyCompressedBuffer)) {  		var decompressed = new MemoryStream ();  		snappyStream.CopyTo (decompressed);  		decompressed.Seek (0' SeekOrigin.Begin);  		var decompressedMessages = ReadMessageSet (decompressed' (int)decompressed.Length);  		foreach (var msg in decompressedMessages)  			yield return msg;  	}  } else {  	throw new BrokerException (string.Format ("Unknown compression type: {0}"' attributes & 3));  }  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,The following statement contains a magic number: if (compression == CompressionType.Snappy) {  	if (_snappyCompressedBuffer == null)  		KafkaSnappyStream.AllocateBuffers (out _snappyUncompressedBuffer' out _snappyCompressedBuffer);  	using (var snappyStream = new KafkaSnappyStream (new MemoryStream (value)' CompressionStreamMode.Decompress' _snappyUncompressedBuffer' _snappyCompressedBuffer)) {  		var decompressed = new MemoryStream ();  		snappyStream.CopyTo (decompressed);  		decompressed.Seek (0' SeekOrigin.Begin);  		var decompressedMessages = ReadMessageSet (decompressed' (int)decompressed.Length);  		foreach (var msg in decompressedMessages)  			yield return msg;  	}  } else {  	throw new BrokerException (string.Format ("Unknown compression type: {0}"' attributes & 3));  }  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ReadMessageSet,The following statement contains a magic number: throw new BrokerException (string.Format ("Unknown compression type: {0}"' attributes & 3));  
Magic Number,kafka4net.Protocols,Serializer,D:\newReposJune17\ntent-ad_kafka4net\src\Protocols\Serializer.cs,ParseCompression,The following statement contains a magic number: return (CompressionType)(attributes & 3);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherCancelSentWakeup,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (2' id);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherCancelSentWakeup,The following statement contains a magic number: Log.WriteEvent (2' id);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherPartitionSubscribed,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (3' id' partitionId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherPartitionSubscribed,The following statement contains a magic number: Log.WriteEvent (3' id' partitionId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherFetchResponse,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (4' id);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherFetchResponse,The following statement contains a magic number: Log.WriteEvent (4' id);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherMessage,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (5' id' keyLen' valueLen' offset' partition);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherMessage,The following statement contains a magic number: Log.WriteEvent (5' id' keyLen' valueLen' offset' partition);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherSleep,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (6' id);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherSleep,The following statement contains a magic number: Log.WriteEvent (6' id);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherWakeup,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (7' id);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherWakeup,The following statement contains a magic number: Log.WriteEvent (7' id);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherFetchRequest,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (8' id' topicCount' partsCount' host' port' brokerId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,FetcherFetchRequest,The following statement contains a magic number: Log.WriteEvent (8' id' topicCount' partsCount' host' port' brokerId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ConnectionConnecting,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (101' host' port);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ConnectionConnecting,The following statement contains a magic number: Log.WriteEvent (101' host' port);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ConnectionConnected,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (102' host' port);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ConnectionConnected,The following statement contains a magic number: Log.WriteEvent (102' host' port);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ConnectionErrored,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (103' host' port);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ConnectionErrored,The following statement contains a magic number: Log.WriteEvent (103' host' port);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ConnectionDisconnected,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (104' host' port);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ConnectionDisconnected,The following statement contains a magic number: Log.WriteEvent (104' host' port);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ConnectionReplaceClosedClient,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (107' host' port);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ConnectionReplaceClosedClient,The following statement contains a magic number: Log.WriteEvent (107' host' port);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,Connection_MarkSocketAsFailed_CorrelationLoopCancelling,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (108' host' port);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,Connection_MarkSocketAsFailed_CorrelationLoopCancelling,The following statement contains a magic number: Log.WriteEvent (108' host' port);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,Connection_MarkSocketAsFailed_TcpClosing,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (109' host' port);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,Connection_MarkSocketAsFailed_TcpClosing,The following statement contains a magic number: Log.WriteEvent (109' host' port);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationCreate,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (201);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationCreate,The following statement contains a magic number: Log.WriteEvent (201);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationStart,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (202);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationStart,The following statement contains a magic number: Log.WriteEvent (202);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationReadingMessageSize,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (203);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationReadingMessageSize,The following statement contains a magic number: Log.WriteEvent (203);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationServerClosedConnection,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (204);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationServerClosedConnection,The following statement contains a magic number: Log.WriteEvent (204);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationReadMessageSize,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (205' size);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationReadMessageSize,The following statement contains a magic number: Log.WriteEvent (205' size);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,Correlation_ReadingBodyChunk,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (206' left);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,Correlation_ReadingBodyChunk,The following statement contains a magic number: Log.WriteEvent (206' left);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationReadBodyChunk,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (207' read' left);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationReadBodyChunk,The following statement contains a magic number: Log.WriteEvent (207' read' left);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationReadBody,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (208' size);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationReadBody,The following statement contains a magic number: Log.WriteEvent (208' size);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationReceivedCorrelationId,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (209' correlationId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationReceivedCorrelationId,The following statement contains a magic number: Log.WriteEvent (209' correlationId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationExecutingHandler,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (210);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationExecutingHandler,The following statement contains a magic number: Log.WriteEvent (210);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationExecutedHandler,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (211);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationExecutedHandler,The following statement contains a magic number: Log.WriteEvent (211);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationError,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (212' message);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationError,The following statement contains a magic number: Log.WriteEvent (212' message);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationComplete,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (213);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationComplete,The following statement contains a magic number: Log.WriteEvent (213);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationWritingMessage,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (214' correlationId' length);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,CorrelationWritingMessage,The following statement contains a magic number: Log.WriteEvent (214' correlationId' length);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_Create,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (301' monitorId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_Create,The following statement contains a magic number: Log.WriteEvent (301' monitorId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_PartitionRecovered,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (302' monitorId' topic' partitionId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_PartitionRecovered,The following statement contains a magic number: Log.WriteEvent (302' monitorId' topic' partitionId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_PartitionFailed,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (303' monitorId' topic' partitionId' errorCode);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_PartitionFailed,The following statement contains a magic number: Log.WriteEvent (303' monitorId' topic' partitionId' errorCode);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_PartitionFailedAgain,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (304' monitorId' topic' partitionId' errorCode);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_PartitionFailedAgain,The following statement contains a magic number: Log.WriteEvent (304' monitorId' topic' partitionId' errorCode);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_RecoveryLoopStarted,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (305' monitorId' host' port' nodeId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_RecoveryLoopStarted,The following statement contains a magic number: Log.WriteEvent (305' monitorId' host' port' nodeId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_SendingPing,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (306' monitorId' host' port);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_SendingPing,The following statement contains a magic number: Log.WriteEvent (306' monitorId' host' port);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_PingResponse,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (307' monitorId' host' port);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_PingResponse,The following statement contains a magic number: Log.WriteEvent (307' monitorId' host' port);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_PingFailed,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (308' monitorId' host' port' message);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_PingFailed,The following statement contains a magic number: Log.WriteEvent (308' monitorId' host' port' message);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_PossiblyHealedPartitions,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (309' monitorId' count);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_PossiblyHealedPartitions,The following statement contains a magic number: Log.WriteEvent (309' monitorId' count);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_NoHealedPartitions,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (310' monitorId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_NoHealedPartitions,The following statement contains a magic number: Log.WriteEvent (310' monitorId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_CheckingBrokerAccessibility,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (311' monitorId' host' port' nodeId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_CheckingBrokerAccessibility,The following statement contains a magic number: Log.WriteEvent (311' monitorId' host' port' nodeId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_BrokerIsAccessible,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (312' monitorId' host' port' nodeId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_BrokerIsAccessible,The following statement contains a magic number: Log.WriteEvent (312' monitorId' host' port' nodeId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_HealedPartitions,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (313' monitorId' host' port' nodeId' topicName' partitions);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_HealedPartitions,The following statement contains a magic number: Log.WriteEvent (313' monitorId' host' port' nodeId' topicName' partitions);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_RecoveryLoopStop,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (314' monitorId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,RecoveryMonitor_RecoveryLoopStop,The following statement contains a magic number: Log.WriteEvent (314' monitorId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolMetadataRequest,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (400' request);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolMetadataRequest,The following statement contains a magic number: Log.WriteEvent (400' request);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolMetadataResponse,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (401' response' host' port' nodeId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolMetadataResponse,The following statement contains a magic number: Log.WriteEvent (401' response' host' port' nodeId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolProduceRequest,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (402' request' nodeId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolProduceRequest,The following statement contains a magic number: Log.WriteEvent (402' request' nodeId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolProduceResponse,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (403' response' nodeId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolProduceResponse,The following statement contains a magic number: Log.WriteEvent (403' response' nodeId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolOffsetRequest,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (404' request);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolOffsetRequest,The following statement contains a magic number: Log.WriteEvent (404' request);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolOffsetResponse,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (405' response);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolOffsetResponse,The following statement contains a magic number: Log.WriteEvent (405' response);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolFetchRequest,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (406' request);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolFetchRequest,The following statement contains a magic number: Log.WriteEvent (406' request);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolFetchResponse,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (407' response);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProtocolFetchResponse,The following statement contains a magic number: Log.WriteEvent (407' response);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProducerPermanentFailure,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (500' producerId' partitionCount);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProducerPermanentFailure,The following statement contains a magic number: Log.WriteEvent (500' producerId' partitionCount);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProducerPermanentFailureDetails,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (501' producerId' error);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProducerPermanentFailureDetails,The following statement contains a magic number: Log.WriteEvent (501' producerId' error);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProducerRecoverableErrors,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (502' producerId' partitionCount);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProducerRecoverableErrors,The following statement contains a magic number: Log.WriteEvent (502' producerId' partitionCount);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProducerStarting,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (503' topic' producerId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProducerStarting,The following statement contains a magic number: Log.WriteEvent (503' topic' producerId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProducerStarted,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (504' topic' producerId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProducerStarted,The following statement contains a magic number: Log.WriteEvent (504' topic' producerId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProducerError,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (505' error' producerId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProducerError,The following statement contains a magic number: Log.WriteEvent (505' error' producerId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProducerStopping,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (506' topic' producerId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProducerStopping,The following statement contains a magic number: Log.WriteEvent (506' topic' producerId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProducerStoped,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (507' topic' producerId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ProducerStoped,The following statement contains a magic number: Log.WriteEvent (507' topic' producerId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ClusterStarting,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (600' clusterId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ClusterStarting,The following statement contains a magic number: Log.WriteEvent (600' clusterId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ClusterStarted,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (601' clusterId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ClusterStarted,The following statement contains a magic number: Log.WriteEvent (601' clusterId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ClusterStopping,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (602' clusterId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ClusterStopping,The following statement contains a magic number: Log.WriteEvent (602' clusterId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ClusterError,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (603' clusterId' error);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ClusterError,The following statement contains a magic number: Log.WriteEvent (603' clusterId' error);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ClusterStopped,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (604' clusterId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ClusterStopped,The following statement contains a magic number: Log.WriteEvent (604' clusterId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataNewTopic,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (700' clusterId' topic);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataNewTopic,The following statement contains a magic number: Log.WriteEvent (700' clusterId' topic);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataPartitionErrorChange,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (702' clusterId' topic' partId' oldCode' newCode);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataPartitionErrorChange,The following statement contains a magic number: Log.WriteEvent (702' clusterId' topic' partId' oldCode' newCode);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataPartitionIsrChange,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (703' clusterId' topic' partId' oldIsrs' newIsrs);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataPartitionIsrChange,The following statement contains a magic number: Log.WriteEvent (703' clusterId' topic' partId' oldIsrs' newIsrs);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataPartitionLeaderChange,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (704' clusterId' topic' partId' oldLeader' newLeader);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataPartitionLeaderChange,The following statement contains a magic number: Log.WriteEvent (704' clusterId' topic' partId' oldLeader' newLeader);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataPartitionReplicasChange,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (705' clusterId' topic' partId' oldReplicas' newReplicas);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataPartitionReplicasChange,The following statement contains a magic number: Log.WriteEvent (705' clusterId' topic' partId' oldReplicas' newReplicas);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataTransportError,The following statement contains a magic number: Log.WriteEvent (706' topicName' clusterId' part' leader);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,MetadataNewBroker,The following statement contains a magic number: Log.WriteEvent (707' clusterId' host' port' nodeId);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ConsumerStarted,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (800' consumerId' topic);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ConsumerStarted,The following statement contains a magic number: Log.WriteEvent (800' consumerId' topic);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ConsumerStopped,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (801' consumerId' topic);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ConsumerStopped,The following statement contains a magic number: Log.WriteEvent (801' consumerId' topic);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ConsumerFlowControl,The following statement contains a magic number: if (IsEnabled ())  	Log.WriteEvent (802' isOpen);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,ConsumerFlowControl,The following statement contains a magic number: Log.WriteEvent (802' isOpen);  
Magic Number,kafka4net.Tracing,EtwTrace,D:\newReposJune17\ntent-ad_kafka4net\src\Tracing\EtwTrace.cs,Marker2,The following statement contains a magic number: Log.WriteEvent (900' marker);  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: if (s.CanSeek && s.Position + 4 > s.Length)  	throw new Exception (string.Format ("ReadInt32 needs 4 bytes but got ony {0}"' s.Length - s.Position));  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: return s.ReadByte () << 3 * 8 | s.ReadByte () << 2 * 8 | s.ReadByte () << 8 | s.ReadByte ();  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: return s.ReadByte () << 3 * 8 | s.ReadByte () << 2 * 8 | s.ReadByte () << 8 | s.ReadByte ();  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: return s.ReadByte () << 3 * 8 | s.ReadByte () << 2 * 8 | s.ReadByte () << 8 | s.ReadByte ();  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: return s.ReadByte () << 3 * 8 | s.ReadByte () << 2 * 8 | s.ReadByte () << 8 | s.ReadByte ();  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: return s.ReadByte () << 3 * 8 | s.ReadByte () << 2 * 8 | s.ReadByte () << 8 | s.ReadByte ();  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: return buff [offset] << 3 * 8 | buff [offset + 1] << 2 * 8 | buff [offset + 2] << 8 | buff [offset + 3];  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: return buff [offset] << 3 * 8 | buff [offset + 1] << 2 * 8 | buff [offset + 2] << 8 | buff [offset + 3];  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: return buff [offset] << 3 * 8 | buff [offset + 1] << 2 * 8 | buff [offset + 2] << 8 | buff [offset + 3];  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: return buff [offset] << 3 * 8 | buff [offset + 1] << 2 * 8 | buff [offset + 2] << 8 | buff [offset + 3];  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: return buff [offset] << 3 * 8 | buff [offset + 1] << 2 * 8 | buff [offset + 2] << 8 | buff [offset + 3];  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: return buff [offset] << 3 * 8 | buff [offset + 1] << 2 * 8 | buff [offset + 2] << 8 | buff [offset + 3];  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt32,The following statement contains a magic number: return buff [offset] << 3 * 8 | buff [offset + 1] << 2 * 8 | buff [offset + 2] << 8 | buff [offset + 3];  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt16,The following statement contains a magic number: if (s.CanSeek && s.Position + 2 > s.Length)  	throw new Exception (string.Format ("ReadInt16 needs 2 bytes but got ony {0}"' s.Length - s.Position));  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt16,The following statement contains a magic number: return (short)((s.ReadByte () << 8) | s.ReadByte ());  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt64,The following statement contains a magic number: if (stream.CanSeek && stream.Position + 8 > stream.Length)  	throw new Exception (string.Format ("ReadInt64 needs 8 bytes but got ony {0}"' stream.Length - stream.Position));  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt64,The following statement contains a magic number: for (int i = 0; i < 8; i++)  	res = res << 8 | stream.ReadByte ();  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt64,The following statement contains a magic number: for (int i = 0; i < 8; i++)  	res = res << 8 | stream.ReadByte ();  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ReadInt64,The following statement contains a magic number: res = res << 8 | stream.ReadByte ();  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: for (int j = 7; j >= 0; j--)  	stream.WriteByte ((byte)(ui >> j * 8 & 0xff));  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: for (int j = 7; j >= 0; j--)  	stream.WriteByte ((byte)(ui >> j * 8 & 0xff));  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: stream.WriteByte ((byte)(ui >> j * 8 & 0xff));  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: WriteByte (stream' i >> 8 * 3);  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: WriteByte (stream' i >> 8 * 3);  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: WriteByte (stream' i >> 8 * 2);  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: WriteByte (stream' i >> 8 * 2);  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: WriteByte (stream' i >> 8);  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: WriteByte (stream' i >> 8);  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: buff [0] = (byte)(i >> 8 * 3);  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: buff [0] = (byte)(i >> 8 * 3);  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: buff [1] = (byte)((i & 0xff0000) >> 8 * 2);  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: buff [1] = (byte)((i & 0xff0000) >> 8 * 2);  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: buff [2] = (byte)((i & 0xff00) >> 8);  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: buff [2] = (byte)((i & 0xff00) >> 8);  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,Write,The following statement contains a magic number: buff [3] = (byte)(i & 0xff);  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ToInt32,The following statement contains a magic number: return (buff [0] << 8 * 3) | (buff [1] << 8 * 2) | (buff [2] << 8) | buff [3];  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ToInt32,The following statement contains a magic number: return (buff [0] << 8 * 3) | (buff [1] << 8 * 2) | (buff [2] << 8) | buff [3];  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ToInt32,The following statement contains a magic number: return (buff [0] << 8 * 3) | (buff [1] << 8 * 2) | (buff [2] << 8) | buff [3];  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ToInt32,The following statement contains a magic number: return (buff [0] << 8 * 3) | (buff [1] << 8 * 2) | (buff [2] << 8) | buff [3];  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ToInt32,The following statement contains a magic number: return (buff [0] << 8 * 3) | (buff [1] << 8 * 2) | (buff [2] << 8) | buff [3];  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ToInt32,The following statement contains a magic number: return (buff [0] << 8 * 3) | (buff [1] << 8 * 2) | (buff [2] << 8) | buff [3];  
Magic Number,kafka4net.Utils,BigEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\BigEndianConverter.cs,ToInt32,The following statement contains a magic number: return (buff [0] << 8 * 3) | (buff [1] << 8 * 2) | (buff [2] << 8) | buff [3];  
Magic Number,kafka4net.Utils,Crc32,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: for (int i = offset; i < offset + len; i++)  	state = (state >> 8) ^ _table [buffer [i] ^ state & 0xff];  
Magic Number,kafka4net.Utils,Crc32,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: state = (state >> 8) ^ _table [buffer [i] ^ state & 0xff];  
Magic Number,kafka4net.Utils,Crc32,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: for (int i = buffer.Offset; i < end; i++)  	state = (state >> 8) ^ _table [array [i] ^ state & 0xff];  
Magic Number,kafka4net.Utils,Crc32,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: state = (state >> 8) ^ _table [array [i] ^ state & 0xff];  
Magic Number,kafka4net.Utils,Crc32,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: state = (state >> 8) ^ _table [(i >> 8 * 3 & 0xff) ^ state & 0xff];  
Magic Number,kafka4net.Utils,Crc32,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: state = (state >> 8) ^ _table [(i >> 8 * 3 & 0xff) ^ state & 0xff];  
Magic Number,kafka4net.Utils,Crc32,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: state = (state >> 8) ^ _table [(i >> 8 * 3 & 0xff) ^ state & 0xff];  
Magic Number,kafka4net.Utils,Crc32,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: state = (state >> 8) ^ _table [(i >> 8 * 2 & 0xff) ^ state & 0xff];  
Magic Number,kafka4net.Utils,Crc32,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: state = (state >> 8) ^ _table [(i >> 8 * 2 & 0xff) ^ state & 0xff];  
Magic Number,kafka4net.Utils,Crc32,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: state = (state >> 8) ^ _table [(i >> 8 * 2 & 0xff) ^ state & 0xff];  
Magic Number,kafka4net.Utils,Crc32,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: state = (state >> 8) ^ _table [(i >> 8 & 0xff) ^ state & 0xff];  
Magic Number,kafka4net.Utils,Crc32,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: state = (state >> 8) ^ _table [(i >> 8 & 0xff) ^ state & 0xff];  
Magic Number,kafka4net.Utils,Crc32,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: state = (state >> 8) ^ _table [(i & 0xff) ^ state & 0xff];  
Magic Number,kafka4net.Utils,Crc32,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\Crc32.cs,Update,The following statement contains a magic number: state = (state >> 8) ^ _table [b ^ state & 0xff];  
Magic Number,kafka4net.Utils,Crc32,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\Crc32.cs,InitializeTable,The following statement contains a magic number: for (var i = 0; i < 256; i++) {  	var entry = (UInt32)i;  	for (var j = 0; j < 8; j++)  		if ((entry & 1) == 1)  			entry = (entry >> 1) ^ _polynomial;  		else  			entry = entry >> 1;  	createTable [i] = entry;  }  
Magic Number,kafka4net.Utils,Crc32,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\Crc32.cs,InitializeTable,The following statement contains a magic number: for (var i = 0; i < 256; i++) {  	var entry = (UInt32)i;  	for (var j = 0; j < 8; j++)  		if ((entry & 1) == 1)  			entry = (entry >> 1) ^ _polynomial;  		else  			entry = entry >> 1;  	createTable [i] = entry;  }  
Magic Number,kafka4net.Utils,Crc32,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\Crc32.cs,InitializeTable,The following statement contains a magic number: for (var j = 0; j < 8; j++)  	if ((entry & 1) == 1)  		entry = (entry >> 1) ^ _polynomial;  	else  		entry = entry >> 1;  
Magic Number,kafka4net.Utils,LittleEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,ReadUInt32,The following statement contains a magic number: return (uint)(buff [offset + 3] << 3 * 8 | buff [offset + 2] << 2 * 8 | buff [offset + 1] << 8 | buff [offset]);  
Magic Number,kafka4net.Utils,LittleEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,ReadUInt32,The following statement contains a magic number: return (uint)(buff [offset + 3] << 3 * 8 | buff [offset + 2] << 2 * 8 | buff [offset + 1] << 8 | buff [offset]);  
Magic Number,kafka4net.Utils,LittleEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,ReadUInt32,The following statement contains a magic number: return (uint)(buff [offset + 3] << 3 * 8 | buff [offset + 2] << 2 * 8 | buff [offset + 1] << 8 | buff [offset]);  
Magic Number,kafka4net.Utils,LittleEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,ReadUInt32,The following statement contains a magic number: return (uint)(buff [offset + 3] << 3 * 8 | buff [offset + 2] << 2 * 8 | buff [offset + 1] << 8 | buff [offset]);  
Magic Number,kafka4net.Utils,LittleEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,ReadUInt32,The following statement contains a magic number: return (uint)(buff [offset + 3] << 3 * 8 | buff [offset + 2] << 2 * 8 | buff [offset + 1] << 8 | buff [offset]);  
Magic Number,kafka4net.Utils,LittleEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,ReadUInt32,The following statement contains a magic number: return (uint)(buff [offset + 3] << 3 * 8 | buff [offset + 2] << 2 * 8 | buff [offset + 1] << 8 | buff [offset]);  
Magic Number,kafka4net.Utils,LittleEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,ReadUInt32,The following statement contains a magic number: return (uint)(buff [offset + 3] << 3 * 8 | buff [offset + 2] << 2 * 8 | buff [offset + 1] << 8 | buff [offset]);  
Magic Number,kafka4net.Utils,LittleEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,Write,The following statement contains a magic number: buff [offset + 1] = (byte)(i >> 8 & 0xff);  
Magic Number,kafka4net.Utils,LittleEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,Write,The following statement contains a magic number: buff [offset + 2] = (byte)(i >> 8 * 2 & 0xff);  
Magic Number,kafka4net.Utils,LittleEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,Write,The following statement contains a magic number: buff [offset + 2] = (byte)(i >> 8 * 2 & 0xff);  
Magic Number,kafka4net.Utils,LittleEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,Write,The following statement contains a magic number: buff [offset + 2] = (byte)(i >> 8 * 2 & 0xff);  
Magic Number,kafka4net.Utils,LittleEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,Write,The following statement contains a magic number: buff [offset + 3] = (byte)(i >> 8 * 3 & 0xff);  
Magic Number,kafka4net.Utils,LittleEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,Write,The following statement contains a magic number: buff [offset + 3] = (byte)(i >> 8 * 3 & 0xff);  
Magic Number,kafka4net.Utils,LittleEndianConverter,D:\newReposJune17\ntent-ad_kafka4net\src\Utils\LittleEndianConverter.cs,Write,The following statement contains a magic number: buff [offset + 3] = (byte)(i >> 8 * 3 & 0xff);  
